[
  {
    "objectID": "PSYC214/index 2.html",
    "href": "PSYC214/index 2.html",
    "title": "Statistics for Psychologists I",
    "section": "",
    "text": "Welcome\nWelcome to the PSYC121 lab material for 2023!\nIn this module you will learn the basics of data handling, data processing and data visualisation. What that means is that, by the end of the this module (at Christmas time), you will be able to take a set of data, look at some basic statistics (e.g., the mean value), filter and process the data in order to answer basic questions about it, and present the data in an appealing way with different graphs. On top of this, you will be able to apply some of your knowledge of the basic “inferential” statistical tests that we will introduce in the lecture series (e.g., “t-test”).\nIn Week 1 we will introduce you to the software that we use to do all this useful work in statistics: “R” and “RStudio”. This is a coding language, and you will be taught the basics of how to write code in order to do all of the above key steps in data analysis. This tuition will continue in Term 2, and in your statistics modules in Year 2. Coding is challenging, but we know from experience that those students who attend classes, who work through the exercises carefully, and who seek help when they need it, do very well on these modules.\nMost of all, it’s important that you recognise that data analysis (statistics) is a critical aspect of the study of psychology. When we want to understand behaviour, we take measurements of that behaviour, which the majority of the time will result in quantitative (numerical) data. In order to understand the behaviour in a meaningful way, we need to conduct all of the above steps in our data analysis workflow. In summary, we cannot investigate psychological processes without the skills and toolbox of statistics and data analysis techniques.\n\n\nWorking at your own pace and seeking help\nWe have carefully prepared and refined the lab materials in this course over several years, and we feel that the pace of the materials is just right for our students. Some students will complete them more quickly, and others more slowly - both of these scenarios are absolutely fine. You should work at the pace that suits you best, making sure you understand the materials before you move forward.\nIt is fairly inevitable that you will get stuck on the lab materials in this module at some point. This might be in Week 1, Week 2, or later. When you do, it’s important you reach out for help:\n\nAsk your friends on your table. We’ve designed this teaching space to help collaborative work. You are encouraged to work with other students. Make sure you ask others to explain how they’ve solved an exercise. Make sure you help out others where you can. Always make sure you understand the code and the exercise; don’t simply be satisfied that you’ve got the right answer.\nAsk a GTA or Lecturer. Our Graduate Teaching Assistants are there to help you. There are no “stupid questions” in statistics, so just ask the GTAs any question about what you’re doing. Likewise, ask the Lecturer.\nAsk on the Discussion Forum. On the PSYC121 moodle page you will find a Discussion Forum. This is a great way to ask a question outside of the lab sessions. It might seem scary to ask a question in the forum, but please don’t be afraid to do this. If you have a question, you can bet 30+ other students also have the same question! So by asking the question on the forum, you help out many more people on the module. A friendly GTA or Lecturer will be along to answer the question as soon as possible (we aim for within 48 hours during the working week).\nAsk on the module Q & A session. Each week we hold a “Q&A” online session where we will try and resolve any general queries and problems. It’s an ideal time to discuss things that students are struggling with or confused about, and can share ideas and answers. You can ask on the discussion forum above and then we might be able to pick up the issues and discuss them, but also you can ask in the session itself.\n\nAsking good questions - it’s really important that you give us as much information as you can when you ask your questions (in class and especially on the forum). It’s so much harder to help respond to “I can’t do Exercise 5 in Week 7” (because we don’t know why it is that you can’t do it) than for example “In Exercise 5 of Week 7, I’ve managed to read in the data, put the graph looks quite odd. Here is the code I’m using…”\n\n\nCourse Contacts\nIf you have something that needs to be private, then please feel free to email the academic staff at the email addresses below:\n\n\n\n\nEmail Address\n\n\n\n\nTom Beesley (Coordinator)\nt.beesley at lancaster dot ac dot uk\n\n\nJohn Towse\nj.towse at lancaster dot ac dot uk\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC214/Week4.html",
    "href": "PSYC214/Week4.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC214/Week4.html#todays-lab-activities",
    "href": "PSYC214/Week4.html#todays-lab-activities",
    "title": "Statistics for Psychologists",
    "section": "2 - Today’s lab activities",
    "text": "2 - Today’s lab activities\nWith all of our house keeping in order, it is time to have a play.\n\n2.1 Some background information about the dataset\nIn recent weeks, we have been preoccupied with the Psyc214 robots and their impact on Psyc214 student learning. This week we will be moving away from our metallic friends and will examine the topic of meat consumption in Psyc214 students.\nThe basic premise of today’s work was introduced in Lecture 4. Here, a researcher was interested in whether a person’s typical meat consumption would alter after watching the documentary ‘Cowspiracy: The Sustainability Secret’. This documentary, for those unfamiliar, investigates environmental impact of animal agriculture.\n\nThe researcher’s broad research hypothesis was that meat consumption would decrease after watching the documentary. The researcher was also interested in whether this decrease in meat consumption would sustain over time.\n\n\n\nIntroducing the meat consumption study\n\n\nTo examine this, a psychology researcher decided to commandeer a seminar comprising of 50 students. The researcher played the Cowspiracy documentary and adminstered a set of comprehension questions to ensure that all students had been alert and paid attention - they all had, of course. Prior to this ‘treatment’, the researcher asked the student participants to record the number of burgers and sausages they consumed in the week preceding the lecture.\n\nA week later the same students returned for their next seminar. The researcher again asked the participants to record the number of burgers and sausages they had eaten over the last week.\n\nFinally, the researcher wanted to examine the prolonged effect (if any) of the documentary on meat consumption. To this end, the researcher hijacked yet another seminar, 3 weeks later (4 after the original documentary showcasing), and asked participants to record the number of burgers and sausages they consumed in prior the week. As such, the research design represents a one-factor within-participants design with three levels (pre documentary, one week post documentary, four weeks post documentary).\nThe researcher had the following experimental hypotheses:\n\nParticipants would eat significantly less meat products in the week after watching Cowspiracy than the week prior to watching Cowspiracy (Hypothesis 1)\n\nParticipants meat consumption would be significantly higher in the four weeks after watching Cowspiracy than the week after watching Cowspiracy (Hypothesis 2)\n\nTAKE A MOMENT TO PAUSE AND THINK ABOUT WHAT A FURTHER H3 HYPOTHESIS COULD BE? DISCUSS WITH YOUR INNER SELF OR A CLASSMATE.\n\n\n\n\nA pensive looking student!\n\n\n\n\n\n2.2 Familiarizing ourselves with the data:\nOk, now the hypothesis generation is over, let’s take a look at our data.\nPlease call up the data using the head() function, and ask for the top 50 data points.\n\n  head(lab4 data, n = x)\n\nStraight away, we can see there is a bug in our code. hint, it’s something with the data name and number of data points being called.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\nhead(lab4_data, n = 50)\n\n# A tibble: 50 × 5\n   participant a1_meat a2_meat a3_burgers a3_sausages\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1           1       6       3          3           3\n 2           2       3       4          4           2\n 3           3       8       1          1           2\n 4           4       6       5          3           2\n 5           5       9       0          0           0\n 6           6       8       1          2           4\n 7           7       9       2          1           1\n 8           8       5       4          2           5\n 9           9       7       4          1           0\n10          10       8       6          2           1\n# ℹ 40 more rows\n\n\nIf all has gone well, you should get the following:\n\n\n# A tibble: 50 × 5\n   participant a1_meat a2_meat a3_burgers a3_sausages\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;\n 1           1       6       3          3           3\n 2           2       3       4          4           2\n 3           3       8       1          1           2\n 4           4       6       5          3           2\n 5           5       9       0          0           0\n 6           6       8       1          2           4\n 7           7       9       2          1           1\n 8           8       5       4          2           5\n 9           9       7       4          1           0\n10          10       8       6          2           1\n# ℹ 40 more rows\n\n\n\n\n\nWe can see all data for our 50 participants. the columns show us the: - participant ID - the a1_meat value, which is the total number of burgers and sausages consumed in the week prior to the watching the documentary - the a2_meat value, which is the total number of burgers and sausages consumed in the week succeeding having watched the documentary - the a3_burgers value, which is the total number of burgers consumed in a week one month after watching the documentary - the a3_sausages value, which is the total number of sausages consumed in a week one month after watching the documentary.\nHave a look over this dataframe and describe the data to your inner self or a classmate.\nThe particularly eagle-eyed individuals among us will see that we appear to have a mistake in our data! While we have burger and sausage consumption together as a single total value for time a1 and a2, we failed to create the equivalent composite variable for time a3. Yep, definitely a mistake - a3 burger and a3 sausages have yet to be summed together to get a total.\n\nWell, it’s up to us to sort this out. To do this, let’s make this new variable using the mutate() function. Using this function, we can add up the total number of burgers and sausages consumed at time A3 and create a new ‘a3_meat’ total variable.**\n\nlab4_data %&gt;% \n  mutate(a3_meat = (a3_burgers + a3_sausages)) -&gt; lab4_data #create a new composite variable 'a3_meat' and add this as a column to the pre-existing lab4_data\n\nOK. We should now have a new variable in our dataset called ‘a3_meat’. Let’s check and see if this has all worked.\n\nlab4_data %&gt;% head(n=50)\n\n# A tibble: 50 × 6\n   participant a1_meat a2_meat a3_burgers a3_sausages a3_meat\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;      &lt;dbl&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1           1       6       3          3           3       6\n 2           2       3       4          4           2       6\n 3           3       8       1          1           2       3\n 4           4       6       5          3           2       5\n 5           5       9       0          0           0       0\n 6           6       8       1          2           4       6\n 7           7       9       2          1           1       2\n 8           8       5       4          2           5       7\n 9           9       7       4          1           0       1\n10          10       8       6          2           1       3\n# ℹ 40 more rows\n\n\nWe can see that it has worked, but the surplus a3_burger and a3_sausage columns remain. Just to keep it all tidy, let’s remove these redundant columns with the select() function.\n\nlab4_data %&gt;% select(-c(a3_burgers, a3_sausages)) -&gt; lab4_data # remove 'a3_burgers' and 'a3_sausages' columns\n\nNow let’s check again using the head() function and see if we’re happy. Please ask again for the first 50 rows.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\nlab4_data %&gt;% head(n=50)\n\n# A tibble: 50 × 4\n   participant a1_meat a2_meat a3_meat\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1           1       6       3       6\n 2           2       3       4       6\n 3           3       8       1       3\n 4           4       6       5       5\n 5           5       9       0       0\n 6           6       8       1       6\n 7           7       9       2       2\n 8           8       5       4       7\n 9           9       7       4       1\n10          10       8       6       3\n# ℹ 40 more rows\n\n\nHopefully you can check your dataframe and see the following…\n\n\n# A tibble: 50 × 4\n   participant a1_meat a2_meat a3_meat\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;\n 1           1       6       3       6\n 2           2       3       4       6\n 3           3       8       1       3\n 4           4       6       5       5\n 5           5       9       0       0\n 6           6       8       1       6\n 7           7       9       2       2\n 8           8       5       4       7\n 9           9       7       4       1\n10          10       8       6       3\n# ℹ 40 more rows\n\n\n\n\n\nphew! that worked a treat!\nSpecial note, the above code will make a new variable by adding together two existing ones - great. However, often we will want to make a new variable by taking the average of multiple rows of data (e.g., adding multiple scores together and then dividing by the number of rows). To do that, you could, for example, replace (a3_burgers + a3_sausages) in the mutate() function with ((a3_burgers + a3_sausages)/2) - where 2 is the number of rows. Alternatively you could incorporate the rowmeans() function from the ‘furniture’ package, e.g., rowmeans(a3_burgers, a3_sausages). The rest of the code for the argument would stay the same. You don’t need to do this now, as this will mess with what we do for the rest of the session!!!! It is just to inform you for future work.\n\n\n\n2.3 Reformatting the data:\n\n\n\nHold Up!\n\n\nHang on a sec folks. The data are currently presented with our repeated measures (A1_meat, A2_meat, A3_meat) formatted on the columns level. To run within-participant ANOVAs in R Studio, however, it is preferable that data are presented in a long format. Essentially, what this means is that all meat consumption scores should belong to a single long column. The grouping variable time (a1_meat, a2_meat, a3_meat) is then housed in its own single column.\nTo convert the data from seperate columns for each level to one long column, please execute the following code:\n\nlab4_data_long &lt;- lab4_data %&gt;%\n  gather(key = \"time\", value = \"meat\", a1_meat, a2_meat, a3_meat)\n\nYou will note, using the &lt;- defining function, we have created a brand new data frame named lab4_data_long\n\nLet’s view this new lab4_data_long dataframe and make sure it looks as we would expect. Here, we can again use the head() function - remember, however, that our number of rows have now increased 150 (all data in one column). Alternatively, you could also use the view(lab4_data_long) function.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\nlab4_data_long %&gt;% head(n=150)\n\n# A tibble: 150 × 3\n   participant time     meat\n         &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1           1 a1_meat     6\n 2           2 a1_meat     3\n 3           3 a1_meat     8\n 4           4 a1_meat     6\n 5           5 a1_meat     9\n 6           6 a1_meat     8\n 7           7 a1_meat     9\n 8           8 a1_meat     5\n 9           9 a1_meat     7\n10          10 a1_meat     8\n# ℹ 140 more rows\n\n\nWe can now see that our data are arranged in long format - huzzah.\n\n\n# A tibble: 150 × 3\n   participant time     meat\n         &lt;dbl&gt; &lt;chr&gt;   &lt;dbl&gt;\n 1           1 a1_meat     6\n 2           2 a1_meat     3\n 3           3 a1_meat     8\n 4           4 a1_meat     6\n 5           5 a1_meat     9\n 6           6 a1_meat     8\n 7           7 a1_meat     9\n 8           8 a1_meat     5\n 9           9 a1_meat     7\n10          10 a1_meat     8\n# ℹ 140 more rows\n\n\n\n\n\nWe now have three columns. ‘participant’ ID (as before), but also ‘time’ - which holds our A1, A2 and A3 levels - and ‘meat’ - which is the total number of burgers/sausages consumed for that time point.\n\nFrom now on, we will be working with our newly created lab4_data_long data frame. Let’s go ahead and look at the descriptive statistics for our data. As with previous weeks, we will use the rstatix() package to get some neat and tidy summary statistics.\nWe first specify the dataset we will use - which is lab4_data_long. After this we will use a pipe to ask R Studio to pass this data set on to the next function. Here, we use the group_by() function to specify that we want to distinguish between our three different levels - the variable we named ‘time’. We then use another pipe to pass the intermediate result onto the next function - the get_summary_stats(). In the parentheses we include our single ‘meat’ dependent variable. Finally we are asked to specify what ‘type’ of summary statistic we would like. Let’s go for mean, standard deviation, min and max values.\n\nDescriptives = lab4_data_long %\n  group_by(Time) %&gt;%\n  get_summary_stats(meat, show = c(\"mean\", \"sd\", \"min\", \"max\"))\noptions(digits = 4)\nprint.data.frame(Descriptives)\n\nOh blast. There seems to be some bugs in the code. Please go through and try to fix them. Hint, there are two errors in the code. When fixed, you will get the following:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\nDescriptives = lab4_data_long %&gt;%\n  group_by(time) %&gt;%\n  get_summary_stats(meat, show = c(\"mean\", \"sd\", \"min\", \"max\"))\noptions(digits = 4)\nprint.data.frame(Descriptives)\n\n     time variable  n mean    sd min max\n1 a1_meat     meat 50 6.16 2.024   2  10\n2 a2_meat     meat 50 2.80 1.414   0   6\n3 a3_meat     meat 50 4.56 1.886   0   8\n\n\n\n\n\nThere do seem to be some tendencies and trends in data. Speak with a classmate or your inner self about what these data indicate."
  },
  {
    "objectID": "PSYC214/Week4.html#assumptions-of-a-within-participants-anova",
    "href": "PSYC214/Week4.html#assumptions-of-a-within-participants-anova",
    "title": "Statistics for Psychologists",
    "section": "3 Assumptions of a within-participants ANOVA:",
    "text": "3 Assumptions of a within-participants ANOVA:\nBefore we progress on and see if there are differences in meat consumption between the different time levels, we first need to check our assumptions of the within-participants ANOVA. This is to ensure that we are not violating expectations of data, which may misconstrue our conclusions.\nAs you will recall from Lecture 4, there are 3 key assumptions of a within-participants ANOVA.\n1) The assumption of independence (i.e., are our participants independent from one another? - let’s say ‘yes!’)\n2) The assumption of normality (i.e., are our levels [a.k.a time groups] showing normal distributions with their data points?)\n3) The assumption of sphericity (i.e., is the variance in data points similar across levels?)\n\nThe first one - assumption of independence - we need to search our souls and ask if this is the case. Let’s say “yes, yes it was”.\nThe other two - assumption of normality and sphericity - we can test by analytical means. We should also check if are there any extreme values in our data - remember Lecture 3.\nLet’s give it a go!\n\n\n3.1 Outliers and extreme values\nWe can eye ball boxplots to look for suspicious data points, or we can test for extreme values very easily using the rstatix() package. It has this wonderful function called identify_outliers(), which will tell us if we have any outliers or extreme values. As a rule, outliers are typically fine to work with, extremes however are when things become a bit hairy.\nPlease copy the following code:\n\nlab4_data_long %&gt;% \n  group_by(time) %&gt;%\n  identify_outliers(meat)# the function to identify outliers and extreme values\n\n[1] time        participant meat        is.outlier  is.extreme \n&lt;0 rows&gt; (or 0-length row.names)\n\n\nThis is marvellous. We find zero outliers or extreme data points.\n\n\n\n3.2 Assumption of normality\nLike last week, we can test whether our data are normal using QQ plots and a Shapiro-Wilk test of normality. The QQ plot maps out the correlation between our data and a normal distribution. If many data points fall away from our reference line and outside the band of the 95% confidence interval we can assume that data are non-normal. \nThe Shapiro-Wilk test statistically calculates whether our data are ‘normal’. If the p-value for our Shapiro-Wilk statistic is equal to or less than p = .05, then this indicates we have failed the test and have non-normal data - not good! The Shapiro-Wilk test, however, becomes less useful as we have a larger N. The larger the sample, the more likely it is that you will get a statistically significant Shapiro-Wilk test result and our data will be assumed to be non-normal.\n\nLet’s start off by examining the QQ plot. Please enter or type the following code:\n\nggqqplot(lab4_data_long, \"meat\", facet.by = \"time\") # where we use \"\" around our variable names\n\n\n\n\nDiscuss with yourself of a colleague whether the data appear normal.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nMy two cents: Data look largely normal. For the A1 (pre) and A3 (after 1 month) levels the data fit nicely within the 95% confidence intervals. Level A2 (after 1 week) looks a little dicier, with some data points falling outside the 95% confidence intervals.\n\n\n\nLet’s run a Shapiro-Wilk test and see what it also tells us.\n\nlab4_data %&gt;%\n  group_by(time) %&gt;%\n  shapiro_test(meat)\n\nUh oh. There is a bug here. Let’s try figuring out the problem. The red error message is telling us that there is no ‘time’ variable, but it is spelled correctly. The problem must be somewhere else. Can you figure it out? Hint, it is only one problem.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nIf run correctly, you should receive the following output.\n\n#ANSWER CODE\nlab4_data_long %&gt;%\n  group_by(time) %&gt;%\n  shapiro_test(meat)\n\n# A tibble: 3 × 4\n  time    variable statistic      p\n  &lt;chr&gt;   &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 a1_meat meat         0.967 0.167 \n2 a2_meat meat         0.940 0.0139\n3 a3_meat meat         0.963 0.116 \n\n\n\n\n\nHave a think what the test is telling us before you scroll down for the answer. Remember, we are hoping that our values are non-signficant!\n\n\n\n\n\n\nAnswer\n\n\n\n\n\nThe Shapiro-Wilk test indicates that while data for levels A1 and A3 are normally distributed, the A2 level is not. Given that ANOVAs are pretty robust to violations of normality (Roberts & Russo, 1999), that we have 150 data points, and that the A2 Shapiro-Wilk test is not highly statistical ((p &gt; .01), I think we are safe to run the ANOVA without transformations.\n\n\n\n\nIn an official report or publication, a researcher should be transparent and inform the reader when the checks of normality fail. The researcher should also justify (with relevant literature) occasions in which they decide to continue on and run an ANOVA when data violations are present.\n\n\n\n3.3 Assumption of sphericity.\nThere is no need to run a separate statistical test to check sphericity (equal variance across levels). This is because R Studio will automatically check for violations of sphericity with a Mauchly’s test when computing the within-participants ANOVA itself. If data violate the assumption of sphericity - and trust me, within-participant designs frequently violate this assumption of ANOVA - the analysis function will automatically apply a Greenhouse-Geisser sphericity correction to handle this issue. Specifically, in cases of sphericity violations, the ANOVA table will correct the degrees of freedom and automatically include an adjusted ‘eta2[g]’ - or a ‘generalized effect size with Greenhouse-Geisser sphericity correction’."
  },
  {
    "objectID": "PSYC214/Week4.html#running-a-one-factor-within-participants-anova",
    "href": "PSYC214/Week4.html#running-a-one-factor-within-participants-anova",
    "title": "Statistics for Psychologists",
    "section": "4 Running a one-factor within-participants ANOVA",
    "text": "4 Running a one-factor within-participants ANOVA\nTo compute our one factor within participants ANOVA, let’s use the rstatix package anova_test() function.\nPlease enter the following code:\n\nmodel &lt;- lab4_data_long %&gt;% #Run an ANOVA with our long data\n    anova_test(dv = meat, wid = participant, within = time) # specify the dv, which rows belong to which participant and that time is the between-participant factor\nget_anova_table(model) # pull up this model\n\nANOVA Table (type III tests)\n\n  Effect  DFn  DFd     F        p p&lt;.05   ges\n1   time 1.75 85.7 47.47 1.54e-13     * 0.374\n\n\nThis provides us with a tonne of information. The ‘F’ value is our F statistic. ‘DFn’ and ‘DFd’ show the degres of freedom for the numerator and denominator of our F ratio formula (remember Lecture 4?). The ‘p’ shows us the p-value for our F statistic, while the ‘ges’ is our effect size with the the Greenhouse-Geisser sphericity correction - yep, we did fail the assumption of sphericity - not to worry.\nHave a think about what the ANOVA output is telling us.\n\n\n4.1 Reporting the results of the one-factor within-participants ANOVA in APA format\nAs always, it is important that we are able to write up our results for others. The following is an example of how we could do so.\nMeat consumption was analysed using a one factor within-participants ANOVA, comprising three time levels (pre-documentary, one-week after the documentary, and one month after the documentary). A Mauchly’s test indicated that the assumption of sphericity had been violated - the degrees of freedom were therefore corrected using Greenhouse-Geisser estimates of sphericity. The meat consumption of participants statistically differed between the different time points F(1.75, 85.7) = 47.47, p &lt; 0.001, ηp2 = 0.37.\n\n\n\n4.2 Running further pairwise comparisons\nAs you will recall, the ANOVA tells us that we have differences between our groups, but not specifically which groups differ. For our between-participants robot ANOVA, we discussed the possibility of running pairwise comparisons and/or posthoc tests.\n\nAs we are running a within-participants ANOVA - and many of the posthoc tests assume that data are from independent groups (i.e., not repeated measures) - we would be better off in the current situation running multiple pairwise t-tests with Bonferroni corrections (see Lecture 3 again if this is hard to follow).\nTo run our pairwise comparisons, please type or paste the following:\n\npairwise_comparisons &lt;- lab4_data_long %&gt;% # name object pairwise_comparisons \n  pairwise_t_test(\n    meat ~ time, paired = TRUE, var.equal = TRUE, # indicate this is paired (within participants) data\n    p.adjust.method = \"bonferroni\" #adjust for bonferroni correction\n    )\n(pairwise_comparisons) # show the output\n\n# A tibble: 3 × 10\n  .y.   group1 group2    n1    n2 statistic    df        p    p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 meat  a1_me… a2_me…    50    50     10.4     49 5.31e-14 1.59e-13 ****        \n2 meat  a1_me… a3_me…    50    50      3.96    49 2.43e- 4 7.29e- 4 ***         \n3 meat  a2_me… a3_me…    50    50     -5.89    49 3.52e- 7 1.06e- 6 ****        \n\n\nSTOP. Please have a look at the different combination of levels: - a1_meat and a2_meat - a1_meat and a3_meat - a2_meat and a3_meat\nDo they statistically differ from one another? What do the data tell us?\nWe need to write this up also!\n\n\n\n4.3 Reporting the results of the one-factor within-participants ANOVA in APA format\nMeat consumption was analysed using a one factor within-participants ANOVA, comprising three time levels (pre-documentary, one-week after the documentary, and one month after the documentary). A Mauchly’s test indicated that the assumption of sphericity had been violated - the degrees of freedom were corrected accordingly using Greenhouse-Geisser estimates of sphericity. The meat consumption of participants statistically differed between the different time points F(1.75, 85.7) = 47.47, p &lt; 0.001, ηp2 = 0.37. Paired pairwise comparisons with Bonferroni corrections found that meat consumption varied between all three time levels (adjusted p &lt; .001 for all three possible combinations). Participants consumed the highest amount of meat prior to watching the documentary (see figure 1). Participants consumed the lowest amount of meat in the one week following the documentary. Participants, on average, consumed significantly less meat a month after watching the documentary than they did before watching the documentary. Participants, however, ate significantly more meat one month after watching the documentary than they did in the week after the documentary.\nNote, the results section was already getting quite wordy and so no mean values of the different levels were included. Instead, the reader was directed to figure 1 - let’s make that now.\n\n\n\n4.4 APA barchart\nHere, we are going to borrow some code from our week 2 lab. We will adapt it for current purposes.\nLike in week 2, let’s start off by creating a new dataset with standard errors.\n\nmeat_plot &lt;- summarySE(lab4_data_long, measurevar=\"meat\", groupvars=c(\"time\")) #create data with standard errors which we can apply for our barchart\n\nOk, great. Now let’s adapt our APA barchart code from week two to make a new figure.\n\nmeat_plot %&gt;% #our dataset\n    ggplot(aes(x= time, # our X axis\n               y = meat, # our Y axis\n               fill = time)) + # How we will colour our seperate bars\n    geom_col(width = .7, position = position_dodge(.3)) + # width of columns and space between columns\n    scale_fill_manual(values = c(\"#D4D4D4\", \"#737373\", \"#323232\")) + # APA colours for bars\n    scale_y_continuous(expand = expansion(0), # trick to remove space below 0 on y axis\n                       limits = c(0, 7),\n                       breaks = seq(0, 7, 1)) + # set limit of Y axis to 7\n  xlab(\"Time since watching documentary\") + #rename the x axis label\n  ylab(\"Average number of burgers and sausages consumed over a week\") + #rename the y axis label\n  scale_x_discrete(labels = c('one week prior','one week after','one month after')) + #rename the groups on x axis\n    theme(panel.background = element_blank(), # removing the background panel colour\n          axis.line = element_line(color = \"black\"), # creating black axes lines\n          axis.ticks.x = element_blank(),\n          legend.position = \"none\") + # removing the legend, which is redundant\n    geom_errorbar(aes(ymin=meat-se, ymax=meat+se, width = 0.2))\n\n\n\n\nThat is all looking hunky dorie. We now have a figure that we can add to our results section, to provide a visual aid regarding the average scores and error bars for our three levels.\nIf you’ve finished this and are happy, please go on to ‘5 Further tasks’."
  },
  {
    "objectID": "PSYC214/Week4.html#further-tasks",
    "href": "PSYC214/Week4.html#further-tasks",
    "title": "Statistics for Psychologists",
    "section": "5 Further tasks",
    "text": "5 Further tasks\nWell done for completing yet another new test! There are not a lot of additional tasks this week. Some things you may wish to try though…\n\nThe bar chart looks good, but the labels of the axes could, potentially, be improved. Rename the x-axis, y-axis and group labels to alternative names that still make sense given the study context.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\nmeat_plot %&gt;% #our dataset\n    ggplot(aes(x= time, # our X axis\n               y = meat, # our Y axis\n               fill = time)) + # How we will colour our seperate bars\n    geom_col(width = .7, position = position_dodge(.3)) + # width of columns and space between columns\n    scale_fill_manual(values = c(\"#D4D4D4\", \"#737373\", \"#323232\")) + # APA colours for bars\n    scale_y_continuous(expand = expansion(0), # trick to remove space below 0 on y axis\n                       limits = c(0, 7),\n                       breaks = seq(0, 7, 1)) + # set limit of Y axis to 7\n  xlab(\"STUDENT CAN CHOOSE XLABEL\") + #rename the x axis label\n  ylab(\"STUDENT CAN CHOOSE YLABEL\") + #rename the y axis label\n  scale_x_discrete(labels = c('one week prior','one week after','one month after')) + #rename the groups on x axis\n    theme(panel.background = element_blank(), # removing the background panel colour\n          axis.line = element_line(color = \"black\"), # creating black axes lines\n          axis.ticks.x = element_blank(),\n          legend.position = \"none\") + # removing the legend, which is redundant\n    geom_errorbar(aes(ymin=meat-se, ymax=meat+se, width = 0.2))\n\n\n\n\n\n\n\n\nTo practice with making an composite variable using an average score of multiple values, please return up to the text marked “Special note” which is just above “2.3 Reformatting the data” and try your hand at making a new average score variable from some existing variable. note, your data will need to be in the original wide format (i.e., the lab4_data), as opposed to long format (i.e., lab4_data_wide).\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\nlab4_data %&gt;% \n  mutate(meat_average = (rowmeans(a1_meat, a2_meat, a3_meat))) -&gt; lab4_data #create a new composite variable 'meat_average', which is the average amount of meat a participant consumed per week across the three time points - i.e., ((amount of meat consumed at time 1 + amount of meat consumed at time 2 + amount of meat consumed at time 3) divided by 3). Add this new composite as a variable to the pre-existing lab4_data\n\n#CHECK IT IS CORRECT AND NEW VARIABLE EXISTS\nprint(lab4_data)\n\n# A tibble: 50 × 5\n   participant a1_meat a2_meat a3_meat meat_average\n         &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;        &lt;dbl&gt;\n 1           1       6       3       6         5   \n 2           2       3       4       6         4.33\n 3           3       8       1       3         4   \n 4           4       6       5       5         5.33\n 5           5       9       0       0         3   \n 6           6       8       1       6         5   \n 7           7       9       2       2         4.33\n 8           8       5       4       7         5.33\n 9           9       7       4       1         4   \n10          10       8       6       3         5.67\n# ℹ 40 more rows\n\n\n\n\n\n\nIn the current lab, the normality assumption was violated for level a2 - the shame, oh the shame. Why not plot the a2 data using a histogram (see lab sheet week 2). Does the histogram make you feel more or less convinced that your decision to continue your analysis was justified?\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\n\n# Histogram for A2. We have 7 bins, because scores ranged from 0-6 [min-max] (see summary statistics of 2.3 as a reminder)\nlab4_data %&gt;%\n    ggplot(aes(x = a2_meat)) + \n    geom_histogram(bins = 7, fill = \"grey\")\n\n\n\n\n\n\n\n\nPlease chill, you’ve done incredible. See you next week and top work.\n\n\n\n\nTop work!"
  },
  {
    "objectID": "PSYC214/Week7.html",
    "href": "PSYC214/Week7.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Watch Part 1 here\nWatch Part 2 here\nWatch Part 3 here\nWatch Part 4 here\nWatch Part 5 here\nDownload the lecture slides here, and here for a larger version."
  },
  {
    "objectID": "PSYC214/Week7.html#learning-objectives",
    "href": "PSYC214/Week7.html#learning-objectives",
    "title": "Statistics for Psychologists",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this week’s lecture, we demonstrated the steps involved in performing a two-factor between-participants ANOVA by hand using the simplest possible instance of such a design, namely a 2 \\(\\times\\) 2 factorial design. We saw that the analysis entails between 1–2 stages. In the first stage, we perform the ANOVA to establish whether either of the two main effects or interaction is significant. If the interaction is not significant, then the analysis stops here. However, if there is a significant interaction, then we must proceed to a second stage and perform a simple main effects analysis to establish the nature of the significant interaction. In today’s lab session, we will demonstrate how to perform a 2 \\(\\times\\) 2 between-participants factorial ANOVA in R, including the calculation of the simple main effects. We will also demonstrate how to write up the results of a factorial ANOVA."
  },
  {
    "objectID": "PSYC214/Week7.html#getting-started",
    "href": "PSYC214/Week7.html#getting-started",
    "title": "Statistics for Psychologists",
    "section": "Getting Started",
    "text": "Getting Started\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n \nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\nOnce you are logged into the server, create a folder for today’s session. Navigate to the bottom right panel and under the Files option select the New Folder option. Name the new folder psyc214_lab_7. Please ensure that you spell this correctly otherwise when you set the directory using the command given below it will return an error.\nSe we can save this session on the server, click File on the top ribbon and select New project. Next, select existing directory and name the working directory ~/psyc214_lab_7 before selecting create project.\nFinally, open a script for executing today’s coding exercises. Navigate to the top left pane of RStudio, select File –&gt; New File –&gt; R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\n\nLet’s set our working directory:\n\nsetwd(\"~/psyc214_lab_7\")\n\nNow that you have created a folder for today’s session, it’s time to add the Week 7 data file. Download the data COVID19Data.csv. Next, in the RStudio Server open your new pscy214_lab_7 folder. When in the new folder, select the Upload tab. This will present a box that will ask where the data is that you want to upload. Click on Browse, find the COVID19Data.csv file on your desktop and click OK.\nBefore moving on, let’s load the relevant libraries that we will be using in today’s session.\n\nlibrary(\"tidyverse\")  # For data storage and manipulation\nlibrary(\"tidyr\")      # For tidy data\nlibrary(\"rstatix\")    # For descriptives statistics, outlier detection etc.\nlibrary(\"effectsize\") # For generating effect sizes for the ANOVA and simple main effects\nlibrary(\"ggpubr\")     # For generating QQ plots\nlibrary(\"phia\")       # For calculating simple main effects"
  },
  {
    "objectID": "PSYC214/Week7.html#analysing-the-hypothetical-covid-19-vaccination-study-data",
    "href": "PSYC214/Week7.html#analysing-the-hypothetical-covid-19-vaccination-study-data",
    "title": "Statistics for Psychologists",
    "section": "Analysing the Hypothetical COVID-19 Vaccination Study Data",
    "text": "Analysing the Hypothetical COVID-19 Vaccination Study Data\nToday, we are going to analyse the hypothetical COVID-19 vaccination study data that we plotted in last week’s lab session. As a reminder, the table below summarises the four different conditions of our 2 \\(\\times\\) 2 between-participant study.\n\nThe data set consists of the following four columns:\n\nPar: represents the participant number, which ranges from 1–200, with \\(N\\) = 50 participants in each of the four conditions resulting from the combination of our two factors, described next.\nFear: represents whether the participant indicated by the row received (Fear Appeal) or did not receive (No Fear Appeal) the fear appeal.\nEfficacy: represents whether the participant indicated by the row received (Efficacy Message) or did not receive (No Efficacy Message) the self-efficacy message.\nLikelihood: represents the vaccination likelihood score of the participant indicated by the row.\n\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as covid19Data. Once you have done this, you need to re-level the factors Fear and Efficacy so that “No Fear Appeal” is the baseline level for the former factor, and “No Efficacy Message” is the baseline level for the latter factor. We showed you how to do this in the Week 6 lab session.\n\n# *** ENTER YOUR OWN CODE HERE IMPORTING THE DATA AND RE-LEVELLING THE FACTORS ***\n\nOnce you have done the above, type (covid19Data) into the console to view the first 10 rows of the data. We want to check that the columns for Fear and Efficacy have the labels &lt;fct&gt; beneath them. This means that R has stored these variables as factors, which is necessary for us to perform the ANOVA. By default, R will import these variables as characters, in which case the variables would have the labels &lt;chr&gt; beneath them. If that is the case for your data, then you will need to modify this before proceeding further.\n\n\n# A tibble: 200 × 4\n     Par Fear           Efficacy            Likelihood\n   &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;                    &lt;dbl&gt;\n 1     1 No Fear Appeal No Efficacy Message          6\n 2     2 No Fear Appeal No Efficacy Message          4\n 3     3 No Fear Appeal No Efficacy Message          7\n 4     4 No Fear Appeal No Efficacy Message          8\n 5     5 No Fear Appeal No Efficacy Message          5\n 6     6 No Fear Appeal No Efficacy Message          6\n 7     7 No Fear Appeal No Efficacy Message          2\n 8     8 No Fear Appeal No Efficacy Message          6\n 9     9 No Fear Appeal No Efficacy Message          5\n10    10 No Fear Appeal No Efficacy Message          3\n# ℹ 190 more rows\n\n\n\nDescriptives statistics and assumption checks\nThe next step is to generate some descriptive statistics. We want the means, standard deviations, and confidence intervals for our four conditions. You have done this before, so you can generate these descriptive statistics for yourselves. When you invoke the group_by() function, remember to include the variables for both factors, (Fear, Efficacy) with a comma seperating them, as I have done here. Make sure you name your descriptive statistics descriptives.\n\n# *** ENTER YOUR OWN CODE FOR GENERATING THE DESCRIPTIVE STATISTICS ***\n\nOnce you have written and executed the code in RStudio, type descriptives in the console to view the descriptive statistics, it should yield the following output:\n\n\n            Fear            Efficacy   variable  n mean    sd    ci\n1 No Fear Appeal No Efficacy Message Likelihood 50 5.16 2.024 0.575\n2 No Fear Appeal    Efficacy Message Likelihood 50 4.66 1.869 0.531\n3    Fear Appeal No Efficacy Message Likelihood 50 4.80 1.829 0.520\n4    Fear Appeal    Efficacy Message Likelihood 50 7.12 1.769 0.503\n\n\nWe won’t linger on these descriptive statistics because we inspected them in last week’s lab session.\nThe next steps are for us to perform our usual checks to make sure that the data are fit for analysis using ANOVA.\nThe first check we are going to perform is to establish whether there are any outliers or extreme values in the data set. Again, you have done this on numerous occasions before, so it is over to you to generate your own code to identify if there are any outliers or extreme values. As per when you generated the descriptive statistic, remember we have two factors, so when you invoke group_by() you will need to include the arguments (Fear, Efficacy).\n\n# *** ENTER YOUR OWN CODE FOR CHECKING FOR OUTLIERS AND EXTREME VALUES ***\n\nIf you have entered your code correctly, then it should generate the following:\n\n\n# A tibble: 2 × 6\n  Fear           Efficacy              Par Likelihood is.outlier is.extreme\n  &lt;fct&gt;          &lt;fct&gt;               &lt;dbl&gt;      &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 No Fear Appeal No Efficacy Message    28         10 TRUE       FALSE     \n2 Fear Appeal    No Efficacy Message   126          0 TRUE       FALSE     \n\n\nThere are only two outliers in the data, participant 28 who received the combination of No Fear Appeal/No Efficacy Message and participant 126 who received the combination of Fear Appeal/No Efficacy Message. Fortunately, there are no extreme values in the data, so we are safe to proceed without having to remove or transform any scores.\nThe next check that we need to undertake is to ensure that the homogeneity of variance assumption has been met. As we have seen in previous lectures and labs, this is an important assumption for single-factor between-participants ANOVA, and the same is true for between-participants factorial ANOVA. Applied to the latter, this assumption states that the variances of the different conditions created through the combination of factors should not differ systematically from one another. For our 2 \\(\\times\\) 2 between-participants factorial design, this means that the variances for the four conditions created by the combination of our two factors should be roughly comparable.\nWe can test whether the data satisfy the homogeneity of variance assumption using Levene’s test, as we have done in previous lab classes. As you are familiar with the code for running Levene’s test, you can write this out for yourself. However, when you invoke Levene’s test using levene_test() you will need to enter (Likelihood ~ Fear * Efficacy) as the arguments. The argument Likelihood is our dependent measure, and the argument Fear * Efficacy tells the function that we want to run the test on the four conditions created through the combination of our two factors (that’s what the * is for; note you can also substitute the * with : which has the same meaning and yields the same result).\n\n# *** ENTER YOUR OWN CODE FOR RUNNING LEVENE'S TEST ***\n\nIf you have entered your code correctly, then it should return the following output:\n\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     3   196     0.231 0.875\n\n\nThe test result is nonsignificant (p = .875), which indicates that the variances of the four conditions do not differ appreciably from one another. Accordingly, based on Levene’s criterion, we can be satisfied that the homogeneity of variance assumption has been satisfied.\nThe next check we need to perform is to ensure that our data satisfy the normality assumption. We can do this by running a Shapiro Wilk test on the data for each of our four conditions to determine if the distribution of scores in each differs significantly from a normal distribution. Remember the rule for using this test—if there is less than \\(N\\) = 50 observations per condition, the Shapiro Wilk test is likely to yield an accurate test result, whereas if it is equal to or greater than this number, we are better off inspecting the QQ plots. Our sample size per condition is right on the threshold, so we will use both approaches to testing the normality assumption.\nYou have executed this test many times before, so once more it’s over to you to generate the code. Just remember once again that when invoking the function group_by you need to supply the arguments (Fear,Efficacy) given that we have two factors.\n\n# *** ENTER YOUR OWN CODE FOR RUNNING THE SHAPIRO WILK TEST ***\n\nIf you have executed your code correct, then you should see the following output:\n\n\n# A tibble: 4 × 5\n  Fear           Efficacy            variable   statistic       p\n  &lt;fct&gt;          &lt;fct&gt;               &lt;chr&gt;          &lt;dbl&gt;   &lt;dbl&gt;\n1 No Fear Appeal No Efficacy Message Likelihood     0.936 0.00971\n2 No Fear Appeal Efficacy Message    Likelihood     0.958 0.0752 \n3 Fear Appeal    No Efficacy Message Likelihood     0.956 0.0587 \n4 Fear Appeal    Efficacy Message    Likelihood     0.949 0.0313 \n\n\nTwo of the test results are nonsignificant, indicating that the distributions of scores in the No Fear Appeal/Efficacy Message condition and Fear Appeal/No Efficacy Message condition do not differ significantly from a normal distribution (p = .075 and \\(p\\) = .059, respectively). However, the other two test results are significant, indicating that the distributions of scores in the No Fear Appeal/No Efficacy Message condition and Fear Appeal/Efficacy Message condition do differ significantly from a normal distribution (p = .010 and \\(p\\) = .031, respectively).\nLet’s not panic just yet. Remember, we are right on the threshold of accuracy for the Shapiro Wilk test, so we should take a look at the QQ plots before taking action.\nI am going to assist with you with this next bit. We generate QQ plots (these have been described previously, most recently in the Week 5 lab session, so I won’t go into the details of what they plot and how to interpret them here again) using the ggqqplot() function. Previously, when you have used this function, you only had a single factor and you would have written something like this: ggqqplot(covid19Data, \"Likelihood\", facet.by = \"Fear\"), where Fear in this context is the one and only factor in the study. However, we have two factors in our study, so to get what we want we have to pass through our two factors to facet.by as a vector using c(\"Fear\", \"Efficacy\"). The complete code is as follows:\n\n# Specify covid19Data as the data; \"Likelihood\" as the dependent measure; and facet.by the \"Fear\" and \"Efficacy\" factors to create separate plots for the four different conditions.   \nggqqplot(covid19Data, \"Likelihood\", facet.by = c(\"Fear\", \"Efficacy\"))\n\n\n\n\nThe QQ plots look very reasonable. The only mildly concerning plot is in the top left-hand corner, which plots the distribution of scores for the No Fear Appeal/No Efficacy Message Condition. Several of the data points at the top end of the scale depart from the diagonal criterion line. I know from experience inspecting these plots that this is not a radical departure, so I’m happy to proceed without taking further action in this instance.\nAll in all, our data are looking in good shape, so we are ready to perform our ANOVA.\n\n\nRunning the ANOVA\nTo run our ANOVA, we are going to use the aov() function that Richard introduced you to (in your Week 2 lab session) that you used to run a single-factor between-participants ANOVA. We can also use this function to run a between-participants factorial ANOVA.\nLet’s suppose, for a moment, that we only had a single factor in our study, Fear. To run the ANOVA we could use: covidModel = aov(data = covid19Data, Likelihood ~ Fear). Here we are asking R to run an ANOVA in which the data is covid19Data (the first argument), the dependent measure is Likelihood (the second argument), and the factor is Fear (the third argument), and we are storing the results in a variable called covidModel — the value to the left of the = sign. The ~ sign after Likelihood tells R that we have declared our dependent measure, and what follows after this symbol are the factors for the analysis.\nHow do we extend this to a two-factor study? Intuitively, you might think we would write something like this: covidModel = aov(data = covid19Data, Likelihood ~ Fear + Efficacy). Now, after the ~, we have added both factors in our study Fear + Efficacy seperated by a + sign. This is part of what we need, but not all that we need. What this code would do is generate the two main effects for Fear and Efficacy, but it would not test the interaction. To test the interaction, we need to modify the code following ~ as follows: Fear + Efficacy + Fear * Efficacy. The new bit + Fear * Efficacy tells R that we want to test the interaction as well (that’s what the * between the two factors indicates; we can also use : in place of * which has the same meaning).\nThe complete piece of code we need to run our ANOVA is as follows:\n\n# Specify our ANOVA model (see the main text for details)\ncovidModel = aov(data = covid19Data, Likelihood ~ Fear + Efficacy + Fear * Efficacy)\n# This bit prompts R to produce our summary ANOVA table\nsummary(covidModel)\n\n               Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nFear            1   55.1   55.13   15.68 0.000105 ***\nEfficacy        1   41.4   41.40   11.78 0.000732 ***\nFear:Efficacy   1   99.4   99.41   28.27 2.86e-07 ***\nResiduals     196  689.2    3.52                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s take a look at the resulting ANOVA table. The first column corresponds to the source of the effect:\n\nFear corresponds to the main effect of Fear.\nEfficacy corresponds to the main effect of Efficacy.\nFear:Efficacy corresponds to the interaction between the two factors.\nResiduals corresponds to the error term used to test the above effects.\n\nThe second column gives us the degrees of freedom for each effect indicated by the row (DF), the third column gives us the Sum of Squares (Sum Sq), the fourth column gives us the Mean Square (Mean Sq), the fifth column gives us the F value (F value), and the final column gives us the p value associated with each effect (PR\\(&gt;\\)F). Let’s run some sanity checks, starting with the Mean Square. These values should be equal to the Sum of Squares divided by the degrees of freedom for the effect indicated by the row, which they are (check for yourself). The F values should be equal to the Mean Square divided by the error term (3.52) for the effect indicated by the row, which they are once again (check for yourself).\nWe can see from the final column that both the main effects and the interaction are significant (all p &lt; .001). To help quantify the size of these effects, let’s get some effect size estimates using the effectsize() function from the effectsize package that we have used previously:\n\neffectsize(covidModel)\n\n# Effect Size for ANOVA (Type I)\n\nParameter     | Eta2 (partial) |       95% CI\n---------------------------------------------\nFear          |           0.07 | [0.03, 1.00]\nEfficacy      |           0.06 | [0.02, 1.00]\nFear:Efficacy |           0.13 | [0.06, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe \\(\\eta^2\\) effect size estimates for the main effect of fear, efficacy, and the interaction are, .07, .06, and .13, respectively, corresponding to medium-, medium-, and large-sized effects (reminder: 0.01 = small effect; 0.06 = medium effect; 0.14 = large effect).\nBecause the two main effects only comprise two levels, these results are easy to interpret as they involve a straightfoward pairwise comparison. Looking at the descriptive statistics we requested earlier, we can see that vaccination likelihood scores are higher in the presence of a fear appeal than in the absence of a fear appeal — this is the significant main effect of fear. We can also see that vaccination likelihood scores are higher in the presence of a self-efficacy message than in the absence of a self-efficacy message — this is the significant main effect of efficacy.\nThe interaction is less straightforward to interpret. To identify the reason for the interaction between factors, we need to proceed to a second stage of analysis in which we test the simple main effects of each factor.\n\n\nGenerating the simple main effects\nRemember that the simple main effects break down the main effects into their component parts. That is, we examine the effect of each factor at each level of the other factor. In a 2 \\(\\times\\) 2 factorial design, each factor has two simple main effects.\nWe will start by looking at the two simple main effects of fear:\n\nWe want to know whether there is a significant effect of fear (no fear appeal \\(vs.\\) fear appeal) at the no efficacy message level of the efficacy factor.\nWe want to know whether there is a significant effect of fear (no fear appeal \\(vs.\\) fear appeal) at the efficacy message level of the efficacy factor.\n\nWe can test these simple main effects using the testInteractions() function in the phia package (phia stands for Post-Hoc Interaciton Analysis) using the following code:\n\n# Get the simple main effects of \"Fear\" at each level of the \"Efficacy\"  factor\nsmeFear = testInteractions(covidModel, fixed = \"Efficacy\", across = \"Fear\")\n\nWarning in rbind(deparse.level, ...): number of columns of result, 6, is not a\nmultiple of vector length 5 of arg 2\n\n# Print the results\n(smeFear)\n\nF Test: \nP-value adjustment method: holm\n                    Value      SE     Df Sum of Sq       F    Pr(&gt;F)    \nNo Efficacy Message  0.36   0.375   1.00      3.24  0.9214    0.3383    \n   Efficacy Message -2.46   0.375   1.00    151.29 43.0238 9.398e-10 ***\nResiduals                 196.000 689.22                                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLet’s clarify what we have just requested in the above code. The first argument we supplied to the function was our ANOVA model, covidModel. Next we specified which simple main effects we wanted to test via two further arguments to the function. Specifically, fixed = \"Efficacy tells the function that we want to fix (or hold constant) the level of the factor efficacy, and across = \"Fear\" tells the function that for each of these levels, we want to compare the difference between the no fear appeal and fear appeal levels of the fear factor. Notice that we have made a specific assignment to a variable called smeFear — the value before the = sign. The reason for that will become clear shortly.\nLet’s look at the output that has been produced. It’s basically another ANOVA table with our two simple main effects (No Efficacy Message and Efficacy Message, respectively) and also the error term (Residuals) from the initial ANOVA (remember from the Week 7 lecture that we use the same error term from the initial ANOVA to test the simple main effects — notice that Sum of Squares for Residuals is 689.22, which is the same as in the ANOVA table for our initial analysis).\nThe first row, shows the simple main effect of fear at the no efficacy message level of the efficacy factor. This simple main effect is nonsignificant (p = .338). Thus, in the absence of a self-efficacy message, the likelihood of vaccinating is unaffected by whether or not a person is a given a fear appeal.\nThe secod row, shows the simple main effect of fear at the efficacy message level of the efficacy factor. This simple main effect is significant (p &lt; .001). Looking at the descriptive statistics we generated earlier, we can see that this is because in the presence of a self-efficacy message vaccination likelihood scores are higher in the presence than in the absence of a fear appeal.\nWe can also request effect sizes for these simple main effects using the effectsize() function. This is why we saved our simple main effects to the variable smeFear. This allows us to get the effect sizes using the following bit of code:\n\neffectsize(smeFear)\n\nWarning: Some CIs could not be estimated due to non-finite F, df, or df_error\n  values.\n\n\n# Effect Size for ANOVA (Type I)\n\nParameter           | Eta2 (partial) |    95% CI\n------------------------------------------------\nNo Efficacy Message |                | [ , 1.00]\nEfficacy Message    |                | [ , 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe effect size for the first simple main effect, which was nonsignificant, is nonsurprisingly very small (\\(\\eta^2\\) = .00), whereas the effect size for the second simple main effect, which was significant, is large (\\(\\eta^2\\) = .18).\nNext, we want to look at the two simple main effects of efficacy:\n\nWe want to know whether there is a significant effect of efficacy (no efficacy message \\(vs.\\) efficacy message) at the no fear appeal level of the fear factor.\nWe want to know whether there is a significant effect of efficacy (no efficacy message \\(vs.\\) efficacy message) at the fear appeal level of the fear factor.\n\nWe can get these simple main effects using the following code:\n\n# Get the simple main effects of \"Efficacy\" at each level of the \"Fear\" factor\nsmeEfficacy = testInteractions(covidModel, fixed = \"Fear\", across = \"Efficacy\")\n\nWarning in rbind(deparse.level, ...): number of columns of result, 6, is not a\nmultiple of vector length 5 of arg 2\n\n(smeEfficacy)\n\nF Test: \nP-value adjustment method: holm\n               Value      SE     Df Sum of Sq       F    Pr(&gt;F)    \nNo Fear Appeal  0.50   0.375   1.00      6.25  1.7774     0.184    \n   Fear Appeal -2.32   0.375   1.00    134.56 38.2661 7.058e-09 ***\nResiduals            196.000 689.22                                \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThis time, to generate the simple main effects of interest we set the second argument to fixed = \"Fear which tells the function that we want to fix (or hold constant) the level of the factor fear, and we set the value of the third argument to across = \"Fear\" which tells the function that for each of those levels, we want to compare the difference between the no efficacy message and efficacy message levels of the efficacy factor. Notice that we have once again made a specific assignment to a variable called smeEfficacy — the value before the = sign, so we can get effect sizes for these simple main effects later.\nLet’s once again look at the output that has been produced.\nThe first row, shows the simple main effect of efficacy at the no fear appeal level of the fear factor. This simple main effect is nonsignificant (p = .184). Thus, in the absence of a fear appeal, the likelihood of vaccinating is unaffected by whether or not a person is given a self-efficacy message.\nThe secod row, shows the simple main effect of efficacy at the fear appeal level of the fear factor. This simple main effect is significant (p &lt; .001). Looking at the descriptive statistics we generated earlier, we can see that this is because in the presence of a fear appeal vaccination likelihood scores are higher in the presence than in the absence of a self-efficacy message.\nWe can get the effect sizes of these simple main effects as follows:\n\neffectsize(smeEfficacy)\n\nWarning: Some CIs could not be estimated due to non-finite F, df, or df_error\n  values.\n\n\n# Effect Size for ANOVA (Type I)\n\nParameter      | Eta2 (partial) |    95% CI\n-------------------------------------------\nNo Fear Appeal |                | [ , 1.00]\nFear Appeal    |                | [ , 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe effect size for the first simple main effect, which was nonsignificant, is nonsurprisingly very small (\\(\\eta^2\\) = .01), whereas the effect size for the second simple main effect, which was significant, is large (\\(\\eta^2\\) = .16).\n\n\nHow to write up the results\n\n\n\nFigure 1. Vaccination likelihood scores as a function of the fear and efficacy manipulations. Error bars represent 95% confidence intervals.\n\n\nFigure 1 shows vaccination likelihood scores as a function of the fear and efficacy manipulations. These data were subjected to a 2 (fear: no fear appeal \\(vs.\\) fear appeal) \\(\\times\\) 2 (efficacy: no efficacy message \\(vs.\\) efficacy message) between-participants Analysis of Variance. There was a significant main effect of fear, F(1, 196) = 15.68, p &lt; .001, \\(\\eta^2\\) = .07, with vaccination likelihood scores being higher in the presence than in the absence of a fear appeal, a significant main effect of efficacy, F(1, 196) = 11.78, p &lt; .001, \\(\\eta^2\\) = .06, with vaccination likelihood scores being higher in the presence than in the absence of a self-efficacy message, and a significant interaction between the two factors, F(1, 196) = 28.27, p &lt; .001, \\(\\eta^2\\) = .13.\nTo scrutinise the interaction, a simple main effects analysis was undertaken. In the absence of an efficacy message, there was no significant effect of fear, F(1, 196) = .92, p = .338, \\(\\eta^2\\) = .00, whereas in the presence of an efficacy message there was a significant effect of fear, F(1, 196) = 43.02, p &lt; .001, \\(\\eta^2\\) = .18, with vaccination likelihood scores being higher in the presence than in the absence of a fear appeal. Mirroring these results, in the absence of a fear appeal, there was no significant effect of efficacy, F(1, 196) = 1.78, p = .184, \\(\\eta^2\\) = .01, whereas in the presence of a fear appeal there was a significant effect of efficacy, F(1, 196) = 38.27, p &lt; .001, \\(\\eta^2\\) = .16, with vaccination likelihood scores being higher in the presence than in the absence of an efficacy message.\nHence, people receiving a fear appeal had a higher likelihood of vaccinating against COVID-19 than those that did not receive a fear appeal, but only when the fear appeal was combined with a self-efficacy enhancing message.\n\n\nAdditional tasks\nThe final task is for you to generate the line graph of the data shown in Figure 1 above. This was generated using the code we supplied in last week’s lab session. However, we are now using confidence intervals for the error bars, so you will need to modify this aspect of the code.\n\n# *** ENTER YOUR OWN CODE FOR GENERTING THE LINE GRAPH IN FIGURE 1 ***\n\n\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\nFinal remarks\nIn this lab session, we have analysed the simplest instance of a two-factor between-participants design—namely a 2 \\(\\times\\) 2 design. But what happens if one or both of the factors have more than two levels? In this case, the analysis is a bit more involved. If the main effect of a factor with three or more levels is significant, then we will need to perform follow-up tests (planned comparisons or post-hoc tests), just like we did for a one-factor between-participants ANOVA to interpret the significant main effect. Another implication is that, if the interaction is significant, the simple main effects for any factor with three or more levels will require follow-up tests. Suppose we have a 2 (factor A: Level A\\(_{1}\\) vs. Level A\\(_{2}\\)) \\(\\times\\) 3 (factor B: Level B\\(_{1}\\) vs. Level B\\(_{2}\\) vs. Level B\\(_{3}\\)) between participants factorial design. The three simple main effects of factor A will involve only pairwise comparisons (namely A\\(_{1}\\) vs. A\\(_{2}\\), at levels B\\(_{1}\\), B\\(_{2}\\), and B\\(_{3}\\)). However, the two simple main effects of factor B will encompass comparisons between three levels (namely B\\(_{1}\\) vs. B\\(_{2}\\) vs B\\(_{3}\\) at levels A\\(_{1}\\) and A\\(_{2}\\)). These latter simple main effects are like running one-factor ANOVA’s on factors with three levels—if the test result is significant, it only tells us that the means differ, but we need to perform follow-up tests to identify the location of those differences. We will consider this issue in more detail in the next lecture and lab class.\nFor now, well done! We’ll see you again next week.\nFYI: at the end of the week I will upload a version of these lab materials to the lab folder that contains the full working code for the analyses."
  },
  {
    "objectID": "PSYC214/Week2.html",
    "href": "PSYC214/Week2.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC214/Week2.html#general-introduction-to-lab-2",
    "href": "PSYC214/Week2.html#general-introduction-to-lab-2",
    "title": "Statistics for Psychologists",
    "section": "1 - General Introduction to Lab 2",
    "text": "1 - General Introduction to Lab 2\n“There is no education like adversity” | Benjamin Disraeli.\n\n\n\nBenjamin Disraeli\n\n\n\n\n1.1 Access to R Studio\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n \nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\n\n\n\n1.2 Loading the packages (dependencies) and loading the data set\nPerfect. If you are this far you’re in R Studio - the hot bed of statistical joys.\nSimilar to last week, we first need to access the dataset.\n\nStep 1. To access today’s dataset, we first just need to download it from here\nNow that you have the data saved on your machine, we next need to make a space on our own individual R Studio servers in which to house the data. Like last week, to do this, we first need to create a folder. On the server, please navigate to the bottom right panel (see figure below). Here, under the ‘files’ you will see the option to add ‘New Folder’. Click on this and name the new folder psyc214_lab_2 Note. This needs to be spelled correctly and we need to make sure we that we specify this is lab_2! We roll with the times, and lab_1 is now an artifact :)\nGreat. You have now created a brand, spanking new folder for week 2’s content. To upload today’s week2_robo_lab.csv file into this folder, please open your new psyc214_lab_2 folder. When in the new folder, select the ‘Upload’ tab (see figure below). This will present a box that will ask where the data is that you want to upload. Click on “Browse…”, find where you have the week2_robo_lab.csv data file on your computer and click ‘OK’\n\nTop work! The data is now chillaxing on your own R Studio server, ready to be called upon.\nSTOPPPPPPPPP!!!!! We’ll want to be able to save this session on our server: to do so please: click ‘File’ on the top ribbon –&gt; New project. Next, select existing directory and name the working directory ~/psyc214_lab_2 –&gt; hit ’create project\nThis will now create a project in this week’s R Studio Server lab 2 folder where it will save your progress! You can then return to this at a later date - likely close to the exam wink wink\nLike last week, it is important that you work through a script and not through the console. A script is a posh text editor that allows you to write, edit, annotate and save lines of of code. Navigate to the top left pane of RStudio, select File -&gt; New File -&gt; R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\nBe sure to save the script using the File -&gt; Save As….\nLet’s start by loading the packages (i.e., the dependencies) we’ll use for this session.\nPlease copy or type the following commands into R Studio to get these packages activated:\n\nlibrary(tidyverse) # allows pipes, arrang(), summary(), aov(), etc\nlibrary(rstatix) # allows group_by()\nlibrary(effectsize) # a neat little package that provides the effect size\nlibrary(Rmisc) # allows shorthand calculations of standard errors and confidence intervals\n\nMore on these packages later.\nData… Assemble! Ok - Let’s load the data and crack on with today’s work.\n\nFirst we need to set the working directory to ‘psyc214_lab_2’, i.e., tell R Studio the location in which today’s data sits, waiting to stand to attention . To recap, the working directory is the default location or folder on your computer or server by which R will read/save any files.\n\nThe working directory can be set with the following R code:\n\n  setwd(\"~/psyc214_lab_2\")\n\n\nGreat. Now we’re ready to load the dataset which we kindly housed on our server. To do this, please type the following code:\n\n\n lab2_data &lt;- read_csv(\"week2_robo_lab.csv\")\n\nwhere ‘lab2_data’ is the name we’ve assigned that R will recognise when it calls up our data, read_csv is the R command to pull up the data and “week2_robo_lab.csv” is the name of the data file stored on the server.\nNote. During the rest of this session, you will not need to refer to the original downloaded .csv data file. R has all the information stored under the ‘lab2_data’ variable. Further note, you could have called ‘lab2_data’ by pretty much any name you like… ‘my_data’, ‘robo_2’, ‘stat_attack’, etc. - the name is somewhat arbitrary. For the purpose of this lab session and for ease of read, ‘lab2_data’, is perhaps more suitable.\n\n\n\n\n\n\n## 2 - Today’s lab activities\n\n\n\n\nSignif. codes: 0 ‘’ 0.001 ’’ 0.01 ’’ 0.05 ‘.’ 0.1 ’ ’ 1 1 observation deleted due to missingness ``` :::\n\n\n{.r .cell-code} effectsize(Model_1) # we ask for an eta2 effect size for our model\n\n\n::: {.cell-output .cell-output-stderr}\n\n\n::: {.cell-output .cell-output-stdout} ``` # Effect Size for ANOVA\n\n\nParameter | Eta2 | 95% CI\n\n\n\nGroup | 0.56 | [0.50, 1.00]\n\nOne-sided CIs: upper bound fixed at [1.00].\n\n:::\n:::\n\n\nThis has provided us with a whole bunch of useful information! \nIt shows us the between group variability value (do you remember, the numerator value in our F ratio equation from lecture 2?), which R has named the Group Mean Square (a value here of 130.80). \n\nIt also provides the within group variability value - aka the error term (do you remember, the denominator in our F ratio equation from lecture 2), which R has named the Residuals mean square (a value here of 0.86)\n\nIt provides the crucial F-ratio statistic, here a value of 151.3. \n**note, like in lecture 2, you could have calculated this value yourselves by dividing the 130.80 numerator by the 0.86 denominator. Try it in the r console by typing: Note, the value will differ slightly because of rounding - i.e., the number of decimals R works with versus us**\n\n\n::: {.cell}\n\n```{.r .cell-code}\n130.80 / 0.86\n\n[1] 152.093\n\n:::\nThe output also shows the Eta-squared (η2) effect size. As a rule of thumb, η2 = 0.01 indicates a small effect; η2 = 0.06 indicates a medium effect; η2 = 0.14 indicates a large effect. Our eta-squared (η2) represents a very large effect - i.e., the effect of our experimental manipulation on the Likeability scores was very large indeed.\n\n\n\n3.2 Reporting the results of the one-factor between-participants ANOVA in APA format\nAll results should be written up in accordance with the American Psychological Association’s (APA) guidance. This ensures that your results are easy to interpret and that all the relevant information is present for those wishing to review/replicate your work.\nThe current results can be reported as following: “A one-factor between-participants ANOVA revealed that likeability scores were significantly different between our robot groups (Robot A M = 2.50, SD = 0.93; Robot B M = 4.50, SD = 1.01; Robot O M = 2.11, SD = 0.85), F(2,236) = 151.3, p &lt; .001, η2 = 0.56, 95% CI[0.50, 1.00].\nPlease note. While the ANOVA tells us that there are differences between groups, it doesn’t tell us specifically which groups differ from one another. For example do Group A and B statistically differ? A and O? B and O? The only way to know this definitively is to carry out posthoc/planned contrast tests. We won’t do this today, as this will be a key facet of Lecture 3 and Lab 3, but please be aware that without these supplementary analyses the above APA reporting is incomplete\n\n\n\n3.3 Creating an APA barchart\nWhen reporting the results of an ANOVA in an APA report or publication, it is typical to include a barchart, illustrating the average scores and error bars for each group.\nAs this is a formal chart, it is more sterile and less jazzy than the coloured graphs you have produced before. There are also important APA aesthetics which are required, such as grey colour scales, white background, black axes and text, etc. Before you fall asleep, let’s make one of these posh plots.\nPlease copy and paste the following code. Please pay attention to the #annotations, which provide detail as to what each line of code is doing:\n\nlab2_data %&gt;% #our dataset\n    ggplot(aes(x= Group, # specifying our X axis\n               y = Likeability, # specifying our Y axis\n               fill = Group)) + # How we will colour our separate bars\n    geom_col(width = .7, position = position_dodge(.3)) + # width of columns and space between columns\n    scale_fill_manual(values = c(\"#D4D4D4\", \"#737373\", \"#323232\")) + # APA colours for bars - yuck!\n    scale_y_continuous(expand = expansion(0), # trick to remove space below 0 on y axis\n                       limits = c(0, 7), # set limit of Y axis to 7\n                       breaks = seq(0, 7, 1)) + # set the breaks between yaxis points\n    theme(panel.background = element_blank(), # removing the default grey background panel\n          axis.line = element_line(color = \"black\"), # creating black axes lines\n          axis.ticks.x = element_blank(),\n          legend.position = \"none\") # removing the legend, which is redundant\n\nWarning: Removed 1 row containing missing values or values outside the scale range\n(`geom_col()`).\n\n\n\n\n\nThis looks great. But to make it APA publishable we need to add some error bars to our plot. To add things like error bars and 95% Confidence intervals, we first need work out the values of these summary statistics. Luckily, an amazing package called Rmisc will provide these values for us with a simple command.\nLet’s make a new dataset for plotting our graphs which will include these values - using the summarySE() function of Rmisc\n\nLike_plot &lt;- summarySE(lab2_data, measurevar=\"Likeability\", groupvars=c(\"Group\"), na.rm = TRUE) #Like_plot is the name of our plot data set and na.rm asks R Studio to ignore our missing data point when calculating there summaries\n\nNot let’s view this new data using head()\n\nhead(Like_plot)\n\n  Group  N Likeability        sd         se        ci\n1     A 80    2.500000 0.9277713 0.10372798 0.2064654\n2     B 80    4.500000 1.0063092 0.11250879 0.2239431\n3     O 79    2.113924 0.8471270 0.09530923 0.1897461\n\n\nYou can see it now gives us a summary of our Likeability data, including the Means, Standard Deviations, Standard Errors and confidence intervals.\nNow let’s add the Standard Error Bars to our graph. To do this, we repeat the same plotting code from above, but with two key differences. 1. We need to ensure that we change the dataset for the graph from lab2_data to Like_plot (our new summary data). 2. We add the function geom_errorbar() to specify that we would like to include error bars for our graph.\n\nLike_plot %&gt;% #our dataset\n    ggplot(aes(x= Group, # our X axis\n               y = Likeability, # our Y axis\n               fill = Group)) + # How we will colour our seperate bars\n    geom_col(width = .7, position = position_dodge(.3)) + # width of columns and space between columns\n    scale_fill_manual(values = c(\"#D4D4D4\", \"#737373\", \"#323232\")) + # APA colours for bars\n    scale_y_continuous(expand = expansion(0), # trick to remove space below 0 on y axis\n                       limits = c(0, 7),\n                       breaks = seq(0, 7, 1)) + # set limit of Y axis to 7\n    theme(panel.background = element_blank(), # removing the background panel colour\n          axis.line = element_line(color = \"black\"), # creating black axes lines\n          axis.ticks.x = element_blank(),\n          legend.position = \"none\") + # removing the legend, which is redundant\n    geom_errorbar(aes(ymin=Likeability-se, ymax=Likeability+se, width = 0.2))\n\n\n\n\nPerfect! Error bars accomplished."
  },
  {
    "objectID": "PSYC214/Week2.html#further-tasks",
    "href": "PSYC214/Week2.html#further-tasks",
    "title": "Statistics for Psychologists",
    "section": "4 Further tasks",
    "text": "4 Further tasks\n\nWe have seen that there are signficant differences between at least two groups when it comes to the Likeability scores. Now please eye ball the means and the barchart and try to predict which groups differed. Talk this over with a lab mate or instructor. We will test this with further analyses next week!\n\n\n#STUDENT COMPLETES\n\n\nWe have yet to look for statistical differences between in Psyc214 Scores between our groups. Run another one-factor between-participants ANOVA following the instructions above. This time, be sure to replace our previous DV Likeability with our other DV Scores.\n\n\n#ANSWER CODE\nModel_2 &lt;- aov(data = lab2_data, Score ~ Group)\nsummary(Model_2)#We ask for a summary of this model. This provides F statistic and P value\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup         2   1223   611.3   12.52 6.77e-06 ***\nResiduals   237  11571    48.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\neffectsize(Model_2) # we ask for an eta2 effect size for our model\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nWhen completely correctly, you should get the following output:\n\n\n             Df Sum Sq Mean Sq F value   Pr(&gt;F)    \nGroup         2   1223   611.3   12.52 6.77e-06 ***\nResiduals   237  11571    48.8                     \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nFor one-way between subjects designs, partial eta squared is equivalent\n  to eta squared. Returning eta squared.\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nGroup     | 0.10 | [0.04, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\n\nReport this second ANOVA to APA standards following example above (3.2)\n\n\n#STUDENT COMPLETES\n\n\nRepeat the steps 3.3 and again create an APA standard barchart - this time plotting our three different groups and their Psyc214 Scores. Please finish this off with error bars. *Hint for the error bars - you will need to create a new ‘Scores_plot’ sub data set using the summarySE() function.\n\n\nScores_plot &lt;- summarySE(lab2_data, measurevar=\"Score\", groupvars=c(\"Group\"), na.rm = TRUE) #Like_plot is the name of our plot data set and na.rm asks R Studio to ignore our missing data point when calculating there summaries\n\nScores_plot %&gt;% #our dataset\n    ggplot(aes(x= Group, # our X axis\n               y = Score, # our Y axis\n               fill = Group)) + # How we will colour our seperate bars\n    geom_col(width = .7, position = position_dodge(.3)) + # width of columns and space between columns\n    scale_fill_manual(values = c(\"#D4D4D4\", \"#737373\", \"#323232\")) + # APA colours for bars\n    scale_y_continuous(expand = expansion(0), # trick to remove space below 0 on y axis\n                       limits = c(0, 90),\n                       breaks = seq(0, 90, 5)) + # set limit of Y axis to 7\n    theme(panel.background = element_blank(), # removing the background panel colour\n          axis.line = element_line(color = \"black\"), # creating black axes lines\n          axis.ticks.x = element_blank(),\n          legend.position = \"none\") + # removing the legend, which is redundant\n    geom_errorbar(aes(ymin=Score-se, ymax=Score+se, width = 0.2))\n\n\n\n\n\nAgain, try to think which groups may significantly differ from one another by eyeballing the means and barchart information. We will confirm this all next week.\n\n\n#STUDENT COMPLETES\n\n\nBefore you finish, make sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\n\nPlease end your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\n…………………. Now breathe! You’ve rocked it!!!\n\n\n\n\nTop work!"
  },
  {
    "objectID": "PSYC214/Week1 2.html",
    "href": "PSYC214/Week1 2.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Watch the introduction: Lecture Part 1\n\nWatch Lecture Part 2\n\nWatch Lecture Part 3\n\nWatch Lecture Part 4\n\nDownload the lecture slides here"
  },
  {
    "objectID": "PSYC214/Week1 2.html#task-1---check-in-with-the-university-attendance-register",
    "href": "PSYC214/Week1 2.html#task-1---check-in-with-the-university-attendance-register",
    "title": "Statistics for Psychologists",
    "section": "Task 1 - check-in with the University attendance register",
    "text": "Task 1 - check-in with the University attendance register\nWhen you arrive, make sure you have checked-in to your Analysis session in the Levy lab. All students are required by the University to confirm attendance at taught session\nStaff will remind you of this in your class."
  },
  {
    "objectID": "PSYC214/Week1 2.html#task-2---getting-dicy",
    "href": "PSYC214/Week1 2.html#task-2---getting-dicy",
    "title": "Statistics for Psychologists",
    "section": "Task 2 - Getting dicy",
    "text": "Task 2 - Getting dicy\n\nHere’s a simple task for you to complete as a group around each of the workstations;\nYou will be given a pair of dice\n\nWorking in pairs, one person rolls both dice.\nAdd up the total on each of them and have someone record that total (if you don’t have some spare paper or a pen, use your computer)\nRepeat those steps 20 times.\nThen swap over your roles (the person rolling the dice, the person recording the outcome)\nOnce everyone at the workstation has had a turn at this, each person should attempt to work out (a) the mean and (b) the median of their dice roll total.\nCheck each others working, and discuss any differences or problems you have.\n\nAre all your answers the same? Why / why not? If not, are they very different or very similar?"
  },
  {
    "objectID": "PSYC214/Week1 2.html#task-3---using-rstudio",
    "href": "PSYC214/Week1 2.html#task-3---using-rstudio",
    "title": "Statistics for Psychologists",
    "section": "Task 3 - Using RStudio",
    "text": "Task 3 - Using RStudio\n\nIntroducing R Studio\nR and RStudio is the software that we will be using to explore and learn about analysis in your Psychology degree. It’s a computational engine: a very snazzy calculator that you should see as your friend and ally in the journey to understand and appreciate psychology. It sits alongside what we teach about the concepts and interpretation of statistical analysis.\nR is the core software, RStudio is the interface for interacting with it. Put another way, R is the engine, RStudio is the cockpit.\nLike even a simplest calculator, it just does what you ask (at least when you ask nicely!) but it requires the user to know what they want from it and to understand what it is telling you. A calculator can’t help a kid get the right answer to a multiplication problem if they don’t know the difference between multiplication and division and addition etc. And whilst a calculator is brilliant at doing the number crunching (and as a bonus, R Studio can help with turning the numbers into beautiful graphs and images too), even a calculator requires a thoughtful person to take the answers and make sensible interpretations from them.\nTherefore, we need to learn both about the concepts of statistical analysis on the one hand, and the processing of statistical information -through R- on the other. The lectures will provide the starting point and the direction for statistical concepts, whilst these analysis labs provide the more practical experiences in how to use R, and how to make R your ally. Over the next year, in these labs we will increasingly be using RStudio to focus on the latter, processing side, which will allow you to focus your energies on the conceptual side and its relevance for appreciating psychology.\n\n\nGetting started with RStudio\nFor Lancaster University Psychology Students in 2023, we will be learning about R Studio through a simple but powerful web server architecture. That is, through the power of the internet, you can access and use R Studio by logging into a free account that we have provided and we will maintain for your use.\n\nHere’s a little secret: There are several different ways to access RStudio. For example, you can download a copy of the software onto your computer, or use a Virtual Machine set up to run a copy. There’s nothing to stop you having your local copy, but please note - we can’t support your own version through lab classes. We’re using the web server to make sure everyone has the same, controlled experience.\n\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n \nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\n\n\nWhat does RStudio look like?\nWhen RStudio starts, it will look something like this: \nRStudio has three panels or windows: there are tabs for Console (taking up the left hand side), Environment (and History top right) , Current file (bottom right). You will also see a 4th window for a script or set of commands you develop, also (on the left hand side).\n\n\nLet’s get started!\nThe first thing we want to do in RStudio is to create a folder for this week so that we can put the relevant material there and keep it tidy.\nFrom the lower-right panel of RStudio, click the files tab.\nSelect the “new folder” option and create a new folder (eg “week 1”)\nClick on that folder to open it\nNext, we’ve prepared some instructions for RStudio to use - this is called a “script”. So we need to get this script into the server for you to explore and play with\n\nDownload the “zip” file by clicking this link\nFind the location of the file on your computer and check it is saved as a “.zip” file\nReturn to RStudio\nClick “Upload”\nClick choose file and find the file on your computer.\nSelect the file and click “Open”. Click “OK”\n\nYou should now see the files extracted in the directory.\nYou should now have the script available in RStudio.\nUse “Save…As” to create a new version of the script. By doing this, you’ll be able to have a “before” and “after” version of the script and can go back over the changes\nIn the script, select or highlight the first line of text, which is this one:\n\nRun your first ever R instruction!\n\n5 + 5\n\nand “run” this line. That tells RStudio to carry out the instruction.\nYou should see that in the console tab, RStudio calculates the answer to this incredibly hard maths challenge! (amazing huh? OK, maybe not *that* amazing…).\n\n\nModify your first ever R instruction!\nUse your imagination – add a new line to the script and ask a different simple arithmetic question of your own choosing! What happens?\n\n\nCalculate descriptive stats in R for the first time!\nIn this week’s analysis lecture, we looked at measures of central tendency and how to calculate them. So let’s get R to do these calculations also!\nFirst, we tell R about the data used in the lecture. We’ve already created the instruction that will do exactly this and it is in the script, so run this line from the script\n\nweek_1_lecture_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nThis creates an “object” called week_1_lecture_data. We can then perform calculations on this object. For example, we can find the mean by running the following command (use the script to do this)\n\nmean(week_1_lecture_data)\n\nCheck the answer is the same we found in the lecture (it should be 6!).\nNext, let’s ask for the median by running this line from the script:\n\nmedian(week_1_lecture_data)\n\nThis also should be the answer from the lecture (7)\nR doesn’t have a single corresponding command for the mode, but we can use the block of code in the script for this that starts and ends with the “getmode” text (there are 6 lines of text)\nThis is just a bit of clever jiggery-pokery that gets the mode. What does R say the mode is?\n\n\nYour challenge\nHow can you get RStudio to verify / check the dice calculations that you attempted earlier? Think about how you might solve this problem, on the basis of what we have covered so far.\nWe will discuss this in class and attempt to get RStudio to check your answers. In doing so, annotate the script (add notes for you - not RStudio) using the “#” command"
  },
  {
    "objectID": "PSYC214/Week1 2.html#before-you-finish",
    "href": "PSYC214/Week1 2.html#before-you-finish",
    "title": "Statistics for Psychologists",
    "section": "Before you finish",
    "text": "Before you finish\n\nMake sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\nEnd your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\nThere is a red “power” button near the top right of the R studio window (do ask for help if you can’t find it). It’s a good habit to get into to turn the session off\n\n\nExtra content for outside the lab class\n\nIn the Howell text book on statistics, there’s some R code on descriptive statistics. It is included in the script for you to look at and play with.\nin your own time and think about the following:\n\nIn R, “&lt;-” is the assignment operator as in the command we used:\n\nPSYC121_week_1_data &lt;- c(7,8,8,7,3,1,6,9,3,8)\n\nWe create the variable label on the left (Analysis_week1_data) and we give it those numbers on the right. The nameAnalysis_week1_data is largely arbitrary: try use a variable of your own naming (your own name?) instead - and then use that alternative name for the other commands.\n\nThroughout this year, we’ll use the convention of the “underscore” to separate words in labels (it_makes_them_easier_to_read than ifyoudidn’thaveanyspaces)"
  },
  {
    "objectID": "PSYC214/Week1 2.html#task-4-review-the-learnr-sample-practice-questions",
    "href": "PSYC214/Week1 2.html#task-4-review-the-learnr-sample-practice-questions",
    "title": "Statistics for Psychologists",
    "section": "Task 4 – Review the learnr sample / practice questions",
    "text": "Task 4 – Review the learnr sample / practice questions\nAfter every block of teaching in part-1 analysis (specifically, we mean in week 5, week 10, week 15 and week 20) there will be a class test. This will assess your knowledge and your understanding of the material that has been covered.\nThe class test will comprise a set of Multiple Choice Questions (and the set of questions will be different for each student, as the test will involve random selection from a larger pool) under timed conditions.\nIn order to help you get (a) a broad or basic feel for the sort of questions you might get in the class test (b) self-review your progress through the term, we will provide MCQs each week for you to attempt.\nSo these are for your benefit… you can take the questions when you choose to, and the learnr quiz will provide feedback on the answers your provide. Just bear in mind:\n\nWe place a set of questions at the end of the learnr pages so that you can attempt these at the end of each week, after you’ve completed lab activities, follow-up work, weekly Q&As etc. But it’s up to you when you answer the questions\nThese are meant as indicative questions. There’s no point in learning/ memorising these questions (they won’t be on the quiz!) and our advice is to reflect on how the teaching and content links to the sorts of questions that get posed."
  },
  {
    "objectID": "PSYC214/Week1 2.html#task-5-data-collection-exercise",
    "href": "PSYC214/Week1 2.html#task-5-data-collection-exercise",
    "title": "Statistics for Psychologists",
    "section": "Task 5 – Data collection exercise",
    "text": "Task 5 – Data collection exercise\nIn order to learn about psychology and data analysis techniques, we need data! Rather than rely too much on artificial data (certainly it is sometimes useful to say “Here are a bunch of numbers and this is what we can do with them” – think about the R Studio example for this week’s lab) for the most part, we prefer to draw on datasets that are a bit more engaging and meaningful that you have a stake in yourself! By using a common data set, that we can return to over the year, we can also build up familiarity and confidence in the data and remove a potential obstacle to thinking about the more important analysis part.\nSo a key task will be for everyone to have a go at taking our online survey, and contribute to a dataset that can be used throughout the year.\nThe survey runs by following this link"
  },
  {
    "objectID": "PSYC214/Week1 2.html#post---lab-recap-the-slides-we-used",
    "href": "PSYC214/Week1 2.html#post---lab-recap-the-slides-we-used",
    "title": "Statistics for Psychologists",
    "section": "Post - lab recap: The slides we used",
    "text": "Post - lab recap: The slides we used\nWant to see again the introduction slides that we used in the Levy lab? They are available here"
  },
  {
    "objectID": "PSYC214/Week9.html",
    "href": "PSYC214/Week9.html",
    "title": "9. Three-Factor ANOVA",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n–&gt;\n\n\n\n\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "PSYC214/Week9.html#learning-objectives",
    "href": "PSYC214/Week9.html#learning-objectives",
    "title": "Statistics for Psychologists",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this week’s lecture, we introduced the procedures involved in interpreting a three-factor ANOVA. Specifically, what to do in the event that the three-way interaction is significant. We saw that the simplest strategy in this instance is to re-analyse the data as a series of two-factor ANOVAs. In today’s lab session, we will demonstrate how to perform a three-factor fully within-participants and mixed ANOVA in R (using the two hypothetical data sets presented in the lecture), and how to analyze a three-way interaction using the procedures described in the lecture. In this lab session, I am also going to show you a better way of rounding the values in dataframes than the options(digits = ) command used in earlier lab sessions.\nIf you get stuck at any point, be proactive and ask for help from one of the GTAs."
  },
  {
    "objectID": "PSYC214/Week9.html#getting-started",
    "href": "PSYC214/Week9.html#getting-started",
    "title": "Statistics for Psychologists",
    "section": "Getting Started",
    "text": "Getting Started\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n \nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\nOnce you are logged into the server, create a folder for today’s session. Navigate to the bottom right panel and under the Files option select the New Folder option. Name the new folder psyc214_lab_9. Please ensure that you spell this correctly otherwise when you set the directory using the command given below it will return an error.\nSe we can save this session on the server, click File on the top ribbon and select New project. Next, select existing directory and name the working directory ~/psyc214_lab_9 before selecting create project.\nFinally, open a script for executing today’s coding exercises. Navigate to the top left pane of RStudio, select File -&gt; New File -&gt; R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\n\nLet’s set our working directory:\n\nsetwd(\"~/psyc214_lab_9\")\n\nNow that you have created a folder for today’s session, it’s time to add the Week 9 files. Download the files and data needed from here and upload to the folder psyc214_lab_9.\nBefore moving on, let’s load the relevant libraries that we will be using in today’s session.\n\nlibrary(\"tidyverse\")  # For data storage and manipulation\nlibrary(\"tidyr\")      # For tidy data\nlibrary(\"rstatix\")    # For descriptives statistics, outlier detection, running the ANOVAs etc.\nsource(\"simple.R\")    # Custom function for generating the simple main effects"
  },
  {
    "objectID": "PSYC214/Week9.html#analysing-the-hypothetical-data-for-the-memory-and-context-study",
    "href": "PSYC214/Week9.html#analysing-the-hypothetical-data-for-the-memory-and-context-study",
    "title": "Statistics for Psychologists",
    "section": "Analysing the hypothetical data for the memory and context study",
    "text": "Analysing the hypothetical data for the memory and context study\nA memory researcher wants to know if memory is better when material is tested in the same context it was learned in. The researcher also wants to know whether recall and recognition memory are equally context dependent. The researcher manipulates three factors in a 2 \\(\\times\\) 2 \\(\\times\\) 2 fully within-participants design:\n\nmemory task (recall vs. recognition)\nlearning context (learn underwater vs. learn land)\ntesting context (test underwater vs. test land)\n\nParticipants are given words to remember in a specific learning context (either under water or on land) and are then tested in either the same context (e.g., under water if the words were learned under water) or a different context (e.g., on land if the words were learned under water). Memory is tested using a recall procedure (by asking participants to recall the studied words) or a recognition procedure (by presenting participants with a list of words and asking them to indicate which they had studied previously). The dependent measure is the number of words remembered correctly.\nThe data set contains the following variables:\n\nParticipant: represents the participant number, which ranges from 1–5.\nRecall_Under_Under: the number of words recalled correctly when material was learned under water and tested under water.\nRecall_Under_Land: the number of words recalled correctly when material was learned under water and tested on land.\nRecall_Land_Under: the number of words recalled correctly when material was learned on land and tested under water.\nRecall_Land_Land: the number of words recalled correctly when material was learned on land and tested on land.\nRecognition_Under_Under: the number of words recognised correctly when material was learned under water and tested under water.\nRecognition_Under_Land: the number of words recognised correctly when material was learned under water and tested on land.\nRecognition_Land_Under: the number of words recognised correctly when material was learned on land and tested under water.\nRecognition_Land_Land: the number of words recognised correctly when material was learned on land and tested on land.\n\n\nImport data, set variables as factors, and generate descriptive statistics\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as memoryContext.\n\n# *** ENTER YOUR OWN CODE HERE TO IMPORT THE DATA ***\n\n\n\n# A tibble: 5 × 9\n  Participant Recall_Under_Under Recall_Under_Land Recall_Land_Under\n        &lt;dbl&gt;              &lt;dbl&gt;             &lt;dbl&gt;             &lt;dbl&gt;\n1           1                  8                 5                 3\n2           2                  9                 6                 3\n3           3                  7                 5                 4\n4           4                  8                 4                 4\n5           5                  6                 3                 3\n# ℹ 5 more variables: Recall_Land_Land &lt;dbl&gt;, Recognition_Under_Under &lt;dbl&gt;,\n#   Recognition_Under_Land &lt;dbl&gt;, Recognition_Land_Under &lt;dbl&gt;,\n#   Recognition_Land_Land &lt;dbl&gt;\n\n\nThe next thing we need to do is convert our data from wide format into long format. The first thing we need to do is group the columns Recall_Under_Under through to Recognition_Land_Land into a new variable called Group using the gather() function:\n\n# Gather factors into a single column\nmemoryContextLong = memoryContext %&gt;%\n  gather(Group,Accuracy,Recall_Under_Under:Recognition_Land_Land,factor_key = TRUE)\n(memoryContextLong)\n\n# A tibble: 40 × 3\n   Participant Group              Accuracy\n         &lt;dbl&gt; &lt;fct&gt;                 &lt;dbl&gt;\n 1           1 Recall_Under_Under        8\n 2           2 Recall_Under_Under        9\n 3           3 Recall_Under_Under        7\n 4           4 Recall_Under_Under        8\n 5           5 Recall_Under_Under        6\n 6           1 Recall_Under_Land         5\n 7           2 Recall_Under_Land         6\n 8           3 Recall_Under_Land         5\n 9           4 Recall_Under_Land         4\n10           5 Recall_Under_Land         3\n# ℹ 30 more rows\n\n\nThis function was explained in the previous lab session, so if it is not clear what is going on here, check the Week 8 lab session materials.\nLooking at the new data frame we have created, we can see that it is not exactly what we want. Our new variable Group actually contains three independent variables. What we want is to separate these independent variables into three separate columns: MemoryTask, LearningContext, and TestingContext. We can do that with the separate() function:\n\n# Now separate the variable \"Group\" into separate columns for each factor\nmemoryContextLongSep = memoryContextLong %&gt;%\n  separate(Group, c(\"MemoryTask\",\"LearningContext\",\"TestingContext\"))\n(memoryContextLongSep)\n\n# A tibble: 40 × 5\n   Participant MemoryTask LearningContext TestingContext Accuracy\n         &lt;dbl&gt; &lt;chr&gt;      &lt;chr&gt;           &lt;chr&gt;             &lt;dbl&gt;\n 1           1 Recall     Under           Under                 8\n 2           2 Recall     Under           Under                 9\n 3           3 Recall     Under           Under                 7\n 4           4 Recall     Under           Under                 8\n 5           5 Recall     Under           Under                 6\n 6           1 Recall     Under           Land                  5\n 7           2 Recall     Under           Land                  6\n 8           3 Recall     Under           Land                  5\n 9           4 Recall     Under           Land                  4\n10           5 Recall     Under           Land                  3\n# ℹ 30 more rows\n\n\nAgain, this function was explained in the previous lab session, so if it is not clear what is going on here, check the Week 8 lab session materials. The latest version of the data set is named memoryContextLongSep, so make sure you use this from henceforth.\nThe next thing we need to do is convert the variables Participant, MemoryTask, LearningContext, and TestingContext into factors and re-order the levels of the latter two variables:\n\n# Make sure all necessary variables are coded as factors -- re-order the levels of \"LearningContext\" and \"TestingContext\"\nmemoryContextLongSep$Participant = factor(memoryContextLongSep$Participant)\nmemoryContextLongSep$MemoryTask = factor(memoryContextLongSep$MemoryTask)\nmemoryContextLongSep$LearningContext = factor(memoryContextLongSep$LearningContext,levels = c(\"Under\",\"Land\"))\nmemoryContextLongSep$TestingContext = factor(memoryContextLongSep$TestingContext,levels = c(\"Under\",\"Land\"))\n\nNext, we will generate some descriptive statistics (mean and standard deviation):\n\n# Get descriptive statistics\ndescriptives = memoryContextLongSep %&gt;%\n  # Organise the output by the \"MemoryTask\", \"LearningContext\", and \"TestingContext\" factors\n  group_by(MemoryTask, LearningContext, TestingContext) %&gt;%\n  # Request means, standard deviations, and confidence intervals\n  get_summary_stats(Accuracy, show = c(\"mean\", \"sd\"))\n  # Round the statistics to two decimal places\n  descriptives$mean = round(descriptives$mean, 2)\n  descriptives$sd = round(descriptives$sd, 2)\n  # Print the results\n  print.data.frame(descriptives)\n\n   MemoryTask LearningContext TestingContext variable n mean   sd\n1      Recall           Under          Under Accuracy 5  7.6 1.14\n2      Recall           Under           Land Accuracy 5  4.6 1.14\n3      Recall            Land          Under Accuracy 5  3.4 0.55\n4      Recall            Land           Land Accuracy 5  6.8 1.30\n5 Recognition           Under          Under Accuracy 5  6.0 1.00\n6 Recognition           Under           Land Accuracy 5  5.4 1.14\n7 Recognition            Land          Under Accuracy 5  5.8 0.84\n8 Recognition            Land           Land Accuracy 5  5.8 1.48\n\n\nNotice the code we have added to round the descriptive statistics to two-decimal places. We use the function round(), specifying how many decimal places we want to round the values (in this case 2). Inside this function, we place the variable we want to be rounded. Because our data are in a dataframe, we need to specify the name of our dataframe followed by a dollar sign and the name of the variable in the dataframe to be rounded (e.g., descriptives$mean tells R to round the values of the variable mean in the dataframe descriptives). We also need to re-assign the rounded values to the dataframe, so that the variable gets updated (e.g., that’s what the descriptives$mean = bit does).\nNotice also that our standard deviations have been reported to two decimal places, but our means haven’t. Why so? This is because the numbers after the first decimal place in this instance are all zeros, so R doesn’t report them. Bearing in mind that APA style requires we report descriptive statistics to two-decimal places, we would just add a single zero to each of the means at the second decimal place. For example, the first mean in the table, 7.6, would be reported as 7.60.\nAt this stage, we would ordinarily perform various checks including identifying possible outliers and checking that our data satisfy the normality assumption. However, as per last week, time is limited, so we won’t perform those checks today (just remember that ordinarily you should not skip this part!). One assumption that is important in within-participants designs is the sphericity assumption, but remember that this only applies to designs with within-participants factors with three or more levels. All of our factors have two levels, so this assumption is not relevant in this instance (it’s also not relevant for our second data set that we analyse later, for which the within-participants factors also only comprise two levels).\n\n\nRunning the ANOVA, follow-up ANOVAs, and simple main effects\nTo run our ANOVA, we are going to use the anova_test function from the rstatix package. This is the same function that we used in the Week 8 lab session to analyse two-factor fully within-participants and mixed designs. The code required to run the ANOVA is given below:\n\n# Create the fully within-participants design ANOVA model\nmemoryContextModel = anova_test(data = memoryContextLongSep, dv = Accuracy, wid = Participant, within = c(MemoryTask, LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nmemoryContextModel$p = round(memoryContextModel$p, 3)\n# Print the model summary\n(memoryContextModel)\n\nANOVA Table (type III tests)\n\n                                     Effect DFn DFd      SSn  SSd       F     p\n1                               (Intercept)   1   4 1288.225 10.9 472.743 0.000\n2                                MemoryTask   1   4    0.225  0.9   1.000 0.374\n3                           LearningContext   1   4    2.025  5.1   1.588 0.276\n4                            TestingContext   1   4    0.025  7.1   0.014 0.911\n5                MemoryTask:LearningContext   1   4    3.025  4.1   2.951 0.161\n6                 MemoryTask:TestingContext   1   4    0.625  3.5   0.714 0.446\n7            LearningContext:TestingContext   1   4   30.625  4.5  27.222 0.006\n8 MemoryTask:LearningContext:TestingContext   1   4   21.025  3.1  27.129 0.006\n  p&lt;.05      ges\n1     * 0.970000\n2       0.006000\n3       0.049000\n4       0.000637\n5       0.072000\n6       0.016000\n7     * 0.439000\n8     * 0.349000\n\n\nTo create the model, the first argument we supplied to anova_test was the name of our data, memoryContextLongSep. The second argument we supplied was our dependent variable, Accuracy. The third argument we supplied was Participant, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our within-participants factors, MemoryTask, LearningContext, and TestingContext.\nAs we saw in last week’s lab session, the resulting ANOVA table is different in format to those given in the lecture, which follow a more conventional style. In the ANOVA tables given in the lecture, each outcome (each main effect and interaction) is given on a separate row, with the error term used to test it given in the row directly beneath it. However, anova_test gives each outcome and its associated error term all in the same row. Specifically, the row corresponding to each outcome contains the between-group degrees of freedom (DFn), the error degrees of freedom (DFd), the between-group sums of squares (SSn), the error sums of squares (SSd), the \\(F\\) ratio (F), the p (p) value, and the generalised eta squared (ges) value (a measure of effect size). What anova_test does not give us is the between-group mean squares and the error mean squares that are used to calculate the \\(F\\) ratios. However, I showed you how to calculate these in last week’s lab session if you should ever have need for these (you probably won’t).\nYou might be wondering why anova_test has not given us Mauchly’s test of sphericity and the Greenhouse-Geisser correction. This is because all of our factors have only two levels, so the sphericity assumption does not apply. Remember, the anova_test function only generates these tests and corrections when at least one of the within-participants factors has three or more levels.\nInspecting the ANOVA table, rows two to four give the main effects of Memory Task, Learning Context, and Testing Context; rows five to seven give the Memory Task \\(\\times\\) Learning Context, Memory Task \\(\\times\\) Testing Context, and Learning Context \\(\\times\\) Testing Context two-way interactions; and row eight gives the Memory Task \\(\\times\\) Learning Context \\(\\times\\) Testing Context three-way interaction. Looking at the p values, we can see that there is a significant Learning Context \\(\\times\\) Testing Context two-way interaction, \\(p\\) = .006, and a significant Memory Task \\(\\times\\) Learning Context \\(\\times\\) Testing Context three-way interaction, \\(p\\) = .006.\nBecause the three-way interaction is significant, we need to analyse it further. As explained in the lecture, a significant three-way interaction occurs when there are different two-way interactions between two of the factors according to the levels of the third factor. The simplest way to analyse a signiﬁcant three-way interaction is to re-analyse it as a series of two-factor ANOVAs. To do this, we first need to decide on a factor that we are going to split the analyses by. We can pick any factor we want, but there is usually one factor that stands out as being an obvious choice and in our case it is the memory task factor. So, what we need to do is to perform two, two-factor ANOVAs:\n\na 2 (learning context: learn under water vs. learn land) \\(\\times\\) 2 (testing context: test under water vs. test land) ANOVA for the recall memory test condition only.\na 2 (learning context: learn under water vs. learn land) \\(\\times\\) 2 (testing context: test under water vs. test land) ANOVA for the recognition memory test condition only.\n\nWe will start by running the two-factor ANOVA for the recall memory test condition (ignoring the recognition memory test condition). To do this, we first need to produce a filtered version of our data set called recallOnly that only includes the results for the recall memory test condition. We can create that with the following piece of code:\n\n# Get the data for the \"Recall\" condition only\nrecallOnly = memoryContextLongSep %&gt;%\n  filter(MemoryTask == \"Recall\") \n\nThe command filter(MemoryTask == \"Recall\") tells R that we only want the data for the recall condition of the Memory Task factor.\nNext, we can run our two-factor ANOVA on this filtered data set. The steps are the same as above, except that we need to drop the Memory Task factor included previously (remember, we are only analysing the recall condition of the Memory Task factor).\n\n# Run the two-factor ANOVA for the \"Recall\" condition only\nrecallModel = anova_test(data = recallOnly, dv = Accuracy, wid = Participant, within = c(LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nrecallModel$p = round(recallModel$p, 3)\n# Print the model summary\n(recallModel)\n\nANOVA Table (type III tests)\n\n                          Effect DFn DFd   SSn SSd       F     p p&lt;.05   ges\n1                    (Intercept)   1   4 627.2 5.3 473.358 0.000     * 0.971\n2                LearningContext   1   4   5.0 5.5   3.636 0.129       0.214\n3                 TestingContext   1   4   0.2 4.3   0.186 0.688       0.011\n4 LearningContext:TestingContext   1   4  51.2 3.3  62.061 0.001     * 0.736\n\n\nThe only thing we are interested in from the ANOVA table is the outcome of the two-way interaction between Learning Context and Testing Context; you can ignore everything else. You can see that the interaction is significant, p = .001, so the next step is to perform a simple main effects analysis to identify the nature of the interaction.\nThe procedure for performing the simple main effects analysis is the same as I demonstrated to you in our Week 8 lab session. That is, we use the pooled error terms approach, which means that the simple main effects of each factor are calculated using the same error term that was used to test the main effect of that factor in the ANOVA that preceded the simple main effects analysis (in this case, our two-factor ANOVA on the recall data only).\nBefore we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n# Get the recall ANOVA table\nrecallAnovaTable = get_anova_table(recallModel)\n\nNext, we need to calculate the cell totals for each of the four conditions and the number of observations (i.e., scores) in each cell:\n\n# Get cell totals and counts\nrecallCellTotals = recallOnly %&gt;%\n  # Organise the output by the \"LearningContext\" and \"TestingContext\" factors\n  group_by(LearningContext, TestingContext) %&gt;%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(Accuracy),n = n())\n  # Print the results\n  (recallCellTotals)\n\n# A tibble: 4 × 4\n# Groups:   LearningContext [2]\n  LearningContext TestingContext   sum     n\n  &lt;fct&gt;           &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Under           Under             38     5\n2 Under           Land              23     5\n3 Land            Under             17     5\n4 Land            Land              34     5\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Learning Context at Testing Context. This means, we are going to:\n\nTest the difference between learning under water and learning on land when tested under water only.\nTest the difference between learning under water and learning on land when tested on land only.\n\nTo do this, we need to declare Learning Context as the “fixed” factor (we are always comparing learning under water and learning on land) and Testing Context as the “across” factor (the comparison between learning under water and learning on land occurs “across” the test under water and test on land levels of the Testing Context factor):\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"LearningContext\"\nacross = \"TestingContext\"\n\nWe then generate the simple main effects of Learning Context by passing these variables into simple():\n\n# Simple main effects of \"Learning Context\" at \"TestingContext\"\nsmeLearningContext = simple(recallCellTotals,recallAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeLearningContext$P = round(smeLearningContext$P, 3)\n(smeLearningContext)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square        F     P\n1      Under           44.1                  1      44.100 32.07273 0.005\n2       Land           12.1                  1      12.100  8.80000 0.041\n3 Error term            5.5                  4       1.375  0.00000 0.000\n\n\nWe can see that there is a significant simple main effect of Learning Context at test under water, \\(p\\) = .005; when tested under water, recall memory scores are higher when the material was learned under water than when it was learned on land. There is also a significant simple main effect of Learning Context at test on land, \\(p\\) = .041; when tested on land, recall memory scores are higher when the material was learned on land than when it was learned under water. You will need to consult the descriptive statistics to verify this is correct.\nNext, we are going to calculate the simple main effects of the factor Testing Context at Learning Context. This means, we are going to:\n\nTest the difference between testing under water and testing on land when material was learned under water only.\nTest the difference between testing under water and testing on land when material was learned on land only.\n\nTo do this, we now need to declare Testing Context as the “fixed” factor and Learning Context as the “across” factor:\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"TestingContext\"\nacross = \"LearningContext\"\n\nWe then generate the simple main effects of Testing Context with the following:\n\n# Simple main effects of \"Testing Context\" at \"LearningContext\"\nsmeTestingContext = simple(recallCellTotals,recallAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeTestingContext$P = round(smeTestingContext$P, 3)\n(smeTestingContext)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square        F     P\n1      Under           22.5                  1      22.500 20.93023 0.010\n2       Land           28.9                  1      28.900 26.88372 0.007\n3 Error term            4.3                  4       1.075  0.00000 0.000\n\n\nWe can see that there is a significant simple main effect of Testing Context at learn under water, \\(p\\) = .010; when the material is learned under water, recall memory scores are higher when tested under water than when tested on land. There is also a significant simple main effect of Testing Context at learn land, \\(p\\) = .007; when the material is learned on land, recall memory scores are higher when tested on land than when tested under water.\nIn sum, from the simple main effects analysis what we can see is that recall memory is context sensitive; that is, recall memory performance is better when people are tested in the same context that they learned the information than when they are tested in a different context to that which they learned the information.\nWhat about recognition memory?\nThat brings us to our second two-factor ANOVA. For this, we now need to produce a filtered version of our data set called recognitionOnly that only includes the results for the recognition memory task condition. We can create that with the following piece of code:\n\n# Get the data for the \"Recognition\" condition only\nrecognitionOnly = memoryContextLongSep %&gt;%\n  filter(MemoryTask == \"Recognition\") \n\nThe command filter(MemoryTask == \"Recognition\") tells R that we only want the data for the recognition condition of the Memory Task factor.\nNext, we can run our two-factor ANOVA on this filtered data set.\n\n# Run the two-factor ANOVA for the \"Recognition\" condition only\nrecognitionModel = anova_test(data = recognitionOnly, dv = Accuracy, wid = Participant, within = c(LearningContext, TestingContext), detailed = TRUE)\n# Round the p values to three decimal places\nrecognitionModel$p = round(recognitionModel$p, 3)\n# Print the model summary\n(recognitionModel)\n\nANOVA Table (type III tests)\n\n                          Effect DFn DFd    SSn SSd       F     p p&lt;.05   ges\n1                    (Intercept)   1   4 661.25 6.5 406.923 0.000     * 0.970\n2                LearningContext   1   4   0.05 3.7   0.054 0.828       0.002\n3                 TestingContext   1   4   0.45 6.3   0.286 0.621       0.021\n4 LearningContext:TestingContext   1   4   0.45 4.3   0.419 0.553       0.021\n\n\nThe key result is that this time the critical two-way interaction is nonsignificant, p = 0.553. What this indicates is that, unlike recall memory, recognition memory is not context sensitive. This is the reason for the three-way interaction; recall memory is sensitive to the learning and testing context, whereas recognition memory is apparently insensitive to the learning and testing context.\n\n\nWriting up the results\n\n\n\nFigure 1. Memory scores as a function of learning context and testing context for the recall memory task (left panel) and the recognition memory task (right panel).\n\n\nFigure 1 shows memory scores as a function of learning context and testing context for the recall and recognition memory tasks. These data were subjected to a 2 (memory task: recall vs. recognition) \\(\\times\\) 2 (learning context: learn under water vs. learn land) \\(\\times\\) 2 (testing context: test under water vs. test land) within-participants ANOVA. There was no significant main effect of memory task, F(1, 4) = 1.00, p = .374, no significant main effect of learning context, F(1, 4) = 1.59, p = .276, and no significant main effect of testing context, F(1, 4) = 0.01, p = .911. Neither the memory task \\(\\times\\) learning context interaction, F(1, 4) = 2.95, p = .161, nor the memory task \\(\\times\\) testing context interaction, F(1, 4) = 0.71, p = .446, were significant. However, the learning context \\(\\times\\) testing context interaction was significant, F(1, 4) = 27.22, p = .006. Critically, there was also a significant three-way interaction, F(1,4) = 27.13, p = .006.\nTo explore the three-way interaction, two 2 (learning context) \\(\\times\\) 2 (testing context) ANOVAs were conducted; one using the data for the recall memory task only, and the second using the data for the recognition memory task only. For the first ANOVA on the recall memory task data, there was a significant interaction, F(1, 4) = 62.06, p = .001. A simple main effects analysis revealed that when tested under water, recall memory was better when the material was learned under water than when it was learned on land, F(1, 4) = 44.10, p = .005, and when tested on land, recall memory was better when the material was learned on land than when it was learned under water, F(1, 4) = 12.10, p = .041. Mirroring these results, when the material was learned under water, recall memory was better when tested under water than when tested on land, F(1, 4) = 22.50, p = .010, and when the material was learned on land, recall memory was better when tested on land than when tested under water, F(1, 4) = 28.90, p = .007.\nFor the second ANOVA on the recognition memory task data, there was no significant interaction, F(1, 4) = 0.42, p = .553.\nIn brief, the three-way interaction reflects the fact that recall memory is sensitive to the learning and testing context, whereas recognition memory is apparently not sensitive to such contextual factors."
  },
  {
    "objectID": "PSYC214/Week9.html#analysing-the-hypothetical-data-for-the-word-pronunciation-study",
    "href": "PSYC214/Week9.html#analysing-the-hypothetical-data-for-the-word-pronunciation-study",
    "title": "Statistics for Psychologists",
    "section": "Analysing the hypothetical data for the word pronunciation study",
    "text": "Analysing the hypothetical data for the word pronunciation study\nA researcher wants to investigate the development in children’s ability to pronounce regular and irregular words. The researcher adopts a 2 \\(\\times\\) 2 \\(\\times\\) 2 mixed design:\n\nage (7 years old vs. 9 years old) is a between-participants factor\nword frequency (low vs. high) is a within-participants factor\nword type (regular vs. irregular) is also a within-participants factor\n\nParticipants are given 10 words to pronounce in each category (40 words in total) and the dependent measure of interest is the number of pronunciation errors.\nThe data set contains the following variables:\n\nParticipant: represents the participant number, which ranges from 1–10.\nAge: whether the participant is 7-years-old or 9-years-old.\nHigh_Regular: pronunciation errors for high frequency regular words.\nHigh_Irregular: pronunciation errors for high frequency irregular words.\nLow_Regular: pronunciation errors for low frequency regular words.\nLow_Irregular: pronunciation errors for low frequency irregular words.\n\n\nImport data, set variables as factors, and generate descriptive statistics\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as wordPron.\n\n# *** ENTER YOUR OWN CODE HERE TO IMPORT THE DATA ***\n\n\n\n# A tibble: 10 × 6\n   Participant Age         High_Regular High_Irregular Low_Regular Low_Irregular\n         &lt;dbl&gt; &lt;chr&gt;              &lt;dbl&gt;          &lt;dbl&gt;       &lt;dbl&gt;         &lt;dbl&gt;\n 1           1 7 Year Olds            6              7           5             6\n 2           2 7 Year Olds            7              5           6             7\n 3           3 7 Year Olds            5              6           7             6\n 4           4 7 Year Olds            6              7           5             7\n 5           5 7 Year Olds            6              6           5             7\n 6           6 9 Year Olds            4              4           3             6\n 7           7 9 Year Olds            3              4           4             7\n 8           8 9 Year Olds            4              3           5             9\n 9           9 9 Year Olds            5              5           3             8\n10          10 9 Year Olds            3              4           3             7\n\n\nThe next thing we need to do is convert our data from wide format into long format. The first thing we need to do is group the columns High_Regular through to Low_Irregular into a new variable called Group using the gather() function. We showed you how to do this in the earlier data set, so give this a go for yourself. Make sure that you name the dependent measure Errors and that you call your new data set wordPronLng.\n\n# *** ENTER YOUR OWN CODE HERE TO GATHER THE WITHIN-PARTICIPANTS FACTORS INTO A COMMON GROUP ***\n\nIf you have executed your code correct, you should see the following output:\n\n\n# A tibble: 40 × 4\n   Participant Age         Group        Errors\n         &lt;dbl&gt; &lt;chr&gt;       &lt;fct&gt;         &lt;dbl&gt;\n 1           1 7 Year Olds High_Regular      6\n 2           2 7 Year Olds High_Regular      7\n 3           3 7 Year Olds High_Regular      5\n 4           4 7 Year Olds High_Regular      6\n 5           5 7 Year Olds High_Regular      6\n 6           6 9 Year Olds High_Regular      4\n 7           7 9 Year Olds High_Regular      3\n 8           8 9 Year Olds High_Regular      4\n 9           9 9 Year Olds High_Regular      5\n10          10 9 Year Olds High_Regular      3\n# ℹ 30 more rows\n\n\nWe have a new variable Group that contains our two independent variables, Frequency and Word Type, and a new variable Errors that contains our dependent measure. The next step is to use the separate() function to divide the variable Group into two new variables, one called Frequency and one called WordType. Again, we gave you an example of this earlier, so try your own code out for this bit. Just make sure you call your new data set wordPronLngSep.\n\n# *** ENTER YOUR OWN CODE HERE TO SEPARATE \"GROUP\" INTO SEPARATE VARIABLES FOR \"FREQUENCY\" AND \"WORDTYPE\" ***\n\nAssuming you have executed your code correctly, you should see the following output:\n\n\n# A tibble: 40 × 5\n   Participant Age         Frequency WordType Errors\n         &lt;dbl&gt; &lt;chr&gt;       &lt;chr&gt;     &lt;chr&gt;     &lt;dbl&gt;\n 1           1 7 Year Olds High      Regular       6\n 2           2 7 Year Olds High      Regular       7\n 3           3 7 Year Olds High      Regular       5\n 4           4 7 Year Olds High      Regular       6\n 5           5 7 Year Olds High      Regular       6\n 6           6 9 Year Olds High      Regular       4\n 7           7 9 Year Olds High      Regular       3\n 8           8 9 Year Olds High      Regular       4\n 9           9 9 Year Olds High      Regular       5\n10          10 9 Year Olds High      Regular       3\n# ℹ 30 more rows\n\n\nOur variable Group has now disappeared and in its place we have two new variables: Frequency and WordType. This is the version of the data set we will be using for the rest of the analysis.\nThe next thing we need to do is convert the columns Participant, Age, Frequency, and WordType into factors and re-order the levels of the latter two variables:\n\n# Make sure all necessary variables are coded as factors -- re-order the levels of \"Frequency\" and \"WordType\"\nwordPronLngSep$Participant = factor(wordPronLngSep$Participant)\nwordPronLngSep$Age = factor(wordPronLngSep$Age)\nwordPronLngSep$Frequency = factor(wordPronLngSep$Frequency,levels = c(\"Low\",\"High\"))\nwordPronLngSep$WordType = factor(wordPronLngSep$WordType,levels = c(\"Regular\",\"Irregular\"))\n\nNext, we will generate some descriptive statistics (mean and standard deviation). You can generate the code for this yourself. Make sure that you round the statistics to two-decimal places using the procedure I showed you in the earlier example.\n\n# *** ENTER YOUR OWN CODE HERE TO GENERATE DESCRIPTIVE STATISTICS ***\n\nIf you have executed the code correctly, then you should see the following output:\n\n\n          Age Frequency  WordType variable n mean   sd\n1 7 Year Olds       Low   Regular   Errors 5  5.6 0.89\n2 7 Year Olds       Low Irregular   Errors 5  6.6 0.55\n3 7 Year Olds      High   Regular   Errors 5  6.0 0.71\n4 7 Year Olds      High Irregular   Errors 5  6.2 0.84\n5 9 Year Olds       Low   Regular   Errors 5  3.6 0.89\n6 9 Year Olds       Low Irregular   Errors 5  7.4 1.14\n7 9 Year Olds      High   Regular   Errors 5  3.8 0.84\n8 9 Year Olds      High Irregular   Errors 5  4.0 0.71\n\n\n\n\nRunning the ANOVA, follow-up ANOVAs, and simple main effects\nThe code required to run the ANOVA is given below:\n\n# Create the mixed design ANOVA model\nwordPronModel = anova_test(data = wordPronLngSep, dv = Errors, wid = Participant, between = Age, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nwordPronModel$p = round(wordPronModel$p, 3)\n# Print the model summary\n(wordPronModel)\n\nANOVA Table (type II tests)\n\n                  Effect DFn DFd    SSn SSd        F     p p&lt;.05   ges\n1            (Intercept)   1   8 1166.4 4.5 2073.600 0.000     * 0.981\n2                    Age   1   8   19.6 4.5   34.844 0.000     * 0.467\n3              Frequency   1   8    6.4 8.7    5.885 0.041     * 0.222\n4               WordType   1   8   16.9 3.7   36.541 0.000     * 0.430\n5          Age:Frequency   1   8    6.4 8.7    5.885 0.041     * 0.222\n6           Age:WordType   1   8    4.9 3.7   10.595 0.012     * 0.179\n7     Frequency:WordType   1   8   12.1 5.5   17.600 0.003     * 0.351\n8 Age:Frequency:WordType   1   8    4.9 5.5    7.127 0.028     * 0.179\n\n\nTo create the model, the first argument we supplied to anova_test was the name of our data, wordPronLngSep. The second argument we supplied was our dependent variable, Errors. The third argument we supplied was Participant, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our between-participants factor, Age. The fifth argument we supplied as our within-participants factors, Frequency and WordType.\nAs in our first example, our factors have only two levels, so anova_test does not give us Mauchly’s test of sphericity or the Greenhouse-Geisser correction for the within-participants factors.\nInspecting the ANOVA table, rows two to four give the main effects of Age, Frequency, and Word Type; rows five to seven give the Age \\(\\times\\) Frequency, Age \\(\\times\\) Word Type, and Frequency \\(\\times\\) Word Type two-way interactions; and row eight gives the Age \\(\\times\\) Frequency \\(\\times\\) Word Type three-way interaction. Looking at the \\(p\\) values, we can see that all the main effects, two-way interactions, and the three-way interaction are significant.\nBecause the three-way interaction is significant, we need to analyse it further. As before, to do this we need to re-analyse the data as a series of two-factor ANOVAs. First, we must decide which factor to split the analysis by and the obvious contender is the between-participants factor of age. Accordingly, what we need to do is to perform two, two-factor ANOVAs:\n\na 2 (frequency: low vs. high) \\(\\times\\) 2 (word type: regular vs. irregular) ANOVA for the 7-year-olds only.\na 2 (frequency: low vs. high) \\(\\times\\) 2 (word type: regular vs. irregular) ANOVA for the 9-year-olds only.\n\nWe will start by running the two-factor ANOVA for the 7-year-olds (ignoring the data for the 9-year-olds). To do this, we first need to produce a filtered version of our data set called sevenYearsOnly that only includes the results for the 7-year-old children. We can create that with the following piece of code:\n\n# Get the data for the \"7 Year Olds\" only\nsevenYearsOnly = wordPronLngSep %&gt;%\n  filter(Age == \"7 Year Olds\") \n\nNext, we can run our two-factor ANOVA on this filtered data set. The steps are the same as above, except that we need to drop the Age factor included previously (remember, we are only analysing the data for the 7-year-olds).\n\n# Run the two-factor ANOVA for the \"7 Year Olds\" only\nsevenYearsModel = anova_test(data = sevenYearsOnly, dv = Errors, wid = Participant, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nsevenYearsModel$p = round(sevenYearsModel$p, 3)\n# Print the model summary\n(sevenYearsModel)\n\nANOVA Table (type III tests)\n\n              Effect DFn DFd   SSn SSd        F     p p&lt;.05   ges\n1        (Intercept)   1   4 744.2 0.3 9922.667 0.000     * 0.988\n2          Frequency   1   4   0.0 2.5    0.000 1.000       0.000\n3           WordType   1   4   1.8 2.7    2.667 0.178       0.164\n4 Frequency:WordType   1   4   0.8 3.7    0.865 0.405       0.080\n\n\nThe only thing we are interested in from the ANOVA table is the outcome of the two-way interaction between Frequency and Word Type; you can ignore everything else. You can see that the interaction is nonsignificant in this instance, \\(p\\) = .405. Thus, for 7-year-old children Frequency and Word Type do not combine with one another to influence pronunciation errors.\nWhat about 9-year-old children?\nThat brings us to our second two-factor ANOVA. For this, we now need to produce a filtered version of our data set called nineYearsOnly that only includes the results for the 9-year-old children. We can create that with the following piece of code:\n\nnineYearsOnly = wordPronLngSep %&gt;%\n  filter(Age == \"9 Year Olds\") \n\nThe command filter(Age == \"9 Year Olds\") tells R that we only want the data for the 9-year-old children.\nNext, we can run our two-factor ANOVA on this filtered data set.\n\n# Run the two-factor ANOVA for the \"9 Year Olds\" only\nnineYearsModel = anova_test(data = nineYearsOnly, dv = Errors, wid = Participant, within = c(Frequency, WordType), detailed = TRUE)\n# Round the p values to three decimal places\nnineYearsModel$p = round(nineYearsModel$p, 3)\n# Print the model summary\n(nineYearsModel)\n\nANOVA Table (type III tests)\n\n              Effect DFn DFd   SSn SSd       F     p p&lt;.05   ges\n1        (Intercept)   1   4 441.8 4.2 420.762 0.000     * 0.971\n2          Frequency   1   4  12.8 6.2   8.258 0.045     * 0.492\n3           WordType   1   4  20.0 1.0  80.000 0.001     * 0.602\n4 Frequency:WordType   1   4  16.2 1.8  36.000 0.004     * 0.551\n\n\nThis time the interaction between Frequency and Word Type is significant, p = .004, so we now need to perform a simple main effects analysis to determine why.\nBefore we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n# Get the 9 year olds ANOVA table\nnineYearsAnovaTable = get_anova_table(nineYearsModel)\n\nNext, we need to calculate the cell totals for each of the four conditions and the number of observations (i.e., scores) in each cell:\n\n# Get cell totals and counts\nnineYearsCellTotals = nineYearsOnly %&gt;%\n  # Organise the output by the \"Frequency\" and \"WordType\" factors\n  group_by(Frequency, WordType) %&gt;%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(Errors),n = n())\n  # Print the results\n  (recallCellTotals)\n\n# A tibble: 4 × 4\n# Groups:   LearningContext [2]\n  LearningContext TestingContext   sum     n\n  &lt;fct&gt;           &lt;fct&gt;          &lt;dbl&gt; &lt;int&gt;\n1 Under           Under             38     5\n2 Under           Land              23     5\n3 Land            Under             17     5\n4 Land            Land              34     5\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Frequency at Word Type. This means, we are going to:\n\nTest the difference between low and high frequency regular words only.\nTest the difference between low and high frequency irregular words only.\n\nTo do this, we need to declare Frequency as the “fixed” factor (we are always comparing low and high frequency words) and Word Type as the “across” factor (the comparison between low and high frequency words occurs “across” the regular and irregular levels of the Word Type factor):\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"Frequency\"\nacross = \"WordType\"\n\nWe then generate the simple main effects of Frequency at Word Type with the following:\n\n# Simple main effects of \"Frequency\" at \"WordType\"\nsmeFrequency = simple(nineYearsCellTotals,nineYearsAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeFrequency$P = round(smeFrequency$P, 3)\n(smeFrequency)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square           F     P\n1    Regular            0.1                  1        0.10  0.06451613 0.812\n2  Irregular           28.9                  1       28.90 18.64516129 0.012\n3 Error term            6.2                  4        1.55  0.00000000 0.000\n\n\nThe simple main effect of Frequency at regular words is nonsignificant, \\(p\\) = .812, indicating that pronunciation errors for regular words do not differ according to whether they are low or high in frequency. However, the simple main effect of Frequency at irregular words is significant, \\(p\\) = .012, indicating that pronunciation errors for irregular words are higher when they are of low frequency than when they are of high frequency (check the descriptive statistics to verify this).\nNow, let’s calculate the simple main effects of Word Type at Frequency. This means, we are going to:\n\nTest the difference between regular and irregular low frequency words only.\nTest the difference between regular and irregular high frequency words only.\n\nTo do this, we need to declare Word Type as the “fixed” factor and Frequency as the “across” factor:\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"WordType\"\nacross = \"Frequency\"\n\nWe then generate the simple main effects of Word Type at Frequency as follows:\n\n# Simple main effects of \"WordType\" at \"Frequency\"\nsmeWordType = simple(nineYearsCellTotals,nineYearsAnovaTable,fixed,across)\n# Round the p values to three decimal places\nsmeWordType$P = round(smeWordType$P, 3)\n(smeWordType)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square     F     P\n1        Low           36.1                  1       36.10 144.4 0.000\n2       High            0.1                  1        0.10   0.4 0.561\n3 Error term            1.0                  4        0.25   0.0 0.000\n\n\nThe simple main effect of Word Type at low frequency is significant, \\(p\\) \\(&lt;\\) .001, indicating that there are more pronunciation errors for low frequency irregular words than for low frequency regular words. Second, the simple main effect of word type at high frequency is nonsignificant, \\(p\\) = .561, indicating that pronunciation errors for high frequency regular and irregular words do not differ.\nIn short, the three-way interaction arose because pronunciation errors in 7-year-old children are unaffected by word frequency and word type, whereas pronunciation errors in 9-year-old children are influenced by these factors. Specifically, low frequency irregular words are associated with more pronunciation errors than low frequency regular words, but there is no difference between the frequency of pronunciation errors for high frequency irregular and regular words. This pattern can be seen in Figure 2 below which plots the data for the word pronunciation study.\n\n\n\nFigure 2. Pronunciation errors as a function of word frequency and word type for 7-year-old children (left panel) and 9-year-old children (right panel).\n\n\n\n\nWriting up the results\nThe conventions for writing up the results of a mixed three-factor ANOVA are the same as for a fully within-participants three-factor ANOVA (and indeed a fully between-participants three-factor ANOVA), so see my example write-up for the memory and context study.\n\n\n\nAdditional tasks\nPhew!! That’s probably the most we have covered in any of our lab sessions. Well done for making it through to the end!\nHere are some additional tasks you might consider doing:\n\nWrite-up the results of the word pronunciation study.\nGenerate the interaction plots in Figures 1 and 2, but add error bars (confidence intervals) to the data points.\n\nI’ll include the write-up/plot code for these additional tasks in the instructors copy of the lab materials at the end of the week."
  },
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Welcome to Year 2 statistics!\n\nThis year you will have the opportunity to build on the skills you developed last year - both in terms of statistical expertise and data coding in R. The year, you will complete two statistics modules:\n\nPSYC214: Statistics for group comparisons, with Sam Russell and Mark Hurlstone\nPSYC234: Statistics: from association to modelling causality, with Emma Mills and Amy Atkinson\n\nClick on the button below to access your module: Term 1: PSYC214, Term 2: PSYC234.\n\n\n  \n    \n\n    \n      PSYC214\n    \n    \n      PSYC234"
  },
  {
    "objectID": "PSYC234/Week5.html",
    "href": "PSYC234/Week5.html",
    "title": "5. Factor analysis & the binomial test",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week5.html#multicollinearity",
    "href": "PSYC234/Week5.html#multicollinearity",
    "title": "5. Factor analysis & the binomial test",
    "section": "Multicollinearity",
    "text": "Multicollinearity\nWhen we have too much overlap between our predictor variables it can be a problem. This is called multicollinearity.\nWhen you have many predictor variables you need to check that they are not too highly correlated with each other. You do this by calling a correlation plot, where correlations &gt; .80 may be problematic. If you have lots of observations, you may be okay, but your model may have problems\n\nMitigation\nChoose a smaller set of variables\n\nBy theory Group together alike variables\nPrincipal component analysis Estimate underlying structures\nFactor analysis\nUnderlying structures are also known as “latent variables”"
  },
  {
    "objectID": "PSYC234/Week5.html#factor-analysis-1",
    "href": "PSYC234/Week5.html#factor-analysis-1",
    "title": "5. Factor analysis & the binomial test",
    "section": "Factor Analysis",
    "text": "Factor Analysis\nFinding underlying structure or latent variables + Latent variables should have three or more observed variables loading onto them + Correlations between those observed variables should be high + The observed variables should load only onto one variable\nBefore beginning:\n\nStandardise your variables\n\nFactor analysis is quite jargon heavy:\nExploratory factor analysis (EFA) and Confirmatory factor analysis (CFA)\nWhere there is overlap, where variables ‘load’ onto factors:\nThe shared variance is called ‘communality’\nAnd the unshared variance is called uniqueness\n\nPerformance\n\nA factor should have at least three variables that are loaded to a sufficient level\nAny one variable should have most of its loading on one factor 3.A factor should have good internal consistency (a common (and maybe overused) method is Cronbach’s alpha) 4.Factors should make theoretical sense\n\nSnapshot of an example output below:  \n\nThe first column is the labels of the observed variables\nColumns 2 – 5 are the latent variable loadings for each of those observed variables.\nThe first three factors each have 3+  of the observed variables\nLength is problematic here\n\n\n\n\nExample of variable loadings\n\n\n\n\nPreparation\n\nRemove outcome variables, categorical variables, ordinal variables\nVisualise the prepared dataset\nInspect correlations (guidelines exist!)\nPerform some EFA specific tests to determine if a factor analysis is feasible + KMO + Bartlett’s test of sphericity + Parallel analysis + Scree test (visualization) + Packages in R support these\n\n\n\n\nExample of variable loadings\n\n\n\nIf the indications are good – then perform a factor analysis\n\nGuidelines exist for all of the following:\n\nLook for patterns inside the factors\nLook for patterns between the factors\nLook for communalities\nLook for factor correlations\nTest for internal consistency of the factors\nDraw! Report!"
  },
  {
    "objectID": "PSYC234/Week5.html#practice",
    "href": "PSYC234/Week5.html#practice",
    "title": "5. Factor analysis & the binomial test",
    "section": "Practice",
    "text": "Practice\nYou will need to review the Birthweight example to be able to complete WBA5.\nYou can choose any of the three examples to explore the process of Factor Analysis.\nBirthweight script and data here, and codebook here\nQuestionnaire script and data here\nSingle Word Reading script and data here\nOr, download all the materials here - treat yourself!"
  },
  {
    "objectID": "PSYC234/Week5.html#preamble",
    "href": "PSYC234/Week5.html#preamble",
    "title": "5. Factor analysis & the binomial test",
    "section": "Preamble",
    "text": "Preamble\nLet’s think back to the one-sample t-test\nRQ: You are a researcher interested in whether babies born in Germany weigh more than babies born in the UK.\nLet’s assume the following are true:\n\nThe NHS keeps good records of birth weights in the UK, and that the average birth weight in the UK in 2020 was 3350g.\nThe health authorities in Germany do not keep good records of birth weights.\n\nYou could collect birth weights from a sample of babies born in Germany and compare this to the known average in the UK (3350g).\n\nBirthweights of babies born in Germany\n\n\nParticipant\nWeight (g)\n\n\n\n\n1\n3004\n\n\n2\n3052\n\n\n3\n3067\n\n\n4\n4063\n\n\n5\n2134\n\n\n6\n2356\n\n\n7\n4356\n\n\n8\n3567\n\n\n9\n3432\n\n\n10\n3245\n\n\n11\n1467\n\n\n12\n2345\n\n\n13\n4532\n\n\n14\n4352\n\n\n15\n2453\n\n\n16\n2343\n\n\n17\n3453\n\n\n18\n3428\n\n\n19\n2344\n\n\n20\n4353\n\n\nMean\n3167.30\n\n\n\nIs the mean of the sample significantly different from a known value? to test this, we can use a one-sample t-test!"
  },
  {
    "objectID": "PSYC234/Week5.html#another-example",
    "href": "PSYC234/Week5.html#another-example",
    "title": "5. Factor analysis & the binomial test",
    "section": "Another example",
    "text": "Another example\nWhat if we asked the question: Is the moon made of cheese?\nTwo answers: Yes, and No.\nDoes the proportion of participants answering the question correctly differ from the chance guessing rate?\n\nResponses to the question “Is the moon made of cheese”?\n\n\nParticipant\nAnswer\n\n\n\n\n1\nNo\n\n\n2\nNo\n\n\n3\nYes\n\n\n4\nNo\n\n\n5\nNo\n\n\n6\nNo\n\n\n7\nNo\n\n\n8\nNo\n\n\n9\nNo\n\n\n10\nNo\n\n\n11\nNo\n\n\n12\nNo\n\n\n13\nNo\n\n\n14\nNo\n\n\n\nHow many participants answered the question correctly? + Correct (“No”) = 13/14 = 0.93\nWhat is chance guessing rate? + 2 possible answers (yes/no), so chance = 50%, expressed as a proportion, this is 0.5\n\n\n\n\n\n\nNote\n\n\n\nTo convert a percentage to a proportion, divide by 100 = 50/100 = 0.5\n\n\n\nWhy can’t we use the one-sample t-test?\nOne sample t-test: Is the mean of the sample significantly different from a known value?\nIssue: we can’t calculate a mean value for the sample – we have a proportion who answered correctly\nSo, we can’t use a one-sample t-test… What can we use?!\n\n\nThe binomial test!\nThe binomial test compares a sample proportion to a known value\nDoes a sample proportion differ significantly from a known value?\nKnown value may be theoretical (e.g. based on chance) or known data about the world (e.g. 26% people die from this disease)\nOther examples:\nYou notice that a lot of the insects in your garden are ants. You work out that 564 out of 712 insects are ants. You hear on a TV show that on average, 64% of insects in UK gardens are ants. Is the proportion of insects that are ants in your garden larger than UK average?\n\nProportion of sample that are ants are: 564/712 = 0.79\nKnown value (expressed as a proportion)= 0.64\n\nYou develop a new vaccine for an illness and give this to 1,000 people - 32/1000 given the vaccine get the illness within a year. You know that approximately 10% of unvaccinated people (or 0.1 expressed as a proportion) get the illness every year. Is the proportion of vaccinated people getting the illness lower than the known value for unvaccinated people?\n\nProportion of sample that get the illness: 32/1000 = 0.03\nKnown value (expressed as a proportion)= 0.10\n\nDoes a sample proportion differ significantly from a known value?\n\n\nBinomial Test assumptions\n\nThe outcome is dichotomous: \u000bThere are only two possible outcomes\nThe outcome can be specified as success or failure\u000b\u000b\nEach trial is independent\nThe probability of ‘success’ remains the same on every trial\n\n\n\nCarrying out the binomial test\nBelow is the code needed for running a binomial test. Pretty simple! binom.test is a function that is in the base R package, meaning that we don’t need to load in any additional libraries to use it.\nThe first number is the number of successes (thirteen said no), the second number is the total number of trials (we asked fourteen people), and the last number is the known value, here expressed as a proportion (here chance guessing rate)\n\n# Code to run the binomial test\n\nbinom.test(13, 14, 0.5)\n\n\n    Exact binomial test\n\ndata:  13 and 14\nnumber of successes = 13, number of trials = 14, p-value = 0.001831\nalternative hypothesis: true probability of success is not equal to 0.5\n95 percent confidence interval:\n 0.6613155 0.9981932\nsample estimates:\nprobability of success \n             0.9285714 \n\n\nAnd here is an annotated version of the output\n\n\n\nAnnotated binom.test output\n\n\n\nHow do I interpret the p-value?\np ≤ .05 = The observed proportion differs significantly from the known value\np &gt; .05 = The observed proportion does not differ significantly from the known value\n\n\nI have a significant effect… In what direction is the effect?\nIs the probability of success higher or lower than the known value?\nThe proportion of children answering the question correctly (.93) is significantly higher than the chance guessing rate (.50)\n\n\nWhat does the 95% confidence interval tell us?\nIf we repeat the sampling method many many times and compute a 95% confidence interval, 95% of the intervals would contain the true value in the population.\nRange that is likely to contain the true value\n\n\n\nConfidence intervals\n\n\n\n\nReporting in APA format\nA binomial test was conducted to determine whether the proportion of participants answering the question correctly differed significantly from chance guessing rate. This revealed that that the proportion of participants answering the question correctly (93%; 95% confidence interval = 66-100%) was significantly higher than the chance guessing rate (50%; p = 0.002)."
  },
  {
    "objectID": "PSYC234/Week5.html#post-lecture-activities---complete-ideally-before-wba",
    "href": "PSYC234/Week5.html#post-lecture-activities---complete-ideally-before-wba",
    "title": "5. Factor analysis & the binomial test",
    "section": "Post-lecture activities - Complete ideally before WBA",
    "text": "Post-lecture activities - Complete ideally before WBA\nThese can be downloaded here and contain a word document worksheet and the answers."
  },
  {
    "objectID": "PSYC234/Week6.html",
    "href": "PSYC234/Week6.html",
    "title": "6. Wilcoxon rank-sum test and Wilcoxon signed-rank test",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week6.html#overview",
    "href": "PSYC234/Week6.html#overview",
    "title": "6. Wilcoxon rank-sum test and Wilcoxon signed-rank test",
    "section": "Overview",
    "text": "Overview\nI’ll provide you with one or two research questions each week which will require you to complete the statistical tests covered in the lectures.\nYou can work in groups.\nUse the lab preparation video and script, lecture slides, and previous content covered in the statistics modules to help you\nBegin the script in the lab and complete in your own time\nYou can write your script as a .R or Rmd file\nSubmit for feedback. For this week’s lab, scripts should be submitted on the moodle link by: Friday of Week 16 at 3pm\nI’ll provide feedback on a subset of scripts - You will not be judged or marked on your scripts – I’m providing feedback to help you to improve your coding skills and prepare you for your dissertation next year\nI’ll also release a model script I’ve written - this will go live on Moodle on Friday of Week 16 at 3pm"
  },
  {
    "objectID": "PSYC234/Week6.html#research-question-1",
    "href": "PSYC234/Week6.html#research-question-1",
    "title": "6. Wilcoxon rank-sum test and Wilcoxon signed-rank test",
    "section": "Research question 1",
    "text": "Research question 1\nYou are a researcher interested in whether the old saying “an apple a day keeps the doctor away is true”.\nYou recruit 16 people and assign each participant to either a “0 apples” or “1 apple” condition. Participants in the “0 apple” condition eat 0 apples every day for a year. Participants in the “1 apple” condition eat 1 apple a day for a year. You ask participants to report how many times they visited the GP in the year."
  },
  {
    "objectID": "PSYC234/Week6.html#research-question-2",
    "href": "PSYC234/Week6.html#research-question-2",
    "title": "6. Wilcoxon rank-sum test and Wilcoxon signed-rank test",
    "section": "Research question 2",
    "text": "Research question 2\nYou are interested in whether eating bananas keeps the doctor away.\nThis time you recruit only one group of participants. In the first year, you ask them to eat 0 banana every day. In the second year, you ask them to eat 1 bananas a day. You ask them to report how many times they visit the GP in Year 1 and Year 2."
  },
  {
    "objectID": "PSYC234/Week6.html#a-template-for-running-the-wilcoxon-rank-sum-test-and-the-wilcoxon-signed-rank-test",
    "href": "PSYC234/Week6.html#a-template-for-running-the-wilcoxon-rank-sum-test-and-the-wilcoxon-signed-rank-test",
    "title": "6. Wilcoxon rank-sum test and Wilcoxon signed-rank test",
    "section": "A template for running the Wilcoxon rank-sum test and the Wilcoxon signed-rank test",
    "text": "A template for running the Wilcoxon rank-sum test and the Wilcoxon signed-rank test\nYour script should aim to answer and interpret both of these research questions\nStart a new session on the server.\nI have uploaded a .R and .Rmd file to this week’s lab folder. You could use this to get started or just create your own blank .R/.Rmd file\n\nLoad packages and data\nNormality checks\nExplore your data (e.g. descriptive statistics, a plot)\nConduct the statistical test\nCalculate an effect size\nInterpret your data"
  },
  {
    "objectID": "PSYC234/Week3.html",
    "href": "PSYC234/Week3.html",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week3.html#interactions",
    "href": "PSYC234/Week3.html#interactions",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Interactions",
    "text": "Interactions\n\nWhen one variable moderates the effect of another variable on the outcome variable, Or\n\nWhen a coefficient for one predictor changes its size when it moves between the levels or values of another predictor, Or\nThe change that one predictor predicts for the outcome variable depends on a specific level of another predictor\n\nModeration – a process for exploring the differing conditions under which two or more variables may work together.\n\nIn a simple regression, such as below, we test the effect of a single predictor on our dependent variable. In this case, the IQ of a mother on a child’s test score.\n\n\n\nSimply regression\n\n\nIf we were to add in another variable that groups the data somehow, perhaps mother’s high school completion (Yes or No), then we may see that there are two lines that represent the two groups in a plot:\n In the example above, there is no interaction, regardless of group, child test scores increase as a constant rate from an effect of Mother’s IQ. One may be slightly higher, indicating one group typically shows a greater IQ, but this increases at a constant rate.\nIn the example below, the lines cross, which means that we have an interaction. Something in the two predictors, IQ and high school completion means that they interact with each other, and have an influence on each other as well as the dependent variable.\n ### What are interaction terms?\nModel for multiple regression with two independent predictors: \\(Y=𝑏_0+ 𝑏_1𝑋_1 + 𝑏_2𝑋_2 + 𝑒\\)\n\nWe interpret each predictor as representing a value of the variable’s influence on the outcome variable while holding all other predictors constant\n\nIn the diagram of the multiple regression model below, note that \\(X_1\\) and \\(X_2\\) do not talk to each other – only Y\nSometimes, we need to consider one variable’s influence on the outcome variable at different levels of another predictor: we need interactions. These are expressed like this in the equation: \\(Y=𝑏_0 + 𝑏_1X_1 + 𝑏_2X_2 + b_3\\times(X_1𝑋_2)+ 𝑒\\)\nSee how in the diagram below, \\(X_1\\) and \\(X_2\\) do talk to each other – and they both talk to Y as well\nInteractions are the product term of two (or more) predictors + We multiply two (or more) variables together + You can do this outside the model and create a new variable in your dataset + Or you can write it into your model formula\nIntroducing an interaction term means that the predictors within the term are no longer acting independently of each other + Interaction terms tend to strengthen or weaken the coefficients’ independent effects\nNow, a predictor’s effect is conditional upon the level of another predictor (example coming)"
  },
  {
    "objectID": "PSYC234/Week3.html#when-are-interaction-terms-used",
    "href": "PSYC234/Week3.html#when-are-interaction-terms-used",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "When are interaction terms used?",
    "text": "When are interaction terms used?\nTheory may predict them, and so research questions/hypotheses may predict them.\nStudy design may implicitly suggest them (groups?)\nResults may suggest them + Regression coefficients that seem large when modelled independently may suggest the need for an interaction term + Make sure you are aware of where you are in the analysis workflow + Don’t just add them + Save any unplanned additions of interaction terms for an exploratory analysis phase…"
  },
  {
    "objectID": "PSYC234/Week3.html#statistical-power-of-interactions",
    "href": "PSYC234/Week3.html#statistical-power-of-interactions",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Statistical Power of Interactions",
    "text": "Statistical Power of Interactions\nMany effect sizes in psychology hover around Cohen’s d = 0.4\nBased upon an effect size like this: + The power of an interaction can be a small as 1/16th of the independent effects within a model (Gelman et al. 2020) + This is for ANOVA and regression methods + This is when all the assumptions of the model are met + Most models violate model assumptions in some way + Although relatively tolerant of violations: + Power of the effects are thus reduced further…"
  },
  {
    "objectID": "PSYC234/Week3.html#preparation-of-data-for-using-interactions",
    "href": "PSYC234/Week3.html#preparation-of-data-for-using-interactions",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Preparation of data for using interactions",
    "text": "Preparation of data for using interactions\nCentre or standardise your continuous variables as standardising puts both predictors on the same scale. So interpreting the effect of one predictor on the other will be using the same scale of units\nConsider how to use your categorical variables + Dummy coding..? + Sum coding..? + Making new binary variables…? + Coercing a binary variable to numeric class and mean centring + (so many choices! So little time!)"
  },
  {
    "objectID": "PSYC234/Week3.html#building-a-model-with-interactions",
    "href": "PSYC234/Week3.html#building-a-model-with-interactions",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Building a model with interactions",
    "text": "Building a model with interactions\nPlan to build the model suggested by your research questions/hypotheses/theory. If that includes the interaction term, build it as the first model. If not, + Build an independent predictor model first + Build a further model that includes independent and interaction terms\nQuestion: does a study design that uses different groups of people, implicitly motivate an interaction term?\nThis implies more than one model; multiple regression equals multiple models\nA cautious and conservative approach to multiple regression is to create several models, each one building upon the next + These are called “nested” models + Every model uses exactly the same data + The latest model has all of the previous models within it + The latest model can differ by +1 predictor or more\n\n\n\nNested models"
  },
  {
    "objectID": "PSYC234/Week3.html#what-model-to-report",
    "href": "PSYC234/Week3.html#what-model-to-report",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "What model to report?",
    "text": "What model to report?\nYou can inspect the \\(R^2\\) or adj. \\(R^2\\) values to see if the more complex model explains more variance in the outcome variable. But more principled is to use an inferential test: + Use anova() to formally test the null hypothesis that two models explain the same amount of variance in the outcome variable = model comparison + If so, p-value is &gt; .05, + we fail to reject the null hypothesis + AND we choose the simpler model + If the p-value of the anova &lt; .05 + we reject the null hypothesis + we choose the more complex model\n ### Hazard Warning!\nThis is where p-hacking, cherry picking and HARKing can occur really easily. Strategies to defend against such questionable research practices: (remember from PSYC123 last year) + State clearly whether you are in confirmatory or exploratory analysis mode + Explain your process to yourself and your future reader before you start (at best in a pre-registration; at the very least before you begin) + Report all models – use a script such as .Rmd file or an interactive notebook so that the process is recorded and can be reproduced + Be honest and comprehensive in your reporting."
  },
  {
    "objectID": "PSYC234/Week3.html#import-packages-and-data",
    "href": "PSYC234/Week3.html#import-packages-and-data",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Import Packages and Data",
    "text": "Import Packages and Data\n\nknitr::opts_chunk$set(include = TRUE)\n\nlibrary(tidyverse)\n\n── Attaching core tidyverse packages ──────────────────────── tidyverse 2.0.0 ──\n✔ dplyr     1.1.3     ✔ readr     2.1.4\n✔ forcats   1.0.0     ✔ stringr   1.5.0\n✔ ggplot2   3.5.1     ✔ tibble    3.2.1\n✔ lubridate 1.9.2     ✔ tidyr     1.3.0\n✔ purrr     1.0.2     \n── Conflicts ────────────────────────────────────────── tidyverse_conflicts() ──\n✖ dplyr::filter() masks stats::filter()\n✖ dplyr::lag()    masks stats::lag()\nℹ Use the conflicted package (&lt;http://conflicted.r-lib.org/&gt;) to force all conflicts to become errors\n\nlibrary(effects)\n\nLoading required package: carData\nlattice theme set by effectsTheme()\nSee ?effectsTheme for details.\n\nlibrary(broom)\nlibrary(gridExtra)\n\n\nAttaching package: 'gridExtra'\n\nThe following object is masked from 'package:dplyr':\n\n    combine\n\nlibrary(PerformanceAnalytics)\n\nLoading required package: xts\n\n\nWarning: package 'xts' was built under R version 4.3.3\n\n\nLoading required package: zoo\n\nAttaching package: 'zoo'\n\nThe following objects are masked from 'package:base':\n\n    as.Date, as.Date.numeric\n\n\n######################### Warning from 'xts' package ##########################\n#                                                                             #\n# The dplyr lag() function breaks how base R's lag() function is supposed to  #\n# work, which breaks lag(my_xts). Calls to lag(my_xts) that you type or       #\n# source() into this session won't work correctly.                            #\n#                                                                             #\n# Use stats::lag() to make sure you're not using dplyr::lag(), or you can add #\n# conflictRules('dplyr', exclude = 'lag') to your .Rprofile to stop           #\n# dplyr from breaking base R's lag() function.                                #\n#                                                                             #\n# Code in packages is not affected. It's protected by R's namespace mechanism #\n# Set `options(xts.warn_dplyr_breaks_lag = FALSE)` to suppress this warning.  #\n#                                                                             #\n###############################################################################\n\nAttaching package: 'xts'\n\nThe following objects are masked from 'package:dplyr':\n\n    first, last\n\n\nAttaching package: 'PerformanceAnalytics'\n\nThe following object is masked from 'package:graphics':\n\n    legend\n\n\n\nd &lt;- read_csv(\"KidIQ.txt\", col_types = cols(mom_work = col_skip()))\n\nNotice how I have turned the knitr::opts_chunk$set(include = TRUE) line of code above to TRUE, this is what prints the code and output when you knit the document. Switching TRUE to FALSE will stop the code from printing out when you knit - try it. You can’t break anything!\nLast week, we worked with multiple regression models that contained both continuous and categorical predictors. This week we continue to work with a mix of predictor variables and introduce the next step - interaction terms.\nWhen an interaction term is entered into a model and is found to be significant (p &lt; .05), this means that the rate of change in the outcome variable changes as the levels in the predictor variables change. We have to consider the independent effects of the predictors that are in the interaction term and infer whether the interaction effect is a weakening or a strengthening of the outcome variable. Here is a plot to illustrate what I mean:"
  },
  {
    "objectID": "PSYC234/Week3.html#visualise",
    "href": "PSYC234/Week3.html#visualise",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Visualise",
    "text": "Visualise\n\nggplot(d, aes(mom_iq, kid_score)) +\n  geom_point() +\n  geom_abline(\n  intercept = c(-10),\n  slope = c(1),\n  color = c(\"black\")) +\n  scale_color_manual(values = c(\"black\")) +\n  labs(x = \"Mother IQ Score\", y = \"Child Test Score\") + \n  theme_bw()\n\n\n\n\n\n# code to show parallel lines as illustration of no interaction.\n# ggplot(d, aes(mom_iq, kid_score)) +\n#   geom_point(aes(color = factor(mom_hs)), show.legend = FALSE) +\n#   geom_abline(\n#   intercept = c(-20.59, 11), # different starting places.\n#   slope = c(1, 1),  # both slopes rise at the same rate - no differences \n#   color = c(\"black\", \"grey\")) +\n#   scale_color_manual(values = c(\"black\", \"grey\")) +\n#   labs(x = \"Mother IQ Score\", y = \"Child Test Score\") + \n#   theme_bw()\n\n\n\n\n\n\nThis plot shows a distribution of 3 year old child test scores (y- axis - outcome variable) and how they correlate with their mother’s IQ Score (x-axis, predictor variable). They grey and the black colours of the dots further denote a grouping variable - whether the mother of the child finished high school or not (yes or no).\nFor the purposes of illustrating the interaction, it is not important to know which colour represents which group. Suffice it to say that the plot shows an interaction between high school graduation status and mother’s IQ score. We know this because the black and grey regression lines are not parallel across the plot. The rate of change for Mother’s IQ on child test scores is not the same for both levels of mother’s high school graduation status.\nAbove, I talked about an interaction effect either weakening or strengthening a relationship with the outcome variable. Both are present here and it depends which group you want to focus on to decided whether the relationship is weaker or stronger.\nLook at the black line on the plot - at the left hand side, it begins at a lower starting score than the grey line. As you move across the values of mother’s IQ, however (move to the right in the plot, tracing the black line), and as mother’s IQ gets higher, the black line crosses the grey line, and those children whose mother is in the black group for high school completion status and whose mother has higher IQ, end up scoring higher test scores than the child who mother is in the grey group for high school completion status and also has the same level of IQ. This is a strengthening relationship.\nLet’s draw the plot again to reduce the need to scroll up and down the page:\n\n\n\n\n\nNow focus on the grey line. This is the other group for mother’s high school completion status. At lower IQ levels, the children in this group begin with higher test scores than the other children whose mothers are in the black group, however, by the time we get to the higher levels of mother’s IQ at the right hand side of the plot, the children of this group have lower test scores. The rate of change in test scores for children in the grey group is slower across the values of Mother’s IQ. It is a weakened relationship of the influence of Mother’s IQ on the child’s test scores within this group.\nYou can also see the relationship in the slant of the slopes:\nIrrespective of whether the coefficient shows a positive or a negative relationship:\nA slower rate of change, or a weakening relationship will have a flatter or a more shallow slope.\nA faster rate of change, or a strengthening relationship will have a steeper slope.\nIn this demonstration, we will model an interaction term, and interpret it. First we will build models that contain only the independent predictors. Then we will add onto the model the interaction term. We will use a model comparison routine to look support decision making as to which model is a better fit for the data."
  },
  {
    "objectID": "PSYC234/Week3.html#the-multilple-regression-equation",
    "href": "PSYC234/Week3.html#the-multilple-regression-equation",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "The Multilple Regression Equation",
    "text": "The Multilple Regression Equation\n\nMultiple Regression With Only Independent Predictors\nKeep the model for multiple regression in mind:\nYou have seen the two predictor regression model equation before:\n\\[\nY_i = b_0 + b_1 * X_1 + b_2*X_2 + e_i\n\\]\n\n\nMultiple Regression With Independent Predictors & Interaction Term\nA small extension includes the product of \\(X_1\\) and \\(X_2\\) multiplied by a further beta coefficient\n\\[\nY_i = b_0 + b_1 * X_1 + b_2*X_2 + b_3*(X_1*X_2) +e_i\n\\] - \\(X_1\\) and \\(X_2\\) can be categorical predictors, continuous predictors or a mix of each."
  },
  {
    "objectID": "PSYC234/Week3.html#visualise-the-data",
    "href": "PSYC234/Week3.html#visualise-the-data",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Visualise The Data",
    "text": "Visualise The Data\nThe data set is retrieved from https://github.com/avehtari/ROS-Examples/tree/master/KidIQ/\n\nhead(d)\n\n# A tibble: 6 × 4\n  kid_score mom_hs mom_iq mom_age\n      &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;\n1        65      1  121.       27\n2        98      1   89.4      25\n3        85      1  115.       27\n4        83      1   99.4      25\n5       115      1   92.7      27\n6        98      0  108.       18\n\n\n\nkid_score = test score @ 3 yrs of age = outcome variable\nmom_hs = a categorical variable. At the moment, R is seeing it as numerical so we can use it as an indicator variable. 1 = kid’s mom completed high school, 0 = kid’s mom did not complete high school.\nmom_iq = the IQ score of the kid’s mom\nmom_age = mother’s age at the time of birth\n\n\nsummary(d) # look at means and ranges within each variable\n\n   kid_score         mom_hs           mom_iq          mom_age     \n Min.   : 20.0   Min.   :0.0000   Min.   : 71.04   Min.   :17.00  \n 1st Qu.: 74.0   1st Qu.:1.0000   1st Qu.: 88.66   1st Qu.:21.00  \n Median : 90.0   Median :1.0000   Median : 97.92   Median :23.00  \n Mean   : 86.8   Mean   :0.7857   Mean   :100.00   Mean   :22.79  \n 3rd Qu.:102.0   3rd Qu.:1.0000   3rd Qu.:110.27   3rd Qu.:25.00  \n Max.   :144.0   Max.   :1.0000   Max.   :138.89   Max.   :29.00  \n\n\nRight now, the R environment sees all the variables as numeric, (blue button also). Lets visualise the variables now and have a look at the relationships between them:\nWhen using this code, the c(2:4) part is calling the numeric columns of the dataset. Notice that I don’t include kid_score in this group. That’s because I am going to use this as the outcome variable.\n\nGraphical Visualisation\n\nchart.Correlation(d[, c(2:4)], histogram=TRUE, pch=19)\n\n\n\n\nEven though R sees all the variables as numeric, you can clearly see the binary structure of the mom_hs variable with two levels at 0 and 1, and that the values are not evenly split within the variable.\n\ntable(d$mom_hs) # how many values are in each level of mom_hs?\n\n\n  0   1 \n 93 341"
  },
  {
    "objectID": "PSYC234/Week3.html#model",
    "href": "PSYC234/Week3.html#model",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Model",
    "text": "Model\nProgress in school is often talked about as better for children whose parents value school. We could use the variable mom_hs as a proxy measure for this. We can see from the table() function above that there are almost 4 times as many parents who completed high school than didn’t in the dataset - an unbalanced variable. But before we do that, lets begin to build some models. Lets construct an intercept only or an empty model and see what the average 3 year old test score for this sample is:\nAn intercept only model is basically a model for the average test score: you can test this by calculating the mean before you model:\n\nmean(d$kid_score)\n\n[1] 86.79724\n\n\n\nsummary(m0 &lt;- lm(kid_score ~ 1, d)) # putting just 1 = estimate the intercept.\n\n\nCall:\nlm(formula = kid_score ~ 1, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-66.797 -12.797   3.203  15.203  57.203 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  86.7972     0.9797   88.59   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 20.41 on 433 degrees of freedom\n\n\nWith no other predictors, the average test score is 86. We can reject the null hypothesis that the kids score is zero (one sample t(433) = 88.59, p &lt; .001)\nLets build another model: Above we discussed how a parent’s value or belief in education is theorised to predict their offspring’s progress. We have operationalised this theory by having the mom_hs variable predict the kid_score outcome variable: completing high school represents behaviour that shows a belief in education.\nBefore we do - think about how R sees this binary variable - two levels - 0 and 1, with the lowest numerical value being estimated in the intercept.\n\nsummary(m1 &lt;- lm(kid_score ~ mom_hs, d)) # by default, lm() will model the \"1\" so it does not needed to be hard coded when other predictors are present\n\n\nCall:\nlm(formula = kid_score ~ mom_hs, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   77.548      2.059  37.670  &lt; 2e-16 ***\nmom_hs        11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nRemember that the mother’s who did not complete high school are represented in the intercept term, so the coefficient for mom_hs represents the difference in test scores between 3 year olds whose mothers did not and who did complete high school - a difference of approximately 12 points."
  },
  {
    "objectID": "PSYC234/Week3.html#transform",
    "href": "PSYC234/Week3.html#transform",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Transform",
    "text": "Transform\nBut the variable was unbalanced, remember. Ninety-three moms had not completed high school compared to 341 who had, and it was recommended by Gelman and colleagues (2020) to code those variables as 0 and 1 and then take away the mean - effectively centring your binary categorical variable:\n\nd$c_mom_hs &lt;- d$mom_hs - mean(d$mom_hs) # mean centring mom_hs\n\nhead(d$c_mom_hs) # positive values = mom completed high school\n\n[1]  0.2142857  0.2142857  0.2142857  0.2142857  0.2142857 -0.7857143"
  },
  {
    "objectID": "PSYC234/Week3.html#model-1",
    "href": "PSYC234/Week3.html#model-1",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Model",
    "text": "Model\n\nsummary(m1_c &lt;- lm(kid_score ~ c_mom_hs, d)) # by default, lm() will model the \"1\" so it does not needed to be hard coded when other predictors are present\n\n\nCall:\nlm(formula = kid_score ~ c_mom_hs, data = d)\n\nResiduals:\n   Min     1Q Median     3Q    Max \n-57.55 -13.32   2.68  14.68  58.45 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   86.797      0.953  91.082  &lt; 2e-16 ***\nc_mom_hs      11.771      2.322   5.069 5.96e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.85 on 432 degrees of freedom\nMultiple R-squared:  0.05613,   Adjusted R-squared:  0.05394 \nF-statistic: 25.69 on 1 and 432 DF,  p-value: 5.957e-07\n\n\nNow the intercept is for test scores with mom’s high school completion at their mid value: the average value of kid_score has returned, because the intercept reflects the average when the predictors are at 0 - the grand mean.\nLets do the math: predicted kid score = intercept + (\\(b_1\\) * mom_hs status):\n\n86.797 + (11.771 * 0.2142857) # predicted score for child who mom completed hs\n\n[1] 89.31936\n\n86.797 + (11.771 * -0.7857143) # predicted score for child whose mom did not complete hs\n\n[1] 77.54836\n\n\nThe predicted score for children whose mom didn’t complete high school in model 1_c is the same as the predicted score in model 1 where the intercept reflects that group."
  },
  {
    "objectID": "PSYC234/Week3.html#communicate",
    "href": "PSYC234/Week3.html#communicate",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Communicate",
    "text": "Communicate\nIf this was the only model we constructed, we could interpret the model like this:\nOur model of mother’s high school predicting test scores is significant (F(1, 432) = 25.69, p &lt; .001). The proportion of variance on test scores explained by the model is approximately 6%. Mother’s educational status significantly predicts kids’ test scores (t = 5.07, p &lt; .001). We can reject the null hypothesis that the status of mom’s high school education has no effect on kids_score. The difference between a child’s test score when mom did not complete high school and a child’s test score when mom did complete high school is 11.77 points.\nWhen you get a model output, try interpreting it and writing it out. You can use the above as a template / model upon which to base your early attempts. If you are unsure about what happens, compute the maths.\n\nComparing models\nBut when we make a series of models, we don’t interpret every model fully each time. Before we interpret a model, we need to make a decision about which is the better model.\nTwo ways of thinking about making a series of regression models:\n1: A principled method is to construct the model that is indicated by the research questions and report that model. This may be in the context of a confirmatory analysis or be regarded as a theory guided approach.\n2: When a line of research is very new, data may be collected in an exploratory research framework. The data may be analysed to reveal patterns that then lead to research questions and confirmatory research designs. Model comparison is a method that can guide model choice. Essentially, it will help you decide if a more complex model (a model with more predictors or independent & interaction terms) is warranted.\nm0 above is an intercept model. m1 and m1_c are equivalent models - it’s just that the predictor has been transformed in m1_c. These are models with the intercept term plus one predictor. m1 is the more complex model of the two.\nIt is much better to choose a simpler model when two models differ only on the number of predictors but pretty much explain the same amount of variance in the outcome variable.\nThe difference between m0 and the others seems to be about 5% of the explained variance (m1 \\(R^2\\) = 5%), so they seem to be doing a better job of predicting test scores. But we can check this using an inferential test.\nWe can check how models compare to each other using the anova() function: I will demonstrate first rather than explain so that we have something concrete to refer to:\n\nanova(m0, m1) # test for a difference between the models\n\nAnalysis of Variance Table\n\nModel 1: kid_score ~ 1\nModel 2: kid_score ~ mom_hs\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    433 180386                                 \n2    432 170261  1     10125 25.69 5.957e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nThe output shows an analysis of variance table - you have seen these before. Our two models are model 0 and model 1 in the raw variables, look at the formula lines of Model 1 and Model 2, and the anova function has used each of the models as if they are factors, like in an ANOVA study.\nThe degrees of freedom are from the models, as are the residual sums of squares (RSS):\n\nmetrics_m0 &lt;- augment(m0)\nmetrics_m0$RSS &lt;- metrics_m0$.resid*metrics_m0$.resid\nsum(metrics_m0$RSS)\n\n[1] 180386.2\n\nmetrics_m1 &lt;- augment(m1)\nmetrics_m1$RSS &lt;- metrics_m1$.resid*metrics_m1$.resid\nsum(metrics_m1$RSS)\n\n[1] 170261.2\n\n\nAnd the F ratio tests calculates the difference between these two values, to see if the difference in the variance explained is significantly different from zero.\nI said to you that the models m1 and m1_c were equivalent. Lets check the ANOVA table for the model m1_c in comparison with m1\n\nanova(m1, m1_c) # test for a difference between the models\n\nAnalysis of Variance Table\n\nModel 1: kid_score ~ mom_hs\nModel 2: kid_score ~ c_mom_hs\n  Res.Df    RSS Df Sum of Sq F Pr(&gt;F)\n1    432 170261                      \n2    432 170261  0         0         \n\n\nThere is no difference.\n\nanova(m0, m1_c) # test for a difference between the models\n\nAnalysis of Variance Table\n\nModel 1: kid_score ~ 1\nModel 2: kid_score ~ c_mom_hs\n  Res.Df    RSS Df Sum of Sq     F    Pr(&gt;F)    \n1    433 180386                                 \n2    432 170261  1     10125 25.69 5.957e-07 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nAnd the comparison between m0 and m1_c gives exactly the same output as the ANOVA for m0 and m1.\nTo interpret these kinds of model comparison tests:\nModel 1 is the first model in the list that we give to the ANOVA function, Model 2 is the second, and so on. We need to check that one of the models differs from the other model in complexity by checking the Df column. Here, it reads as 1 so model two is more complex by one predictor. And then we see that the p value is &lt; .001. The variance explained by adding the additional predictor is significantly different from the variance explained by the smaller model. The significant difference, justifies the added complexity, so the more complex model is preferred.\nAnd then we choose whether we build another model. Can you see how the visualise - transform - model cycle iterates again and again in the analysis workflow?\nNext we’ll add mom_age at birth of the child. I am choosing to do this, reasoning that the it may be more likely for younger mothers to have not completed high school. What is the range of the age of mom’s at the time they had their children?"
  },
  {
    "objectID": "PSYC234/Week3.html#visualise-1",
    "href": "PSYC234/Week3.html#visualise-1",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Visualise",
    "text": "Visualise\n\nrange(d$mom_age) # show the minimum and maximum values in mom_age\n\n[1] 17 29\n\n\nThis is a feasible relationship. American high school finishes at 18 years of age.\nBecause we have centred the first variable, it makes sense to centre this variable also. And, the intercept term would otherwise represent age at 0 years, which also doesn’t make sense, right?"
  },
  {
    "objectID": "PSYC234/Week3.html#transform-1",
    "href": "PSYC234/Week3.html#transform-1",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Transform",
    "text": "Transform\n\nd$c_mom_age &lt;- d$mom_age - mean(d$mom_age) #\n\nAnd then build a new model by adding it to the previous model formula:"
  },
  {
    "objectID": "PSYC234/Week3.html#model-2",
    "href": "PSYC234/Week3.html#model-2",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Model",
    "text": "Model\n\nsummary(m2_c &lt;- lm(kid_score ~ c_mom_hs + c_mom_age, d))\n\n\nCall:\nlm(formula = kid_score ~ c_mom_hs + c_mom_age, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-57.980 -12.545   2.057  14.709  59.325 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  86.7972     0.9532  91.063  &lt; 2e-16 ***\nc_mom_hs     11.3112     2.3783   4.756  2.7e-06 ***\nc_mom_age     0.3261     0.3617   0.902    0.368    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 19.86 on 431 degrees of freedom\nMultiple R-squared:  0.05791,   Adjusted R-squared:  0.05353 \nF-statistic: 13.25 on 2 and 431 DF,  p-value: 2.614e-06\n\n\nSo for every year that a child’s mother is older than the average aged mother, the child’s test score is increased by 0.32 of a point. The effect is non-significant (p = .368).\nI’m betting that m2 is not a better model than m1_c given the weak prediction that c_mom_age seems to have on kid’s score.\nWe check again with a model comparison:\n\nanova(m1_c, m2_c)\n\nAnalysis of Variance Table\n\nModel 1: kid_score ~ c_mom_hs\nModel 2: kid_score ~ c_mom_hs + c_mom_age\n  Res.Df    RSS Df Sum of Sq     F Pr(&gt;F)\n1    432 170261                          \n2    431 169941  1    320.54 0.813 0.3678\n\n\nSo, very quickly we have a demonstration of when a model comparison shows that the more complex model does not do a better job of explaining variance in the outcome variable. With a p = .368, we fail to reject a null hypothesis of no difference and the simpler model, m1_c, remains the preferred model.\nYou probably already knew that by looking at m2_c’s \\(R^2\\) value ,(m2_c \\(R^2\\) = 5.8%), but the inferential test makes it official.\nAnd, if you think about it - there could be an explanation. The “value” of school to the “mom” is in the first variable. And the lower values of the age range are 17 and 18 - which suggests that there are observations in the data that represent mums who didn’t finish school - both variables contain a similar type of information.\nThe visualisation of the data we did earlier, hints at this; lets repeat this again to have a closer look:\n\nchart.Correlation(d[, c(2:4)], histogram=TRUE, pch=19)\n\n\n\n\nif we change the code, ever so slightly, we can remove mom’s iq from the image so that the information stands out to you:\n\nchart.Correlation(d[, c(2, 4)], histogram=TRUE, pch=19)\n\n\n\n\nmom_hs and mom_age at birth have a small correlation of r = 0.21. Which tells you that the two variables are measuring a similar construct to a weak extent.\nThe astute student is going “yeah but they’re not the variables in the model - we used the centred variables.”\nA quick change from c(2, 4) to c(5, 6) to select the matching columns for those variables:\n\nchart.Correlation(d[, c(5, 6)], histogram=TRUE, pch=19)\n\n\n\n\nRemember - centring and standardising your variables does not change the relationships between your variables. It changes how your model works with the data and makes your model easier to interpret."
  },
  {
    "objectID": "PSYC234/Week3.html#adding-mom_iq",
    "href": "PSYC234/Week3.html#adding-mom_iq",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Adding mom_iq",
    "text": "Adding mom_iq\nLet’s add mom_iq, because a supporting hypothesis could be that mom’s IQ is a hereditary trait? Or even that given a level of IQ, mom’s completion of high school is affected which in turn may predict test scores…"
  },
  {
    "objectID": "PSYC234/Week3.html#transform-using-a-population-mean-rather-than-the-variable-mean",
    "href": "PSYC234/Week3.html#transform-using-a-population-mean-rather-than-the-variable-mean",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Transform using a population mean rather than the variable mean",
    "text": "Transform using a population mean rather than the variable mean\n..and again, we can centre the variable. IQ has a meaningful mean at the population level (100) - so we could use that value to mean centre mom_iq (and it just so happens that the mean of the sample is identical: sample mean = 100).\n\nd$c_mom_iq &lt;- d$mom_iq - mean(d$mom_iq)"
  },
  {
    "objectID": "PSYC234/Week3.html#model-3",
    "href": "PSYC234/Week3.html#model-3",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Model",
    "text": "Model\nSo now the intercept reflects the test score for the child at the average level of high school completion, average age and average IQ for the mothers.\n\nsummary(m3_c &lt;- lm(kid_score ~ c_mom_hs + c_mom_age + c_mom_iq, d))\n\n\nCall:\nlm(formula = kid_score ~ c_mom_hs + c_mom_age + c_mom_iq, data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-53.289 -12.421   2.399  11.223  50.169 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 86.79724    0.87108  99.643   &lt;2e-16 ***\nc_mom_hs     5.64715    2.25766   2.501   0.0127 *  \nc_mom_age    0.22475    0.33075   0.680   0.4972    \nc_mom_iq     0.56254    0.06065   9.276   &lt;2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 18.15 on 430 degrees of freedom\nMultiple R-squared:  0.215, Adjusted R-squared:  0.2095 \nF-statistic: 39.25 on 3 and 430 DF,  p-value: &lt; 2.2e-16\n\n\nWith the addition of the c_mom_iq, the model remains significant and a larger amount of variance is explained (21.5%). c_mom_iq is also a significant predictor in the model (t = 9.276, p &lt; .001). It shows a positive relationship with kids score, with every point increase in IQ, a kid’s score improves by 0.6 points. Also, look at what has happened to the other coefficients - reduced in size. c_mom_iq, while having its own independent effect, seems to have an effect upon the other coefficients - suggestive of interactions."
  },
  {
    "objectID": "PSYC234/Week3.html#model-comparison",
    "href": "PSYC234/Week3.html#model-comparison",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Model comparison",
    "text": "Model comparison\n\nanova(m1_c, m3_c) \n\nAnalysis of Variance Table\n\nModel 1: kid_score ~ c_mom_hs\nModel 2: kid_score ~ c_mom_hs + c_mom_age + c_mom_iq\n  Res.Df    RSS Df Sum of Sq      F    Pr(&gt;F)    \n1    432 170261                                  \n2    430 141605  2     28656 43.509 &lt; 2.2e-16 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nI have performed model comparison using m1_c rather than m2_c, because the last model comparison showed that m2_c was not a better model fit for the data. We could have removed the c_mom_age variable, but I had a feasible reason for it to be included, and it feels like p-hacking to put it in and then take it out just because it was not significant.\nAnyway, m3_c is a significantly better fit than m1_c (F(2, 430) = 43.51, p &lt; .001), so we will take this one forward as the new preferred model and introduce an interaction term."
  },
  {
    "objectID": "PSYC234/Week3.html#adding-an-interaction-term",
    "href": "PSYC234/Week3.html#adding-an-interaction-term",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Adding an interaction term",
    "text": "Adding an interaction term\nIt felt like a long time getting here - sorry about that, but think about it - the steps along the way have been like setting up your different levels of ANOVA, and that took a couple of weeks too.\nFor instance:\nM1 = one way ANOVA M2 = if you had young and old groups for age, instead of a continuum of age, you would have a two way ANOVA M3 = similarly, if you had low and high IQ, instead of a continuum, you would have three factors and a three way ANOVA\nHopefully you can see how ANOVA and regression are linked. Not quite equivalent but linked.\nWe will introduce the interaction between the centred variables of c_mom_hs and c_mom_iq. We write it like this: c_mom_hs:c_mom_iq. This piece of code is transforming two variables into one interaction term in the model."
  },
  {
    "objectID": "PSYC234/Week3.html#model-4",
    "href": "PSYC234/Week3.html#model-4",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Model",
    "text": "Model\n\n(m4_summary &lt;- summary(m4_c &lt;- lm(kid_score ~ c_mom_hs + \n                                  c_mom_age + \n                                  c_mom_iq + \n                                  c_mom_hs:c_mom_iq, d))) # save the output for use in inline code for the model reporting\n\n\nCall:\nlm(formula = kid_score ~ c_mom_hs + c_mom_age + c_mom_iq + c_mom_hs:c_mom_iq, \n    data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.709 -11.394   1.939  11.268  43.839 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       87.67680    0.90811  96.549  &lt; 2e-16 ***\nc_mom_hs           2.22584    2.49372   0.893  0.37258    \nc_mom_age          0.35236    0.33008   1.068  0.28634    \nc_mom_iq           0.58735    0.06058   9.695  &lt; 2e-16 ***\nc_mom_hs:c_mom_iq -0.50607    0.16347  -3.096  0.00209 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.97 on 429 degrees of freedom\nMultiple R-squared:  0.2321,    Adjusted R-squared:  0.225 \nF-statistic: 32.42 on 4 and 429 DF,  p-value: &lt; 2.2e-16\n\n\nBefore we interpret anything, lets see if adding the interaction term is justified by performing the model comparison:\n\nanova(m3_c, m4_c) \n\nAnalysis of Variance Table\n\nModel 1: kid_score ~ c_mom_hs + c_mom_age + c_mom_iq\nModel 2: kid_score ~ c_mom_hs + c_mom_age + c_mom_iq + c_mom_hs:c_mom_iq\n  Res.Df    RSS Df Sum of Sq      F   Pr(&gt;F)   \n1    430 141605                                \n2    429 138511  1    3094.3 9.5838 0.002092 **\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nm4_c has the interaction term as the model with added complexity. The difference between m3_c and m4_c is significant (F (1, 429) = 9.58, p = .002). m4_c is the preferred model."
  },
  {
    "objectID": "PSYC234/Week3.html#communicate-1",
    "href": "PSYC234/Week3.html#communicate-1",
    "title": "3. Multiple regression models that include interactions (moderated variables)",
    "section": "Communicate",
    "text": "Communicate\nSo what does the model output say? Lets reprint the summary to reduce the need for scrolling.\n\nm4_summary\n\n\nCall:\nlm(formula = kid_score ~ c_mom_hs + c_mom_age + c_mom_iq + c_mom_hs:c_mom_iq, \n    data = d)\n\nResiduals:\n    Min      1Q  Median      3Q     Max \n-52.709 -11.394   1.939  11.268  43.839 \n\nCoefficients:\n                  Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)       87.67680    0.90811  96.549  &lt; 2e-16 ***\nc_mom_hs           2.22584    2.49372   0.893  0.37258    \nc_mom_age          0.35236    0.33008   1.068  0.28634    \nc_mom_iq           0.58735    0.06058   9.695  &lt; 2e-16 ***\nc_mom_hs:c_mom_iq -0.50607    0.16347  -3.096  0.00209 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 17.97 on 429 degrees of freedom\nMultiple R-squared:  0.2321,    Adjusted R-squared:  0.225 \nF-statistic: 32.42 on 4 and 429 DF,  p-value: &lt; 2.2e-16\n\n\nLook once more at the coefficients. c_mom_hs is no longer significant; c_mom_age remains non-significant, as does c_mom_iq, and the new predcitor, the interaction between mother’s high school graduation status and IQ is signfificant, showing a negative relationship.\nFrom earlier: When an interaction term is entered into a model and is found to be significant (p &lt; .05), this means that the rate of change in the outcome variable changes as the levels in the predictor variables change. We have to consider the independent effects of the predictors that are in the interaction term and infer whether the interaction effect is a weakening or a strengthening of the outcome variable.\nFurther, most of the focus for interpretation will now go onto the interaction term rather than the independent level effects of those predictors. We can no longer, usefully interpret the independent effects of high school graduation status and IQ as independent predictors because their rates of change are NOT uniform across all levels of predictor.\nWhen you have performed a model comparison:\n\nTell your readers which model was preferred first.\n\nThen describe the significance of the whole model…\n…and then describe a range of predictors.\n\nOnce you have described an individual predictor, interpret it in real terms i.e. what the variables mean:\n\nIf you present the full model summary in a table, you don’t have to interpret all the coefficients in a paragraph. So here we will just talk about the significant predictors, and imagine that we would be putting the full model summary in a table.\nExample:\n\nWe built a series of models, with the model containing the interaction term being preferred by model comparison (F(1, 429) = 9.58, p = .002).\n\nA multiple linear regression with a combination of the mother’s high school completion status, age at child’s birth, mother’s intelligence score and an interaction between mother’s high school completion and mother’s intelligence score was a significant model (F (4, 429) = 32.42, p &lt; .001) and explained 23.2% of the variance in kids’ test scores.\nNeither high school completion status nor mother’s age at the child’s birth was significant (p = 0.373 and p = 0.286). Mother’s intelligence was significant (t = 9.7, p &lt; .001) and so was the interaction term between high school completion status and mother’s intelligence (t = -3.1, p = .002).\nThe relationship between test scores and mother’s high school graduation status and test scores and mothers IQ is positive, suggesting that high school graduation status means that test scores are higher on average he positive relationship that we see between test scores and mother’s intelligence predicts that for each point increase in IQ, the test score for a child who’s mother did not complete school increases by 0.59 of a test point. And while having a mother that completed high school increases test scores by 2.23 points, the negative direction of the interaction between high school successfully completed and a point increase in IQ actually weakens the strength of this effect by 0.51.\n\n\nModel diagnostics\n\npar(mfrow = c(2, 2)) # display plots in a 2 x 2 panel\nplot(m4_c)\n\n\n\n\nphew! looks a little healthier than the last dataset analysis!\n\n\nA possible reporting and interpretation of the model\nOur multiple regression modelled 3-year-old test scores as a function of whether their mothers completed high school, the mother’s age at the child’s birth and their IQ score. We also included an interaction term for the relationship between high school completion and IQ score. We tentatively framed feasible research questions and built a series of models, using model comparison to select models to carry forward. All variables were centred before being entered into the model.\nThe full model with all independent predictors plus the interaction term was significant (\\(F{_(}{_4}{_,}{_4}{_2}{_9}{_)}\\) = 32.42, p &lt; 0.001). and the best model of the four completed (p = .002). This set of predictors explains 23.21% of the variance in child test scores. While the status of the mother’s high school completion status was not significant (mom_hs: t = 0.89, p = 0.373), the mother’s IQ score was (mom_iq: t = 9.7, p &lt; .001). This showed a positive relationship with the child test scores, indicating that as the mother’s IQ increased, so to did the test scores. The interaction however, showed a negative relationship which indicated that this increase was shallower for those children whose mother had completed high school (hs x iq interaction: t = -3.1, p = 0.002).\nWe inspected diagnostic plots which show reasonable dispersion of residuals. There are a number of points that could be influential or show high leverage however they don’t appear to be aberrant.\n\n\nPlotting the raw values to show the interaction\nThis is the same code and plot as we started with at the top of the script. Repeated here.\n\nm4_raw &lt;- lm(kid_score ~ mom_hs + mom_age + mom_iq + mom_hs:mom_iq, d)\n\n\nggplot(d, aes(mom_iq, kid_score)) +\n  geom_point(aes(color = factor(mom_hs)), show.legend = FALSE) +\n  geom_abline(\n    intercept = c(coef(m4_raw)[1], sum(coef(m4_raw)[1:2])),\n    slope = c(coef(m4_raw)[4], sum(coef(m4_raw)[4:5])),\n    color = c(\"black\", \"grey\")) +\n  scale_color_manual(values = c(\"black\", \"grey\")) +\n  labs(x = \"Mother IQ Score\", y = \"Child Test Score\") + \n  theme_bw()\n\n\n\n\n\n\nPlotting the fitted model\n\nggplot(d, aes(c_mom_iq, kid_score)) +\n  geom_point(aes(color = factor(c_mom_hs)), show.legend = FALSE) +\n  geom_abline(\n    intercept = c(coef(m4_c)[1], sum(coef(m4_c)[1:2])),\n    slope = c(coef(m4_c)[4], sum(coef(m4_c)[4:5])),\n    color = c(\"black\", \"grey\")) +\n  scale_color_manual(values = c(\"black\", \"grey\")) +\n  labs(x = \"Mother IQ score\", y = \"Child test score\") + \n  theme_bw()\n\n\n\n\nI’ve taken this code from the website from which the data was retrieved - it is a much cleaner plot and great code! I have adapted the code to fit the model fitted and the raw data above and plotted two versions to demonstrate that the transformations do not change the information about the comparisons / effects within the model - only the scale of measurement as observed on the plot axes.\nThe grey circles represent the group of children whose mothers completed high school (HS) while the black circles represent the group of children whose mothers did not complete high school (no HS). The interaction indicates that while the mother’s IQ score is below a score of approximately 105, the no HS child test scores are lower than the HS child test scores, however rising above approximately 105 IQ points, the no HS child test scores are higher than the HS child test scores. One interpretation could be that having an above average IQ score, in the context of not completing high school is a protective factor for an offspring’s attainment at the age of 3!\nHow are we doing in our list of things to think about - We need to look at:\n\na model with some significant predictors! - done\ncorrelation matrices - done\ncentering predictors - done\nstandardising predictors - done\nmodels with categorical predictors - done & ongoing\n\ninterpreting models with categorical predictors - done & ongoing\n\nmodels with interaction terms - done\nthe properties of multicollinearity\nchoosing between different models - done\nreporting models - ongoing"
  },
  {
    "objectID": "PSYC234/Week1.html",
    "href": "PSYC234/Week1.html",
    "title": "1. Review of correlation, simple regression and demonstration of multiple regression",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week1.html#materials",
    "href": "PSYC234/Week1.html#materials",
    "title": "1. Review of correlation, simple regression and demonstration of multiple regression",
    "section": "Materials",
    "text": "Materials\nThe lecture materials, including and and R files are downloaded from here as a zip file. You can upload the zip file directly on to the R server and it will populate a new folder with the files and data files automatically."
  },
  {
    "objectID": "PSYC234/Week1.html#what-happens",
    "href": "PSYC234/Week1.html#what-happens",
    "title": "1. Review of correlation, simple regression and demonstration of multiple regression",
    "section": "What happens",
    "text": "What happens\n\nIn each lab:\n\n\nModel output, or model plot with dataset\nIn small groups, work backwards to plan the analysis script that produced the model output\n\nUse lab time/staff to check plan\n\n\n\nIndependently of the lab:\n\n\nGenerate the script as a group\n\nHelp each other\nScrape code from all your statistics modules\n\nThe internet\nR books\n\n\n\n\nSubmit for feedback (optional)\n\n\nSubmit as group\nRandom sample of scripts will get feedback\nNo credit here – learning process as preparation for your own project!\nYou can see all scripts per week if you submit a script per week\n\nYou only get access to view when you submit a script – to reduce freeloading behaviour\nI will give feedback on 20% sample or maximum of 3 per lab group\nI will post the submitted scripts each week on moodle.\n\nWhy should you contribute here?\n\nYou are a massive community and can support each other going forward\nBreaks down the idea that there is one correct way to write an analysis script\nGive you greater exposure to code that shares a purpose\nSome students have asked for this potential for feedback on scripts – not just analysis"
  },
  {
    "objectID": "PSYC234/Week1.html#planning-data-analysis",
    "href": "PSYC234/Week1.html#planning-data-analysis",
    "title": "1. Review of correlation, simple regression and demonstration of multiple regression",
    "section": "Planning Data Analysis",
    "text": "Planning Data Analysis\nData analysis requires the following steps\n\n\n\nAnalysis Workflow\n\n\nWith each step requiring its own blocks of code. Specifically, the communication sep requires us to produce plots and model summaries along wit text to help other people understand our research and what was completed.\n\n\n\nAnalysis Workflow\n\n\nModel plots and summaries are the output from our statistical analyses and look similar to the example below:\n\n\n\nAnalysis Workflow\n\n\nWe, as communicators, need to present this information in text form too. So from the above figure and summary we can create a body of text. Text equating to a paragraph reporting model significance, then coefficients (emphasised in italics and bold below as: statistics + verbal description).\nExample of model significance: The model predicting children’s test scores from mother’s graduation status, mother’s age and IQ was significant (F (3, 430) = 39.25, p &lt; .001, R2 = 0.21).\nExample text for the ‘c_mom_hs’ coefficient:\nThe difference for whether the mother graduated high school or not is significant (b = 5.65, SE = 2.26, p = .013). Holding all other predictors constant, the difference in scores for a child whose mother finished high school and a child whose mother did not finish high school is 5.64 test points."
  },
  {
    "objectID": "PSYC234/Week1.html#staff-role-in-labs",
    "href": "PSYC234/Week1.html#staff-role-in-labs",
    "title": "1. Review of correlation, simple regression and demonstration of multiple regression",
    "section": "Staff role in labs",
    "text": "Staff role in labs\n\nGet us to check your planning / answer your questions.\nWe will:\n\nIdentify gaps\nTell you that there is an error somewhere with an indication of where to look more closely\nGive you hints\nExtend your thinking if you are working at your best level already….\n\nWe won’t give you the answers straight out. Because most of your project experience is going to be about problem solving"
  },
  {
    "objectID": "PSYC234/Week9.html",
    "href": "PSYC234/Week9.html",
    "title": "9. Expanding on binary logistic regression",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week9.html#research-question-1",
    "href": "PSYC234/Week9.html#research-question-1",
    "title": "9. Expanding on binary logistic regression",
    "section": "Research Question 1",
    "text": "Research Question 1\nYou are interested in factors that predict academic achievement in mathematics. Children’s academic achievement can be rated as below expected, at expected or above expected.\nThe predictors you are interested in are:\n\nNumber if hours spent revising (continuous)\nLikes school (Yes/no)\nFavourite subject (Maths, English or Science)\n\nFor the categorical predictors:\n-Set the reference category for Likes school as “No” -Set the reference category for Favourite subject as “Maths”\n\n\n\n\n\n\nTip\n\n\n\nSee last week’s content for how to set the reference category. Or better yet, revisit your own script to see how you did it last time."
  },
  {
    "objectID": "PSYC234/Week9.html#research-question-2",
    "href": "PSYC234/Week9.html#research-question-2",
    "title": "9. Expanding on binary logistic regression",
    "section": "Research Question 2",
    "text": "Research Question 2\nYou work in a nursery. In the nursery, there has been an outbreak of measles. You are interested in factors that predict whether a child in your nursery will have measles (yes/no).\nThe predictors you are interested in are:\n\nNumber of hours spent at nursery weekly (continuous)\nHas siblings(Yes/no)\nVaccinated against measles (Yes/no)\n\nFor the categorical predictors:\n\nSet the reference category for Siblings as “No”\nSet the reference category for Vaccinated as “No”\n\nFor the outcome (measles – yes/no):\n\nSet No as 0, and Yes as 1"
  },
  {
    "objectID": "PSYC234/Week9.html#steps",
    "href": "PSYC234/Week9.html#steps",
    "title": "9. Expanding on binary logistic regression",
    "section": "Steps",
    "text": "Steps\n\nStart a new session\nSome packages loaded in have the same function – these needed to be inputted in a certain order\nUse the blank R/R Markdown script I’ve uploaded as a starting point\n\n\nPrepare our data for analysis\nExplore our data\nRun the model\nEvaluate the model\nEvaluate the individual predictors\nPredicted probabilities\nInterpret the output"
  },
  {
    "objectID": "PSYC234/Week8.html",
    "href": "PSYC234/Week8.html",
    "title": "8. Binary logistic regression models",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week8.html#research-question",
    "href": "PSYC234/Week8.html#research-question",
    "title": "8. Binary logistic regression models",
    "section": "Research Question",
    "text": "Research Question\nYou are interested in whether the country an individual lives (UK/Australia) predicts reptile ownership (Yes/No).\nIn the dataset, the outcome variable (reptile) is coded as “Y” and “N”:\n\nY = Yes\nN = No\n\nTo make sure we should all end up with the same output, set UK as your reference category\n\n\n\n\n\n\nHint\n\n\n\n\n\nNot sure how to set the reference category? Have a look at the recode() function from dplyr (tidyverse). Either search in the Help tab on the right, or type ?recode in the console below. Have a read of the second and third paragraph in the description and check out the examples at the bottom of the help pane.\nOther options exist too, you can look at relevel() or using the levels argument inside the function as.factor() or factor(). Check these out in the same way as described above."
  },
  {
    "objectID": "PSYC234/Week8.html#steps",
    "href": "PSYC234/Week8.html#steps",
    "title": "8. Binary logistic regression models",
    "section": "Steps",
    "text": "Steps\n\nPrepare our data for analysis\nExplore our data\nRun the binary logistic regression model\nEvaluate the model\nEvaluate the individual predictors\nPredicted probabilities\nInterpret the output"
  },
  {
    "objectID": "PSYC234/index.html",
    "href": "PSYC234/index.html",
    "title": "From association to modelling causality",
    "section": "",
    "text": "Teaching team: Emma Mills (module coordinator) and Amy Atkinson\nPSYC234 builds upon your prior learning of t-tests (PSYC121), correlation and simple regression (PSYC122) and ANOVA (PSYC214). All of these can be thought of as components that use parts of a statistical framework called the “general linear model” and PSYC234 moves into models of multiple linear regression. The “multiple” part here means more than one predictor variable.\nThe linear model is extremely flexible in that it can use continuous and categorical predictors, and can model continuous (week 11 - 14) and categorical (week 18 & 19) outcome variables. Rather than conducting planned comparisons or post hoc tests as we do when using ANOVA techniques, the planned comparisons can be structured into the regression model. We can also fit interactions and non-linear relationships within the linear model.\nNon-parametric statistical techniques (week 16 - 17): These are a range of techniques that are called upon when our data violates the assumptions upon which parametric models are built. The t-test, correlation, ANOVA and regression are all parametric tests, so it is a good back-up to have knowledge of the alternative methods, should you find your data does not meet parametric analysis method assumptions.\nPSYC234 is also the last statistics module before the independent work of your final year project begins, so throughout the labs there is a focus on independent R script generation while practising construction, interpretation and reporting of data analyses. Each lab will be centred around a new dataset, a research output (either model summaries or effects plots) and small group work will involve the construction of the analysis steps to recreate the output.\nWe encourage you to try writing your own scripts outside of the lab scheduled slot, and use the lab time to clarify the teaching team’s experience, asking questions about analysis steps, getting feedback on code. So the focus in labs is much more on the analytic workflow process, problem solving using the given dataset and R code, rather than structured worksheets for which you generate an output to answer questions. This way of working is very much the way that you will work with your own data that you collect for your third year project, so it is a good chance to rehearse ways of working and get a feel for practising data analysis with R away from the lab space, using your peers and the wider R community for support.\nIt is a different way of working from your previous statistics modules. There may be many methods to get to the same end result - which is okay. Hopefully, over the nine weeks of labs, you will become more confident in your choices and skills taking you from generating descriptive statistics to inferential test results.\n\n\nWeeks 11-14: Multiple Linear Regression (all in the class test in week 20)\nLecturer: Emma Mills\nLecture 1: Review of correlation, simple regression and demonstration of multiple regression\nLecture 2: Multiple regression including categorical predictors and planned contrasts\nLecture 3: Multiple regression models that include interactions (moderated variables)\nLecture 4: Multiple regression and mediation\nWeek 15: Factor Analysis and Binomial Tests\nWe have split lecture 5 into two parts.\nLecture 5 Part 1: Factor analysis (Emma Mills) - not in the class test in week 20\nLecture 5 Part 2: Binomial test (Amy Atkinson) - in the class test in week 20\nWeeks 16-17: Non Parametric Tests - in the class test in week 20\nLecturer: Dr Amy Atkinson\nLecture 6: Wilcoxon rank-sum test and Wilcoxon signed-rank test\nLecture 7: Kruskal-Wallis test and Friedman’s ANOVA\nWeeks 18- 19: Non-linear regression models - in the class test in week 20.\nLecturer: Dr Amy Atkinson\nLecture 8: Binary logistic regression models.\nLecture 9: Multiple binary logistic regression & ordinal logistic regression models."
  },
  {
    "objectID": "PSYC234/Week2.html",
    "href": "PSYC234/Week2.html",
    "title": "2. Multiple regression including categorical predictors",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week2.html#part-1-multiple-regression-2-or-more-predictors",
    "href": "PSYC234/Week2.html#part-1-multiple-regression-2-or-more-predictors",
    "title": "2. Multiple regression including categorical predictors",
    "section": "Part 1: Multiple Regression: 2 or more predictors",
    "text": "Part 1: Multiple Regression: 2 or more predictors\n\nContinuous predictors:\n\n\\(Y = Intercept + predictor_{continuous} + predictor_{categorical} + Error\\)\n\\(Y = b_0 + b_1X_1 + b_2X_2 +𝜀\\)\n\nInterpretation of Intercept term (\\(b_0\\)): Average when all continuous predictors are at 0 and categorical predictors are at their reference level\nInterpretation for continuous coefficients: + A one unit increase in \\(X_1\\) gives a change in Y by the amount of \\(b_1\\) + A one unit increase in \\(X_2\\) gives a change in Y by the amount of \\(b_2\\)\n\n\nContinuous OR categorical predictors\n\\(Y = Intercept + predictor_1 + predictor_2 + Error\\) \\(Y = b0 + b_1X_1 + b_2X_2 +𝜀\\)\nInterpretation for continuous coefficients: A one unit increase in \\(X_1\\) gives a change in Y by the amount of \\(b_1\\)\nInterpretation for categorical coefficients: + A change to another category within \\(X_2\\) gives a change in Y by the amount of \\(b_2\\)\n\n\nBinary categorical variables\nThese are predictors with only two levels\n\nExamples of binary variables\n\n\nPredictor\nLevel 1\nLevel 2\n\n\n\n\nAccuracy\n0\n1\n\n\nAccuracy\nNo\nYes\n\n\nSmoker\nNo\nYes\n\n\nGroup\nControl\nTreatment\n\n\n\n\n\n\n\n\nRegression models cope with the categorical nature by assigning numbers and creating ‘contrasts’. + Default contrast scheme in R for binary predictors: + 0 to a level 1 + 1 to level 2 + this is known as ‘Dummy coding’ or ‘treatment coding’, and it automatically creates a ‘reference level’ = 0 which is the level that comes first in the alphabet/numerically. The reference level group is estimated in the Intercept coefficient.\nYou can allow R to automatically set your reference, which is fine for a balanced variable (with equal numbers of participants/stimuli in each level of the variable), but setting a different reference level may make your hypothesis easier to interpret + This may be useful in the context of factors with more than two levels + You can change the reference level by using the relevel() function to manually reorder the levels of your grouping variable"
  },
  {
    "objectID": "PSYC234/Week2.html#demonstration-multiple-regression-with-categorical-predictors",
    "href": "PSYC234/Week2.html#demonstration-multiple-regression-with-categorical-predictors",
    "title": "2. Multiple regression including categorical predictors",
    "section": "Demonstration: Multiple regression with categorical predictors",
    "text": "Demonstration: Multiple regression with categorical predictors\nIn the demonstration sections, you can follow along using the Multiple_Regression_Categorical_Predictors.Rmd file.\n\ndummy / treatment coding\nsum coding\n\nchanging the reference level\n\ncentring and standardising\n\nMake sure to load in the following libraries to follow along\n\nlibrary(tidyverse)\nlibrary(effects)\nlibrary(broom)\nlibrary(gridExtra)\nlibrary(PerformanceAnalytics)\n\n\nModel for multiple regression\nKeep the model for multiple regression in mind:\nYou have seen the two predictor regression model equation before:\n\\[\nY_i = b_0 + b_1 * X_1 + b_2*X_2 + e_i\n\\] - \\(X_1\\) and \\(X_2\\) can be categorical predictors here - there is no different notation between continuous and categorical predictors.\n\n\nImport the data\nThe data set is retrieved from https://www.kaggle.com/mirichoi0218/insurance\n\nd &lt;- read_csv(\"data/Wk2/_Week2_lecture_materials/insurance.csv\")\n\nLooking at the information on reading the data in, we have three variables that R has detected as character variables: sex, smoker and region. Brilliant - three categorical variables.\nWe also have three variables that are continuous: age, bmi, children\n\n\nTidy\nPurely for cosmetic reasons, I am going to change the region variable to acronyms of the geographic regions. This is for labeling of plots:\n\n# use nested ifelse() statements\nd$region &lt;- ifelse(d$region == \"northeast\", \"NE\",\n                   ifelse(d$region == \"northwest\", \"NW\",\n                          ifelse(d$region == \"southeast\", \"SE\", \"SW\"))) \n\nVery quickly, from the title of the dataset, it looks like several variables are being used as predictors for insurance charges. You can read a little more at the website above if you like.\n\n\nVisualise\nLets visualise the structure of the variables of the data set:\n\n# bar charts for categorical variables\np_sex &lt;- ggplot(d, aes(x = factor(sex), fill = factor(sex))) + geom_bar()\np_smok &lt;- ggplot(d, aes(x = factor(smoker), fill = factor(smoker))) + geom_bar()\np_reg &lt;- ggplot(d, aes(x = factor(region), fill = factor(region))) + geom_bar()\n\n# (so called) continuous variables\np_age &lt;- ggplot(d, aes(x = age)) + geom_density(fill = \"slateblue\")\np_age_hist &lt;- ggplot(d, aes(x = age)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi &lt;- ggplot(d, aes(bmi)) + geom_density(fill = \"red\")\np_child &lt;- ggplot(d, aes(children)) + geom_histogram()\ngrid.arrange(p_sex, p_smok, p_reg, p_age_hist, p_bmi, p_child)\n\n\n\n\nWe can also look at the relationships between each of the variables:\n\nchart.Correlation(d[, c(1, 3, 4)], histogram=TRUE, pch=19)\n\n\n\n\n\n\nModel\nwe’ll just add them all at this point:\n\nsummary(m6 &lt;- lm(charges ~ age + bmi + children + sex + smoker + region, d))\n\n\nCall:\nlm(formula = charges ~ age + bmi + children + sex + smoker + \n    region, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -11938.5      987.8 -12.086  &lt; 2e-16 ***\nage            256.9       11.9  21.587  &lt; 2e-16 ***\nbmi            339.2       28.6  11.860  &lt; 2e-16 ***\nchildren       475.5      137.8   3.451 0.000577 ***\nsexmale       -131.3      332.9  -0.394 0.693348    \nsmokeryes    23848.5      413.1  57.723  &lt; 2e-16 ***\nregionNW      -353.0      476.3  -0.741 0.458769    \nregionSE     -1035.0      478.7  -2.162 0.030782 *  \nregionSW      -960.0      477.9  -2.009 0.044765 *  \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,    Adjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16\n\n\nLets have a quick look at the output and the coefficient labels column. All of the categorical variables are toward the bottom of the list because we entered them toward the end of the list in the model.\n\nthe label for sex is sexmale, which tells us that the reference level for the sex variable is female - makes sense since the default is for the first group in the alphabet to be taken as the reference level group.\nsmoker becomes smokeryes to reflect that being a non-smoker is the reference level group.\n\nand so on…"
  },
  {
    "objectID": "PSYC234/Week2.html#part-2-reference-levels-in-the-regression-model",
    "href": "PSYC234/Week2.html#part-2-reference-levels-in-the-regression-model",
    "title": "2. Multiple regression including categorical predictors",
    "section": "Part 2: Reference levels in the regression model",
    "text": "Part 2: Reference levels in the regression model\nReference level in dummy coding becomes hidden in the intercept for the categorical predictor. It’s a good idea to write out what your intercept represents before you start to interpret your model. If dummy coding is used, switching from one level of the categorical predictor to the second level is the same as moving along one unit of a continuous predictor. It follows that for a binary categorical predictor, the slope term is giving you the mean difference between the two groups. If sum coding is used, switching from one level of the categorical predictor to the second level is the same as moving along two units of a continuous predictor\nMuch better choice if you are including interactions in your regression model. Instead of 0 and 1, -1 and +1 are used. Intercept is at 0 – + Like continuous predictors + Categorical predictor is now ‘centred’ (explained later) + This type of contrast can help for interpreting interactions that occur between categorical and continuous predictors"
  },
  {
    "objectID": "PSYC234/Week2.html#demonstration-types-of-categorical-contrasts---transformations",
    "href": "PSYC234/Week2.html#demonstration-types-of-categorical-contrasts---transformations",
    "title": "2. Multiple regression including categorical predictors",
    "section": "Demonstration: Types of categorical contrasts - transformations",
    "text": "Demonstration: Types of categorical contrasts - transformations\n\nDummy or Treatment Coding Scheme\nlets check how the model sees the categorical variables:\nat the moment, R sees the variables as character variables:\n\nstr(d)\n\nspc_tbl_ [1,338 × 7] (S3: spec_tbl_df/tbl_df/tbl/data.frame)\n $ age     : num [1:1338] 19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : chr [1:1338] \"female\" \"male\" \"male\" \"male\" ...\n $ bmi     : num [1:1338] 27.9 33.8 33 22.7 28.9 ...\n $ children: num [1:1338] 0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : chr [1:1338] \"yes\" \"no\" \"no\" \"no\" ...\n $ region  : chr [1:1338] \"SW\" \"SE\" \"SE\" \"NW\" ...\n $ charges : num [1:1338] 16885 1726 4449 21984 3867 ...\n - attr(*, \"spec\")=\n  .. cols(\n  ..   age = col_double(),\n  ..   sex = col_character(),\n  ..   bmi = col_double(),\n  ..   children = col_double(),\n  ..   smoker = col_character(),\n  ..   region = col_character(),\n  ..   charges = col_double()\n  .. )\n - attr(*, \"problems\")=&lt;externalptr&gt; \n\n\nTo check the type of contrast coding we need convert the variables to factor class:\n\nd &lt;- d %&gt;% \n  mutate(sex = factor(sex),\n         smoker = factor(smoker),\n         region = factor(region))\n\ncheck that the class has been changed through calling str() once more\n\nstr(d)\n\ntibble [1,338 × 7] (S3: tbl_df/tbl/data.frame)\n $ age     : num [1:1338] 19 18 28 33 32 31 46 37 37 60 ...\n $ sex     : Factor w/ 2 levels \"female\",\"male\": 1 2 2 2 2 1 1 1 2 1 ...\n $ bmi     : num [1:1338] 27.9 33.8 33 22.7 28.9 ...\n $ children: num [1:1338] 0 1 3 0 0 0 1 3 2 0 ...\n $ smoker  : Factor w/ 2 levels \"no\",\"yes\": 2 1 1 1 1 1 1 1 1 1 ...\n $ region  : Factor w/ 4 levels \"NE\",\"NW\",\"SE\",..: 4 3 3 2 2 3 3 2 1 2 ...\n $ charges : num [1:1338] 16885 1726 4449 21984 3867 ...\n\n\nNow we use the contrasts() function to see how R views them:\n\ncontrasts(d$sex) # reference level = female\n\n       male\nfemale    0\nmale      1\n\n\n\ncontrasts(d$smoker) # reference level = non-smoker\n\n    yes\nno    0\nyes   1\n\n\n\ncontrasts(d$region) # reference level = NE\n\n   NW SE SW\nNE  0  0  0\nNW  1  0  0\nSE  0  1  0\nSW  0  0  1\n\n\nYou can see from the rows that contain the zeroes (female, nonsmoker and northeast) that these reflect the reference levels automatically selected in the first full regression model summary print out above.\nAs a quick demonstration, you can control the reference level in the dummy coding command: Using the contr.treatment() function.\n\n(contrasts(d$region) = contr.treatment(4, base = 3)) # change ref level to southeast (row 3)\n\n  1 2 4\n1 1 0 0\n2 0 1 0\n3 0 0 0\n4 0 0 1\n\n\nLook at row three in the output - the row of zeros indicates that this is now the reference level. I’ll change it back before moving forward.\n\n(contrasts(d$region) = contr.treatment(4, base = 1)) # change ref level back to northeast (row 1)\n\n  2 3 4\n1 0 0 0\n2 1 0 0\n3 0 1 0\n4 0 0 1\n\n\nHas our model explained all the structured variance? Lets look at diagnostic plots before we go any further:\n\npar(mfrow = c(2, 2)) # display plots in a 2 x 2 panel\nplot(m6)\n\n\n\n\nSo, we know little about these variables, and we haven’t even looked at the individual coefficients in the summary outputs but we can see that the percentage of variance explained in the charges variable is greater in the 6 predictor model (75%) over the 3 predictor model (12%) but it is clear from the diagnostic plot above that there is still something not explained in the structure of the residuals.\n\n\nSum Coding Schemes\nSo, lets think about interactions - introduced in the context of ANOVA last term and thinking about many of the presentations in the 204 module at the end of last term, they are germain to the structure of experimental design whenever groups are included.\nWhat interactions could be plausible here? Just using what we know about medical problems and charges and the variables we have - could there be an interaction between bmi and smoker status? Do lifestyle choices cluster together to effect medical insurance charges? Does the number of children covered by insurance vary systematically by region?\nPlausible interactions from the perspective of a layman (me) and I am not going to model them here. But it may inform the choice of contrasting scheme. If we want to interpret interactions between variables later on - either a mix of continuous and categorical or both categorical predictors, it makes sense to use sum coding. Even though we have one categorical variable with four levels, we can still do this.\nIt’s a good idea to make copies of variables so that your original variables remain intact.\n\nd &lt;- d %&gt;% \n  mutate(sexsum = sex,\n         smokersum = smoker,\n         regionsum = region) # copy categorical variables to the same dataset with a suffix to denote sum coding status\n\n\ncontrasts(d$sexsum) # dummy coded right now\n\n       male\nfemale    0\nmale      1\n\n\n\n(contrasts(d$sexsum) &lt;- contr.sum(2)) # levels are now 1 and -1 and variable is now centred\n\n  [,1]\n1    1\n2   -1\n\n\n\n# repeat for the other two variables\n(contrasts(d$smokersum) &lt;- contr.sum(2)) # levels are now 1 and -1 and variable is now centred\n\n  [,1]\n1    1\n2   -1\n\n\n\n(contrasts(d$regionsum) &lt;- contr.sum(4)) # levels are now 1 and -1 but variable is not centred (because this isn't a binary variable)\n\n  [,1] [,2] [,3]\n1    1    0    0\n2    0    1    0\n3    0    0    1\n4   -1   -1   -1\n\n\nLets refit the regression model using this contrast scheme:\n\nsummary(m6_sum &lt;- lm(charges ~ age + bmi + children + sexsum + smokersum + regionsum, d))\n\n\nCall:\nlm(formula = charges ~ age + bmi + children + sexsum + smokersum + \n    regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n             Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   -666.94     968.64  -0.689 0.491239    \nage            256.86      11.90  21.587  &lt; 2e-16 ***\nbmi            339.19      28.60  11.860  &lt; 2e-16 ***\nchildren       475.50     137.80   3.451 0.000577 ***\nsexsum1         65.66     166.47   0.394 0.693348    \nsmokersum1  -11924.27     206.58 -57.723  &lt; 2e-16 ***\nregionsum1     587.01     293.11   2.003 0.045411 *  \nregionsum2     234.05     292.92   0.799 0.424427    \nregionsum3    -448.01     291.24  -1.538 0.124219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,    Adjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16\n\n\nThe intercept is massively reduced because:\n\nthe sum coding scheme has swapped the reference levels of the categorical predictors - For the sex and smoker coefficients, they now represent level 1s female and non-smokers.\n\nthe binary categorical variables are now centred. You can see that the estimates are halved from the m6 model that uses dummy / treatment contrast coding.\nThe binary coefficient estimates shows how much a change in Y for level 1 of the coefficient. A minus sign means it is smaller than the intercept term. No minus sign (a positive value) means it is more than the intercept term.\nTo find how much of a change for level -1 of the coefficient, you simply change the sign of the coefficient.\n\nLets do the math:\n\\[\nY_i = b_0 + b_1 * X_1 + b_2*X_2 + e_i\n\\] We are just going to do the maths for the sex variable for each level: female and male. The sex variable is our fourth predictor, so our maths equation looks like this:\n\\[\nY_i = b_0 + b_4 * X_4\n\\]\nWe need the:\n\nintercept term = \\(b_0\\) = (-666.94)\nthe coefficient value for sexsum1 (65.66) from the model output; this is the fourth predictor, so if we had written out our equation for all the predictors, it would be \\(b_4\\) (as above)\nwe also need the labels for the two levels of the sex predictor: female (1) and male (-1) because we are going to put those in our \\(X_4\\) part of the equation.\n\nHere we go:\n\n(InsuranceCharge_Female &lt;- -666.94 + (65.66 * 1))\n\n[1] -601.28\n\n\n\n(InsuranceCharge_Male &lt;- -666.94 + (65.66 * -1))\n\n[1] -732.6\n\n\nLets do the math for the smoker variable:\nThe smoker variable is our fifth predictor, so our maths equation looks like this:\n\\[\nY_i = b_0 + b_5 * X_5\n\\]\nWe need the:\n\nintercept term = \\(b_0\\) = (-666.94)\nthe coefficient value for smokersum1 (-11924.27) from the model output; this is the fifth predictor, so if we had written out our equation for all the predictors, it would be \\(b_5\\) (as above)\nwe also need the labels for the two levels of the smoker variable: no (1) and yes (-1) because we are going to put those in our \\(X_5\\) part of the equation.\n\nHere we go:\n\n(InsuranceCharge_nonsmoker &lt;- -666.94 + (-11924.27 * 1))\n\n[1] -12591.21\n\n\n\n(InsuranceCharge_smoker &lt;- -666.94 + (-11924.27 * -1))\n\n[1] 11257.33\n\n\n\nYou can compare the continuous predictor coefficients and see no difference for their coefficients from model m6.\n\nThe region variable is a little harder to interpret. Remember that the reference level is now the southwest level of the variable and each region coefficient is the difference from the intercept. So if we want to find the value for southwest, we have to take away region 1 2 and 3 from the intercept term.\n\n(InsuranceCharge_SW &lt;- -666.94 + (-587.01 * 1) + (234.05 * 1) + (-448.01 *1))\n\n[1] -1467.91"
  },
  {
    "objectID": "PSYC234/Week2.html#mean-centering-continuous-predictors",
    "href": "PSYC234/Week2.html#mean-centering-continuous-predictors",
    "title": "2. Multiple regression including categorical predictors",
    "section": "Mean Centering Continuous Predictors",
    "text": "Mean Centering Continuous Predictors\nIf you have unequal numbers of participants in your groups: + It may be better to transform your categorical variable to a numerical variable (sometimes called an indicator variable)  + and mean centre the new variable as if it was a continuous variable + Use ifelse() function ifelse(d$sex == \"female\", 1,0) + Use x_mean &lt;- x– mean(x) on the variable to mean centre d$sex_n - mean(d$sex_n)\nRemember that the raw intercept term represents the average value when all predictors are at zero, so the coefficients are interpreted as the change in Y for some value (larger than 0) the X predictor\nAn Intercept like this often doesn’t make sense for the verbal model, i.e. the research questions or our hypotheses. What does an intercept term of 0 years or 0 kgs mean?\nIf we mean-centre the continuous predictors, the Intercept now represents the average value when all continuous predictors are at their average value\nSubtract the mean value of a variable from every observation in that variable + The mean of the variable is then = 0 + All values are ‘centred’ around zero + Some will be below zero, some will be above + The intercept term in your model now represents the intercept at the average for the predictor + The predictors are still in their raw units (e.g. years/kgs) + (learn to do these techniques by hard coding – easy to do and the common function that is used, scale(), doesn’t always play very nicely with some other packages!)"
  },
  {
    "objectID": "PSYC234/Week2.html#demonstration-unbalanced-variables---transforming-binary-categorical-to-numerical-variables-and-centering-them",
    "href": "PSYC234/Week2.html#demonstration-unbalanced-variables---transforming-binary-categorical-to-numerical-variables-and-centering-them",
    "title": "2. Multiple regression including categorical predictors",
    "section": "Demonstration: Unbalanced variables - transforming binary categorical to numerical variables and centering them",
    "text": "Demonstration: Unbalanced variables - transforming binary categorical to numerical variables and centering them\nLook back at the visualisations of the dataset above. Female and male participants in the sex variable are quite evenly spread. Smoker status, however, is very unbalanced. Gelman (2007) recommends creating a new numeric variable in this case and then mean centring a binary variable in this case. If you use this method, do this across each binary categorical variable for consistency.\n\n# bar charts for categorical variables\np_sex &lt;- ggplot(d, aes(x = factor(sex), fill = factor(sex))) + geom_bar()\np_smok &lt;- ggplot(d, aes(x = factor(smoker), fill = factor(smoker))) + geom_bar()\np_reg &lt;- ggplot(d, aes(x = factor(region), fill = factor(region))) + geom_bar()\n\n# (so called) continuous variables\np_age &lt;- ggplot(d, aes(x = age)) + geom_density(fill = \"slateblue\")\np_age_hist &lt;- ggplot(d, aes(x = age)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi &lt;- ggplot(d, aes(bmi)) + geom_density(fill = \"red\")\np_child &lt;- ggplot(d, aes(children)) + geom_histogram()\ngrid.arrange(p_sex, p_smok, p_reg, p_age_hist, p_bmi, p_child)\n\n\n\n\n\nd$sex_n &lt;- ifelse(d$sex == \"female\", 1, 0) # if sex = female recode as 1 otherwise as 0 in a new numeric variable\n\nHave a look at what the summary of the new variable shows - should be quite symmetric:\n\nsummary(d$sex_n) # balanced variable - look at the mean value\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.4948  1.0000  1.0000 \n\n\nso if we transform it to be centred now, by subtracting the mean from each observation rather than leaving it to R:\n\nd$sex_c &lt;- signif(d$sex_n - mean(d$sex_n), 3) # round to 3 significant figures for readability\n\n\nsummary(d$sex_c) #\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.4950000 -0.4950000 -0.4950000 -0.0002317  0.5050000  0.5050000 \n\n\nhave a look at the first few values of the variable:\n\nhead(d$sex_c, 8) # female was 1 so is the positive numbers; male was zero so is the negative numbers here.\n\n[1]  0.505 -0.495 -0.495 -0.495 -0.495  0.505  0.505  0.505\n\n\nIf we do the same for the smoker variable - we saw in the bar chart that this was a very unbalanced variable, however here are the actual numbers:\n\ntable(factor(d$smoker))\n\n\n  no  yes \n1064  274 \n\n\n\nd$smoker_n &lt;- ifelse(d$smoker == \"yes\", 1, 0) # if smoker = yes recode as 1 otherwise as 0 in a new variable\n\nsummary(d$smoker_n) # value is not 0.5 which is would be if the variable was balanced in observations\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.0000  0.0000  0.0000  0.2048  0.0000  1.0000 \n\n\n\nd$smoker_c &lt;- signif(d$smoker_n - mean(d$smoker_n), 3) # round to 3 significant figures for readability\n\n\nsummary(d$smoker_c) # yes was coded as 1 so the positive values here are smokers and the negative values are non-smokers\n\n      Min.    1st Qu.     Median       Mean    3rd Qu.       Max. \n-0.2050000 -0.2050000 -0.2050000 -0.0002167 -0.2050000  0.7950000 \n\n\nhave a look at the first few values of the variable:\n\nhead(d$smoker_c, 8)\n\n[1]  0.795 -0.205 -0.205 -0.205 -0.205 -0.205 -0.205 -0.205\n\n\nLet’s refit the model with these variables:\n\nsummary(m6_cen &lt;- lm(charges ~ age + bmi + children + sex_c + smoker_c + regionsum, d))\n\n\nCall:\nlm(formula = charges ~ age + bmi + children + sex_c + smoker_c + \n    regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  -7702.9      963.6  -7.994 2.82e-15 ***\nage            256.9       11.9  21.587  &lt; 2e-16 ***\nbmi            339.2       28.6  11.860  &lt; 2e-16 ***\nchildren       475.5      137.8   3.451 0.000577 ***\nsex_c          131.3      332.9   0.394 0.693348    \nsmoker_c     23848.5      413.1  57.723  &lt; 2e-16 ***\nregionsum1     587.0      293.1   2.003 0.045411 *  \nregionsum2     234.1      292.9   0.799 0.424427    \nregionsum3    -448.0      291.2  -1.538 0.124219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,    Adjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16\n\n\nSo - we have changes for the categorical variables - they are now “centred” - either automatically by R or we have hand coded, transforming our categorical variables into numeric variables.\nFor consistency - it would make sense to centre the continuous variables also. While you are learning, it’s a good idea to do it longhand:\n\n# create new variables to keep the original variables intact:\n\nd &lt;- d %&gt;% \n  mutate(age_c = age - mean(age), # new variable = old variable minus the mean of the old variable\n         bmi_c = bmi - mean(bmi),\n         child_c = children - mean(children)\n         )\n\nYou can check: redraw the plots to check the axes - Note now that the axes are centred around zero for the continuous predictors. Note also how no information has changed within and between the variables - all relative relationships remain the same.\n\n# bar charts for categorical variables\np_sex_c &lt;- ggplot(d, aes(x = factor(sex_c), fill = factor(sex_c))) + geom_bar()\np_smok_c &lt;- ggplot(d, aes(x = factor(smoker_c), fill = factor(smoker_c))) + geom_bar()\np_regsum &lt;- ggplot(d, aes(x = factor(regionsum), fill = factor(regionsum))) + geom_bar()\n\n# frequency polygons for (so called) continuous variables\np_age_c &lt;- ggplot(d, aes(x = age_c)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi_c &lt;- ggplot(d, aes(bmi_c)) + geom_density(fill = \"red\")\np_child_c &lt;- ggplot(d, aes(child_c)) + geom_histogram()\ngrid.arrange(p_sex_c, p_smok_c, p_regsum, p_age_c, p_bmi_c, p_child_c)\n\n\n\n\nLets plug the centred continuous variables into the model:\n\nsummary(m6_cen_all &lt;- lm(charges ~ age_c + bmi_c + child_c + sex_c + smoker_c + regionsum, d))\n\n\nCall:\nlm(formula = charges ~ age_c + bmi_c + child_c + sex_c + smoker_c + \n    regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13289.1      165.9  80.079  &lt; 2e-16 ***\nage_c          256.9       11.9  21.587  &lt; 2e-16 ***\nbmi_c          339.2       28.6  11.860  &lt; 2e-16 ***\nchild_c        475.5      137.8   3.451 0.000577 ***\nsex_c          131.3      332.9   0.394 0.693348    \nsmoker_c     23848.5      413.1  57.723  &lt; 2e-16 ***\nregionsum1     587.0      293.1   2.003 0.045411 *  \nregionsum2     234.1      292.9   0.799 0.424427    \nregionsum3    -448.0      291.2  -1.538 0.124219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,    Adjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16\n\n\nYou’ll notice that the only change is on the intercept value - because it is now at the average for each predictor, rather than when each predictor is 0…and it makes more sense, right - the intercept is no longer a negative unit value. Although it would be nice to think that people could pay negative insurance charges - what does that mean? That everyone is in debt? That the health treatment costs less than 0?\nCreate some more variables to store the standardised variables, which we will create next.\nRemember to create a standardised variable, we first centre the variable and then divide it by its standard deviation\n\n# create new variables to store the standardised values\n# these are standardised by dividing by 1 x sd:\n\nd &lt;- d %&gt;% \n  mutate(age_z1 = (age - mean(age)) / sd(age), # new variable = centred variable divided by the standard deviation of the centred variable\n         bmi_z1 = (bmi - mean(bmi)) / sd(bmi),\n         child_z1 = (children - mean(children)) / sd(children)\n         )\n\n\n# bar charts for categorical variables\np_sex_c &lt;- ggplot(d, aes(x = factor(sex_c), fill = factor(sex_c))) + geom_bar()\np_smok_c &lt;- ggplot(d, aes(x = factor(smoker_c), fill = factor(smoker_c))) + geom_bar()\np_regsum &lt;- ggplot(d, aes(x = factor(regionsum), fill = factor(regionsum))) + geom_bar()\n\n# frequency polygons for (so called) continuous variables\np_age_z1 &lt;- ggplot(d, aes(x = age_z1)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi_z1 &lt;- ggplot(d, aes(bmi_z1)) + geom_density(fill = \"red\")\np_child_z1 &lt;- ggplot(d, aes(child_z1)) + geom_histogram()\ngrid.arrange(p_sex_c, p_smok_c, p_regsum, p_age_z1, p_bmi_z1, p_child_z1)\n\n\n\n\nOnly the axes have changed once more….the centre is still zero but the units are now in standard deviations.\nWhat happens when we use the standardised variables with the centred binary variables:\n\nsummary(m6_z &lt;- lm(charges ~ age_z1 + bmi_z1 + child_z1 + sex_c + smoker_c + regionsum, d))\n\n\nCall:\nlm(formula = charges ~ age_z1 + bmi_z1 + child_z1 + sex_c + smoker_c + \n    regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11304.9  -2848.1   -982.1   1393.9  29992.8 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)  13289.1      165.9  80.079  &lt; 2e-16 ***\nage_z1        3608.8      167.2  21.587  &lt; 2e-16 ***\nbmi_z1        2068.5      174.4  11.860  &lt; 2e-16 ***\nchild_z1       573.2      166.1   3.451 0.000577 ***\nsex_c          131.3      332.9   0.394 0.693348    \nsmoker_c     23848.5      413.2  57.723  &lt; 2e-16 ***\nregionsum1     587.0      293.1   2.003 0.045411 *  \nregionsum2     234.0      292.9   0.799 0.424427    \nregionsum3    -448.0      291.2  -1.538 0.124219    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6062 on 1329 degrees of freedom\nMultiple R-squared:  0.7509,    Adjusted R-squared:  0.7494 \nF-statistic: 500.8 on 8 and 1329 DF,  p-value: &lt; 2.2e-16\n\n\nNo change in the original correlations either:\nThis line of code charts correlations between columns 18 - 20 of the dataset d - i.e. the standardised varaibles we just made.\n\nchart.Correlation(d[, c(18:20)], histogram=TRUE, pch=19)"
  },
  {
    "objectID": "PSYC234/Week2.html#transformation-via-standardising",
    "href": "PSYC234/Week2.html#transformation-via-standardising",
    "title": "2. Multiple regression including categorical predictors",
    "section": "Transformation via Standardising",
    "text": "Transformation via Standardising\nTake your centred variable and divide each observation by one standard deviation of the variable. Interpretation is now a one standard deviation decrease / increase in the predictor is a (part of) standard deviation in the outcome variable. Because all predictors are now measured in standard deviation units, instead of their raw units, you can compare the size of their coefficients. You couldn’t do this before because they were in different units.\nDividing by 2 standard deviations allows you to directly compare the coefficients for continuous and binary predictors."
  },
  {
    "objectID": "PSYC234/Week2.html#demonstration-standardising-by-two-standard-deviations",
    "href": "PSYC234/Week2.html#demonstration-standardising-by-two-standard-deviations",
    "title": "2. Multiple regression including categorical predictors",
    "section": "Demonstration: Standardising by Two Standard Deviations",
    "text": "Demonstration: Standardising by Two Standard Deviations\n\n# create new variables to store the standardised values\n# these are standardised by dividing by 1 x sd:\n\nd &lt;- d %&gt;% \n  mutate(age_z2 = (age - mean(age)) / (2*sd(age)), # new variable = centred variable divided by the standard deviation of the centred variable\n         bmi_z2 = (bmi - mean(bmi)) / (2*sd(bmi)),\n         child_z2 = (children - mean(children)) / (2*sd(children))\n         )\n\n\n# bar charts for categorical variables\np_sex_c &lt;- ggplot(d, aes(x = factor(sex_c), fill = factor(sex_c))) + geom_bar()\np_smok_c &lt;- ggplot(d, aes(x = factor(smoker_c), fill = factor(smoker_c))) + geom_bar()\np_regsum &lt;- ggplot(d, aes(x = factor(regionsum), fill = factor(regionsum))) + geom_bar()\n\n# frequency polygons for (so called) continuous variables\np_age_z2 &lt;- ggplot(d, aes(x = age_z2)) + geom_histogram() # possibly a little more detail using a histogram?\np_bmi_z2 &lt;- ggplot(d, aes(bmi_z2)) + geom_density(fill = \"red\")\np_child_z2 &lt;- ggplot(d, aes(child_z2)) + geom_histogram()\ngrid.arrange(p_sex_c, p_smok_c, p_regsum, p_age_z2, p_bmi_z2, p_child_z2)\n\n\n\n\nOnly the axes have changed once more….the centre is still zero, the units are now in standard deviations. The correlations remain the same also. I leave it to the interested reader to cut and paste the code from above and visualise columns 21 - 23 to check this for themselves.\n\nOrdered variables\nWe’re not quite finished on our contrast journey…\nConsider the children variable - so far I have treated it as a continuous variable but it’s definitely ordered: look at the histogram above again. If we factorise the variable, we will better see the categories\n\nsummary(d$children) # summary as a continuous predictor\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.000   1.000   1.095   2.000   5.000 \n\n\n\nsummary(d$children_f &lt;- factor(d$children)) # wrap the factorising command within a summary command for speed\n\n  0   1   2   3   4   5 \n574 324 240 157  25  18 \n\n\nand we can be pretty confident that the step from 0 - 1 and 3 - 4 is the same size unit of an increase in one person so children is the kind of variable that can be treated as continuous - it has a true zero, but it also is discrete….groups representing counts of different numbers of children…so what happens if we treat this as an ordered variable and use the helmert variant of contrast coding:\n\nd$children_h &lt;- factor(d$children)\n(contrasts(d$children_h) &lt;- contr.helmert(6))\n\n  [,1] [,2] [,3] [,4] [,5]\n1   -1   -1   -1   -1   -1\n2    1   -1   -1   -1   -1\n3    0    2   -1   -1   -1\n4    0    0    3   -1   -1\n5    0    0    0    4   -1\n6    0    0    0    0    5\n\n# levels(d$children_h) # checking the order of levels in the children_h variable\n\nThis contrast coding tell us that the first regression coefficient (look at column [ ,1]) will be the comparison between the first two categories of the children variable - 0 children and one-child families. The second regression coefficient (look at column [ ,2]) for the variable will show a comparison between families with two children and families with either 0 or one child and so on.\n\nsummary(m6_zh &lt;- lm(charges ~ age_z1 + bmi_z1 + children_h + sex_c + smoker_c + regionsum, d))\n\n\nCall:\nlm(formula = charges ~ age_z1 + bmi_z1 + children_h + sex_c + \n    smoker_c + regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11689.4  -2902.6   -943.7   1492.2  30042.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13893.35     337.07  41.218  &lt; 2e-16 ***\nage_z1       3613.56     167.40  21.587  &lt; 2e-16 ***\nbmi_z1       2054.53     174.48  11.775  &lt; 2e-16 ***\nchildren_h1   195.49     210.68   0.928  0.35362    \nchildren_h2   480.10     148.35   3.236  0.00124 ** \nchildren_h3    72.19     130.47   0.553  0.58017    \nchildren_h4   439.92     245.53   1.792  0.07341 .  \nchildren_h5   -11.94     243.80  -0.049  0.96094    \nsex_c         128.16     332.83   0.385  0.70025    \nsmoker_c    23836.41     414.14  57.557  &lt; 2e-16 ***\nregionsum1    591.52     293.16   2.018  0.04382 *  \nregionsum2    211.47     293.68   0.720  0.47160    \nregionsum3   -441.62     291.54  -1.515  0.13006    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6059 on 1325 degrees of freedom\nMultiple R-squared:  0.7519,    Adjusted R-squared:  0.7497 \nF-statistic: 334.7 on 12 and 1325 DF,  p-value: &lt; 2.2e-16\n\n\nThis children predictor now has an independent coefficient for each level of the categorical variable. Previously, we had entered the children predictor as continuous, and we were getting an even rate of change, which registered as a significant impact on insurance charges.\nNow that we have transformed the variable - and respected the structure of the variable data (it was never continuous), out interpretation of the model needs to change somewhat. The rate of change in insurance chargest across the increase in children is not even (480.10 is not the same as 72.19). This is not a linear relationship. The only significantly different change in insurance charges is the change from 0 or 1 child to two child families. Can you think of a reason why a familiy of two children is predicted to pay significantly higher insurance charges than a family of one child?\nThere is a little more detail in this model than the previous ones - whether it is useful would depend upon the research question, or a better model than the previous ones, we could deduce using formal model comparison techniques - more of that later.\nLets check the residuals once more:\n\npar(mfrow = c(2, 2)) # display plots in a 2 x 2 panel\nplot(m6_zh)\n\n\n\n\nThat structure is still there!\n\n\nInterpretation of the model\nLets reprint the summary to save scrolling but also save the summary to an object so that we can call the values while interpreting and reporting the model\n\n(m6_zh_summary &lt;- summary(m6_zh))\n\n\nCall:\nlm(formula = charges ~ age_z1 + bmi_z1 + children_h + sex_c + \n    smoker_c + regionsum, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-11689.4  -2902.6   -943.7   1492.2  30042.7 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) 13893.35     337.07  41.218  &lt; 2e-16 ***\nage_z1       3613.56     167.40  21.587  &lt; 2e-16 ***\nbmi_z1       2054.53     174.48  11.775  &lt; 2e-16 ***\nchildren_h1   195.49     210.68   0.928  0.35362    \nchildren_h2   480.10     148.35   3.236  0.00124 ** \nchildren_h3    72.19     130.47   0.553  0.58017    \nchildren_h4   439.92     245.53   1.792  0.07341 .  \nchildren_h5   -11.94     243.80  -0.049  0.96094    \nsex_c         128.16     332.83   0.385  0.70025    \nsmoker_c    23836.41     414.14  57.557  &lt; 2e-16 ***\nregionsum1    591.52     293.16   2.018  0.04382 *  \nregionsum2    211.47     293.68   0.720  0.47160    \nregionsum3   -441.62     291.54  -1.515  0.13006    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 6059 on 1325 degrees of freedom\nMultiple R-squared:  0.7519,    Adjusted R-squared:  0.7497 \nF-statistic: 334.7 on 12 and 1325 DF,  p-value: &lt; 2.2e-16\n\n\n\n\nA possible interpretation and reporting:\nWe have modelled a dataset that has several variables that may be informative in the prediction of insurance charges. With no guiding research questions or hypotheses, we have looked at the structure of the variables and entered them all into a multiple regression model. This is a data-driven, exploratory analysis.\nContinuous variables have been standardised; gender and smoker status have been coerced to be numerical variables and mean centred. The six levels of the children variable have been given a helmert type contrast to provide detail of which numbers of children are influential.\nWithout engaging in any sensitivity analysis or outlier analysis, we report a model with a full set of predictors. The model is significant (\\(F{_(}{_12,_1325}{_)}\\) = 334.7, p &lt; 0.001). This set of predictors explains 75.19% of the variance in insurance charges.\nThe following details the significant predictors in this model. Each value represents a change in the predictor variable of one standard deviation and its impact upon the outcome variable while holding all other variables constant. Because we have standardised the continuous variables this means they are held constant at their average level. The intercept therefore represents non smoking male individuals, with no children, that live in the SW region of the US.\nAge shows a positive association with insurance charges, with an increase in approximately $3613.56 for a one standard deviation unit change on the age scale (age: t = 21.59, p &lt; .001). A person’s body mass index (BMI) also shows a positive relationship and increases insurance charges by approximately $2054 with a one standard deviation change in BMI score (t = 11.77, p &lt; .001). The number of dependents included in insurance cover was only significant on insurance charges at the comparison between 0 and 1 child to the two child level of coverage. Having two children increased insurance charges by approximately $480 (t = 3.24, p &lt; .005), compared to the mean levels for 0 and 1 child. Being a woman is positively associated with an increase in insurance charges, raising insurance costs by approximately $128, however this increase is non-significant (t = 0.39, p = .700). Being a smoker is significantly and positively associated with an increase in insurance charges, raising insurance costs by approximately $23,836 (t = 57.56, p &lt; .001). While living in the northeast region of the country also shows a positive association with insurance charges compared to living in the southwest region (reference level), showing an increase of around $591 (t = 2.02, p = 0.044).\nWe should be cautious with these interpretations as the residuals show that there is unexplained variability and there are a number of observations that are indicated as having high leverage or high influence so further terms and sensitivity analyses are warranted for a fuller understanding.\n\nPlotting predictions\nPlotting an individual effect (or lack of it) is very simple if we are not too worried about being pretty. Plus - because we haven’t changed any of the essential information in any of the coding / centring schemes, you can choose how to plot them, based upon the model.\n\nplot(predictorEffect(\"age_z1\", m6_zh))\n\n\n\nplot(predictorEffect(\"bmi_z1\", m6_zh))\n\n\n\nplot(predictorEffect(\"children_h\", m6_zh))\n\n\n\nplot(predictorEffect(\"sex_c\", m6_zh))\n\n\n\nplot(predictorEffect(\"smoker_c\", m6_zh))\n\n\n\nplot(predictorEffect(\"regionsum\", m6_zh))\n\n\n\n\n\n\nChecking that assumptions of linear regression are not violated\nWe can collect some diagnostic measures to help with assumption checking using the augment() function from the broom package:\n\n(m6zh_metrics &lt;- augment(m6_zh))\n\n# A tibble: 1,338 × 13\n   charges age_z1 bmi_z1 children_h  sex_c smoker_c regionsum .fitted  .resid\n     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;    &lt;dbl&gt; &lt;fct&gt;       &lt;dbl&gt;   &lt;dbl&gt;\n 1  16885. -1.44  -0.453 0           0.505    0.795 SW         25243. -8358. \n 2   1726. -1.51   0.509 1          -0.495   -0.205 SE          3309. -1584. \n 3   4449. -0.798  0.383 3          -0.495   -0.205 SE          6195. -1746. \n 4  21984. -0.442 -1.31  0          -0.495   -0.205 NW          3702. 18283. \n 5   3867. -0.513 -0.292 0          -0.495   -0.205 NW          5525. -1658. \n 6   3757. -0.584 -0.807 0           0.505   -0.205 SE          3685.    71.9\n 7   8241.  0.483  0.455 1           0.505   -0.205 SE         10528. -2287. \n 8   7282. -0.157 -0.479 3           0.505   -0.205 NW          7519.  -238. \n 9   6406. -0.157 -0.137 2          -0.495   -0.205 NE          9147. -2740. \n10  28923.  1.48  -0.791 0           0.505   -0.205 NW         11830. 17093. \n# ℹ 1,328 more rows\n# ℹ 4 more variables: .hat &lt;dbl&gt;, .sigma &lt;dbl&gt;, .cooksd &lt;dbl&gt;, .std.resid &lt;dbl&gt;\n\n\nColumns 1 - 7 are our variables. fitted and residuals are the predicted values of charges and the error values. The final four columns we use below with some explanation.\nEvery regression model is built upon the following assumptions:\n\nThe relationship between \\(X\\) and \\(Y\\) is assumed to be linear (additive)\nThe residual errors are normally distributed\nThe residuals have constant variance (homoscedasticity)\nThe residuals are not correlated (assumption of independence)\n\n\npar(mfrow = c(2, 2)) # display plots in a 2 x 2 panel\nplot(m6_zh) # plot diagnostic plots for m6_zh\n\n\n\n\nResiduals vs Fitted: “fitted” here means the predicted values. We want the pink line to align pretty closely with the horizontal dashed line. Comparing this plot with that from the simple regression, this plot looks better. Take note of observation 13 - that was also labelled in the simple regression plots.\nNormal Q-Q: If the residual points (open circles) follow the dashed line, you can assume the residuals are normally distributed\nScale-Location: This is checking for constant variance in the residuals - not much here. A good indication would be a horizontal pink line with equally spread points. Our graph is not good.\nResiduals vs Leverage - are there any points that are having a large influence on the regression results. They will be numbered and you can then inspect them in your data file. Observations that show standardised residuals (see the table above) above 3 would be problematic. As would observations of a hat value above \\(2(p+1)/n\\) where \\(p\\) = is the number of predictors (but see below) and \\(n\\) = is the number of observations\nA different way to observe points with high leverage and high influence:\nIn the previous scripts, we used a formula for calculating hat values and Cook’s Distance values. Now we are modelling both continuous and categorical predictors that can have reference levels or more than two levels etc etc - each of which have their own model coefficient, it gets a little confusing remembering what ‘p’ stands for. So an easier way is to graph your outlier observations:\nGraphing Model Hat Values and Observations with High Leverage: This is a larger dataset, so lets get R doing the sorting for us to check if there are any hat values larger than the model threshold:\n\ncalculating hat value by hand\nusing the hat value and filter out any observations from the augmented dataset\n\n\np &lt;- length(coefficients(m6_zh)) # number of parameters estimated by the model\nN &lt;- nrow(d) # number of observations\n\n\n# 1) calulate the hat value\n(m6zh_hat &lt;- (2*(p+1))/N)  # model hat value\n\n[1] 0.02092676\n\n# 2) filter out observations that are above the hat value in the augmented dataset\nm6zh_hatvalues &lt;- m6zh_metrics %&gt;% \n  filter(.hat &gt; 0.02092676) \n\n\nPlot them: hat.plot function taken from Kabacoff, (2022), R in Action.\n\n\nhat.plot &lt;- function(fit) {\n              p &lt;- length(coefficients(fit))\n              n &lt;- length(fitted(fit))\n              plot(hatvalues(fit), main=\"Index Plot of Hat Values\")\n              abline(h=c(2,3)*p/n, col=\"red\", lty=2)\n              identify(1:n, hatvalues(fit), names(hatvalues(fit)))\n            }\n\nhat.plot(m6_zh)\n\n\n\n\ninteger(0)\n\n\n43 of the datapoints above have hat values larger than the model hat threshold value.\nChecking for observations that are influential follows a similar pattern: observations that exceed the Cook’s distance value = \\(4/(n-p-1)\\) are likely to have high influence and the regression results may change if you exclude them. In the presence of such observations that exceed Cook’s distance, unless you know the observation are errors, you probably need to estimate the model without the observations and report both sets of results.\n\n(m6zh_Cooks &lt;- 4/(N-p-1)) # model Cook's distance value\n\n[1] 0.003021148\n\n\n\nm6zh_Cooksvalues &lt;- m6zh_metrics %&gt;% \n  filter(.cooksd &gt; 0.003021148) \n\n# make sure your dataset is in nrow() function\n# and your model is before $coefficients and in the plot line\n# here it is m6_zh\n# when you are ready to draw the plot\n# select all three lines at once and press control and enter\n# or rerun the chunk by pressing the green arrow in the top right hand corner.\ncutoff &lt;- 4/(nrow(d)-length(m6_zh$coefficients)-1)\nplot(m6_zh, which=4, cook.levels=cutoff)\nabline(h=cutoff, lty=2, col=\"red\")\n\n\n\n\nThere are 77 observations that appear to be influential within the dataset…So we may need to run a reduced dataset and compare the outputs\nBut the model building is not complete. It is likely that there are interactions that could be put into the model and age, as a variable, may be nonlinear - how to deal with both of these is for the next lecture.\nThere are lots of things to do as we run a multiple regression model - both before, while and after building the model, which will be the focus of the future lectures. We need to look at:\n\na model with some significant predictors! - done\ncorrelation matrices - done\ncentering predictors - done\nstandardising predictors - done\nmodels with categorical predictors - done & ongoing\n\ninterpreting models with categorical predictors - done & ongoing\n\nmodels with interaction terms\nthe properties of multicollinearity\nchoosing between different models\nreporting models - ongoing"
  },
  {
    "objectID": "PSYC234/Week2.html#more-than-two-levels-and-ordered-variables",
    "href": "PSYC234/Week2.html#more-than-two-levels-and-ordered-variables",
    "title": "2. Multiple regression including categorical predictors",
    "section": "More than two levels and ordered variables?",
    "text": "More than two levels and ordered variables?\nWhat about categorical variables with more than two levels? + Dummy / treatment coding - each level’s coefficient is the difference between the reference level and that level + Sum coding – each level’s coefficient is the difference of the level from the intercept + In variables with more than one level, To find the value of -1 , you have to calculate the sum of the intercept and all the other levels of the variable.\nWhat about ordered categorical variables? + Ordered outcome variables use ordinal regression model + Ordered predictor variables use “helmert” coding + Each level of the coefficient is the difference between those level or levels below them + To get the value of the nth level in an ordered variable, you add all the other coefficients that come before that level to the intercept."
  },
  {
    "objectID": "PSYC234/Week7.html",
    "href": "PSYC234/Week7.html",
    "title": "7. Kruskal-Wallis test and Friedman’s ANOVA",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week7.html#research-question-1",
    "href": "PSYC234/Week7.html#research-question-1",
    "title": "7. Kruskal-Wallis test and Friedman’s ANOVA",
    "section": "Research Question 1",
    "text": "Research Question 1\nYou are a psychology lecturer. You hear that the library is offering three statistics courses. You are interested in whether students who attend the courses perform significantly differently from each other.\nYou recruit 18 people and assign each one to a course. After the courses are finished, you ask them to write an R script. You time how long it takes students to complete the task. You are interested in whether there is a significant effect of course on the time taken to complete the task."
  },
  {
    "objectID": "PSYC234/Week7.html#research-question-2",
    "href": "PSYC234/Week7.html#research-question-2",
    "title": "7. Kruskal-Wallis test and Friedman’s ANOVA",
    "section": "Research Question 2",
    "text": "Research Question 2\nYou are a developmental psychologist. You are interested in whether working memory develops between 15 and 17 years of age.\nYou recruit a sample of adolescents and test them on a working memory task when they are 15 years of age, 16 years of age, and 17 years of age.\nYou then examine whether there is a significant effect of age on working memory score."
  },
  {
    "objectID": "PSYC234/Week7.html#a-template-for-running-the-kruskal-wallis-test-and-friedmans-anova",
    "href": "PSYC234/Week7.html#a-template-for-running-the-kruskal-wallis-test-and-friedmans-anova",
    "title": "7. Kruskal-Wallis test and Friedman’s ANOVA",
    "section": "A template for running the Kruskal-Wallis test and Friedman’s ANOVA",
    "text": "A template for running the Kruskal-Wallis test and Friedman’s ANOVA\n\nLoad packages and data\nNormality checks\nExplore your data (e.g. descriptive statistics, a plot)\nConduct the statistical test\nConduct post-hoc tests\nInterpret your data"
  },
  {
    "objectID": "PSYC234/Week4.html",
    "href": "PSYC234/Week4.html",
    "title": "4. Mediation",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC234/Week4.html#differences-between-means",
    "href": "PSYC234/Week4.html#differences-between-means",
    "title": "4. Mediation",
    "section": "Differences between means",
    "text": "Differences between means\nT-tests helps answer the question: + ‘Is there a difference between two groups in performance on X?’\nANOVA helps answer the question: + ‘Is there a difference between two or more groups / factors in performance on X?’\nWith a 3rd variable we can see if this affects performace at different levels – we can introduce an interaction term…"
  },
  {
    "objectID": "PSYC234/Week4.html#association-correlation---regression",
    "href": "PSYC234/Week4.html#association-correlation---regression",
    "title": "4. Mediation",
    "section": "Association: Correlation -> Regression",
    "text": "Association: Correlation -&gt; Regression\nMeasures of association help answer the question ‘What is the relationship between two variables?’\nCorrelation looks at pairs of variables\n\nlibrary(DiagrammeR)\n\ngrViz(diagram = \"digraph flowchart {\n  rankdir=LR;\n  node [fontname = arial, shape = oval]\n  tab1 [label = '@@1']\n  tab2 [label = '@@2']\n\n  tab1 -&gt; tab2 [dir='both'];\n}\n  \n  [1]: 'X'\n  [2]: 'Y'\n  \") \n\n\n\n\n\nRegressions chooses one as the outcome variable & one as the predictor variable + Simple regression = 1 outcome and 1 predictor\n\ngrViz(diagram = \"digraph flowchart {\n  rankdir=LR;\n  node [fontname = arial, shape = oval]\n  tab1 [label = '@@1']\n  tab2 [label = '@@2']\n\n  tab1 -&gt; tab2;\n}\n  \n  [1]: 'X'\n  [2]: 'Y'\n  \") \n\n\n\n\n\n\nMultiple regression = 1 outcome and 1+ predictor\n\nInteractions also possible\n\n\n\ngrViz(diagram = \"digraph flowchart {\n  rankdir=LR;\n  node [fontname = arial, shape = oval]\n  tab1 [label = '@@1']\n  tab2 [label = '@@2']\n  tab3 [label = '@@3']\n\n  tab1 -&gt; tab3;\n  tab2 -&gt; tab3;\n\n  {rank=same; tab1, tab2}\n\n}\n  \n  [1]: 'X1'\n  [2]: 'X2'\n  [3]: 'Y'\n  \")"
  },
  {
    "objectID": "PSYC234/Week4.html#mediation-a-causal-model",
    "href": "PSYC234/Week4.html#mediation-a-causal-model",
    "title": "4. Mediation",
    "section": "Mediation: a causal model",
    "text": "Mediation: a causal model\nMediation helps answer the question: + ‘how does a predictor variable (X) influence / effect the outcome variable (Y)?’\nWe assume a third variable is involved + The third variable is called the mediator (M) + It is situated between the predictor (X) and outcome variable (Y)\n\ngrViz(diagram = \"digraph {\n  rankdir=LR;\n  node [fontname = arial, shape = circle]\n  ranksep = .5;\n\n  tab1 [label = 'X']\n  tab2 [label = 'M']\n  tab3 [label = 'Y']\n\n  tab1 -&gt; tab3;\n  tab1 -&gt; tab2 -&gt; tab3;\n\n\n}\n  \")"
  },
  {
    "objectID": "PSYC234/Week4.html#mediation-parts-of-the-model",
    "href": "PSYC234/Week4.html#mediation-parts-of-the-model",
    "title": "4. Mediation",
    "section": "Mediation: parts of the model",
    "text": "Mediation: parts of the model\nUnmediated relationship + path of total effect = c\n\ngrViz(diagram = \"digraph flowchart {\n  rankdir=LR;\n  node [fontname = arial, shape = oval]\n  tab1 [label = '@@1']\n  tab2 [label = '@@2']\n\n  tab1 -&gt; tab2 [label = 'c'];\n}\n  \n  [1]: 'X'\n  [2]: 'Y'\n  \") \n\n\n\n\n\nMediated relationship + mediator variable (M) + path of indirect effect = ab + a = X predicts M + b = M predicts Y + path of direct effect = c’ + ab + c’ = c = total effect of X on Y + either partial or full mediation\n\ngrViz(diagram = \"digraph {\n  rankdir=LR;\n  node [fontname = arial, shape = circle]\n  ranksep = .5;\n\n  tab1 [label = 'X']\n  tab2 [label = 'M']\n  tab3 [label = 'Y']\n\n  tab1 -&gt; tab3 [label = 'c`'];\n  tab1 -&gt; tab2 [label = 'a'];\n  tab2-&gt; tab3 [label = 'b'];\n\n\n}\n  \")"
  },
  {
    "objectID": "PSYC234/Week4.html#mediation-conditions",
    "href": "PSYC234/Week4.html#mediation-conditions",
    "title": "4. Mediation",
    "section": "Mediation: conditions",
    "text": "Mediation: conditions\n\nX need not be a significant predictor of Y\nM must not be a primary predictor variable\nM must not be any of the study conditions\nM must be dependent upon X\nM must reduce or eradicate the impact of X on Y"
  },
  {
    "objectID": "PSYC234/Week4.html#mediation-different-types-partial-and-full",
    "href": "PSYC234/Week4.html#mediation-different-types-partial-and-full",
    "title": "4. Mediation",
    "section": "Mediation: Different types: partial and full",
    "text": "Mediation: Different types: partial and full\nWhen path c’ is reduced but non-zero Mediation is said to be partial\n\ngrViz(diagram = \"digraph {\n  rankdir=LR;\n  node [fontname = arial, shape = circle]\n  ranksep = .5;\n\n  tab1 [label = 'X']\n  tab2 [label = 'M']\n  tab3 [label = 'Y']\n\n  tab1 -&gt; tab3 [label = 'c` &gt;0'];\n  tab1 -&gt; tab2 [label = '&gt;0 a'];\n  tab2-&gt; tab3 [label = 'b &gt;0'];\n\n\n}\n  \") \n\n\n\n\n\nWhen path c’ is at 0 Mediation is said to be complete or full\n\ngrViz(diagram = \"digraph {\n  rankdir=LR;\n  node [fontname = arial, shape = circle]\n  ranksep = .5;\n\n  tab1 [label = 'X']\n  tab2 [label = 'M']\n  tab3 [label = 'Y']\n\n  tab1 -&gt; tab3 [label = 'c` =0'];\n  tab1 -&gt; tab2 [label = '&gt;0 a'];\n  tab2-&gt; tab3 [label = 'b &gt;0'];\n\n\n}\n  \") \n\n\n\n\n\nBut be mindful of power – bootstrap method offers strongest solution here."
  },
  {
    "objectID": "PSYC234/Week4.html#additional-assumptions-to-the-linear-model-assumptions",
    "href": "PSYC234/Week4.html#additional-assumptions-to-the-linear-model-assumptions",
    "title": "4. Mediation",
    "section": "Additional assumptions to the linear model assumptions",
    "text": "Additional assumptions to the linear model assumptions\nA mediated model follows all the assumptions of linear regression\nAs an explanatory process, a predictor (X) can be said to be ‘causally’ related to the outcome (Y) when: + X is associated with Y + X precedes changes in Y + No other unmeasured variables are related to X and also affect Y\nX should / could precede M in time\nM should significantly predict Y but Y could also significantly predict M + M and Y could be correlated if they are both causally related to X. + Swapping the order of variables can check this High power +Study design can help this: from weakest to strongest for assumptions +Cross-sectional design (v. popular in student projects – beware…) +Panel designs that allow for staggered measurement in waves +Experimental designs with random assignment and manipulated variables"
  },
  {
    "objectID": "PSYC234/Week4.html#method-4-step-approach-baron-kenny-1986",
    "href": "PSYC234/Week4.html#method-4-step-approach-baron-kenny-1986",
    "title": "4. Mediation",
    "section": "Method: 4 step approach (Baron & Kenny, 1986)",
    "text": "Method: 4 step approach (Baron & Kenny, 1986)\n\nStep 1:\nTest path of total effect = Test the significance of slope c = Linear regression of X on Y 𝑌=𝑏_(0 ) + 𝑏_1 𝑋 = a simple, straightforward simple regression\n\n\nStep 2 & 3:\nTest path of indirect effect a and b = Test the significance of slope a and slope b in two independent models = Linear regression of X on M M =𝑏_(0 )+ 𝑏_1 𝑋 = Linear regression of M on Y while controlling for X = Y =𝑏_(0 )+𝑏_1 𝑋+𝑏_2 𝑀\n\n\nStep 4:\nTest if c’ &lt; c = \\(Y=b_0= b_1X + b_2M\\) (step 3 and 4 are in the same equation / model)\nIf c’ is significant = partial mediation If c’ is non- significant = full mediation"
  },
  {
    "objectID": "PSYC234/Week4.html#method-bootstrap-test-preacher-hayes-2004-2008",
    "href": "PSYC234/Week4.html#method-bootstrap-test-preacher-hayes-2004-2008",
    "title": "4. Mediation",
    "section": "Method: Bootstrap test\u000b(Preacher & Hayes, 2004, 2008)",
    "text": "Method: Bootstrap test\u000b(Preacher & Hayes, 2004, 2008)\n\nAutomated process\nResampling method for the indirect pathway using the model data with replacement\nIndirect pathway (ab) estimated for each set of sampled data\nAverage = indirect effect estimate\nGenerates confidence intervals also\nmediation package (Tingley et al., 2013) in R"
  },
  {
    "objectID": "PSYC234/Week4.html#reporting-results",
    "href": "PSYC234/Week4.html#reporting-results",
    "title": "4. Mediation",
    "section": "Reporting Results",
    "text": "Reporting Results\nReport the indirect effect and its confidence intervals + This is generally the effect with the most power + A nonsignificant test for c’ may occur due to low power + i.e. give a Type II Error + Be careful of claiming full / complete mediation given this information Report each pathway with either its significance value or confidence interval + Pathways a, b, c’ and c Discuss how the additional assumptions of mediation analysis are met Muller et al. (2008) give further details."
  },
  {
    "objectID": "PSYC234/Week4.html#interpreting-results",
    "href": "PSYC234/Week4.html#interpreting-results",
    "title": "4. Mediation",
    "section": "Interpreting Results",
    "text": "Interpreting Results\n\nBenefits of a direct effect in the context of a significant indirect effect (partial mediation) – it informs theory development\nSize of the indirect effect indicates the strength of mediation\nZhao et al. (2010) gave descriptive labels to mediation as a function of the directions of effect for the direct and indirect pathways:\n\nComplementary – effects for both pathways are in the same direction\nCompetitive – effects for both pathways are in opposite directions"
  },
  {
    "objectID": "PSYC234/Week4.html#extensions",
    "href": "PSYC234/Week4.html#extensions",
    "title": "4. Mediation",
    "section": "Extensions",
    "text": "Extensions\nHayes (2018) talks about: + Moderated mediation + The mediator depends on a fourth variable (!eek) and it could be + partial moderated mediation + conditional moderated mediation + moderated moderated mediation\nOut of scope for this module, but have a think when you are designing your studies!"
  },
  {
    "objectID": "PSYC234/Week4.html#demonstration",
    "href": "PSYC234/Week4.html#demonstration",
    "title": "4. Mediation",
    "section": "Demonstration",
    "text": "Demonstration\n\nMediation analysis\nIn this demonstration, we will model a mediation analysis. The total effect of an unmediated relationship is below in pathway c.\n\ndata &lt;- c(0, \"a\", 0,\n          0, 0, 0, \n          \"b\", \"c\", 0)\nM&lt;- matrix (nrow=3, ncol=3, byrow = TRUE, data=data)\nplot&lt;- plotmat (M, pos=c(1,2), \n                name= c( \"M\",\n                         \"Supervision\", \n                         \"Dissertation \\nPerformance\"), \n                box.type = \"rect\", \n                box.size = 0.12, \n                box.prop=0.5,  \n                curve=0)\n\n\n\n\nWe are going to test for the mediator variable of self esteem. This needs the prime mark added to the direct pathway “c” text!\n\ndata &lt;- c(0, \"a\", 0,\n          0, 0, 0, \n          \"b\", \"c\", 0)\nM&lt;- matrix (nrow=3, ncol=3, byrow = TRUE, data=data)\nplot&lt;- plotmat (M, pos=c(1,2), \n                name= c( \"Self-esteem\",\n                         \"Supervision\", \n                         \"Dissertation \\nPerformance\"), \n                box.type = \"rect\", \n                box.size = 0.12, \n                box.prop=0.5,  \n                curve=0)\n\n\n\n\n\n\nRead in the data\nFirst download the data file from here and upload to the R server.\n\nd_full &lt;- read_sav(\"mediation exercise 2 data.sav\")\n\nhead(d_full)\n\n\n\n# A tibble: 6 × 3\n  supervision self_esteem dissertation_performance\n        &lt;dbl&gt;       &lt;dbl&gt;                    &lt;dbl&gt;\n1        2.67        3.14                     3   \n2        2.67        3.57                     3.56\n3        3.33        3                        3.41\n4        3.33        2.57                     1   \n5        2.67        4.71                     2.26\n6        2.67        3.29                     2.41\n\n\nEarly in the construction of the script, I noticed that there were different levels of missingness across the models - this means that coefficients are estimated on different datasets, so we are introducing a potential source of systematic error if we do not correct for this.\nChecking for missingness:\n\nsummary(d_full)\n\n  supervision     self_esteem    dissertation_performance\n Min.   :1.667   Min.   :1.571   Min.   :1.000           \n 1st Qu.:3.333   1st Qu.:3.286   1st Qu.:2.380           \n Median :4.000   Median :3.857   Median :3.037           \n Mean   :3.786   Mean   :3.864   Mean   :3.028           \n 3rd Qu.:4.333   3rd Qu.:4.500   3rd Qu.:3.593           \n Max.   :5.000   Max.   :5.714   Max.   :5.815           \n                 NA's   :2       NA's   :3               \n\n\nFor the purposes of this analysis, I will remove the observations (rows) with NA values. This is not the best way of working with missingness, but for the purposes of the demonstration it is ok.\n\nd &lt;- na.omit(d_full) # 4 observations removed\nsummary(d) # no NA values listed\n\n  supervision     self_esteem    dissertation_performance\n Min.   :1.667   Min.   :1.571   Min.   :1.000           \n 1st Qu.:3.333   1st Qu.:3.286   1st Qu.:2.370           \n Median :4.000   Median :3.857   Median :3.074           \n Mean   :3.791   Mean   :3.853   Mean   :3.030           \n 3rd Qu.:4.333   3rd Qu.:4.429   3rd Qu.:3.593           \n Max.   :5.000   Max.   :5.714   Max.   :5.815           \n\n\nI am going to copy and rename the variables to save on typing\nX = supervision Y = dissertation_performance M = self_esteem\n\nd &lt;- d %&gt;% \n  mutate(X = supervision,\n         Y = dissertation_performance,\n         M = self_esteem)\n\n\n\nLonghand - Steps of Baron & Kenny (1986)\nFour independent linear regression models\n\nThe effect of X on Y (total effect = pathway c)\nThe effect of X on M (indirect effect pathway a)\nThe effect of M on Y (indirect effect pathway b) while controlling for X\nThe effect of X and M on Y (for direct effect estimation = pathway c’)\n\nWhen running your models, you need to assign them to objects in the environment to then be able to use them in a call to the mediation package.\n\nStep 1: Test the total effect - pathway c\n\\[\nY = b_0 + b_1 * X + e\n\\]\nvia a simple regression model:\n\n(fit_total &lt;-  summary(lm(Y ~ X, d)))\n\n\nCall:\nlm(formula = Y ~ X, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.83011 -0.56623 -0.04308  0.51478  2.25553 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   1.3718     0.4679   2.931 0.004470 ** \nX             0.4375     0.1209   3.620 0.000532 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.8361 on 75 degrees of freedom\nMultiple R-squared:  0.1488,    Adjusted R-squared:  0.1374 \nF-statistic: 13.11 on 1 and 75 DF,  p-value: 0.0005321\n\n\nThe total effect of our predictor on our outcome is significant. In other words, supervision on dissertation performance is significant (p &lt; .001). A change in one unit of supervision is associated with an increase in dissertation performance of 0.44.\nThere is an effect that can be tested for mediation.\n\n\nStep 2: Test the a pathway of the indirect effect\n\\[\nM = b_0 + b_1 * X + e\n\\] a second simple regression model, using X as a predictor but this time, M (self esteem here) is our outcome variable:\n\n(fit_indirecta &lt;- summary(lm(M ~ X, d)))\n\n\nCall:\nlm(formula = M ~ X, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-2.34553 -0.48838  0.02337  0.61288  1.49353 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept)   2.7018     0.4406   6.132 3.74e-08 ***\nX             0.3038     0.1138   2.670   0.0093 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.7872 on 75 degrees of freedom\nMultiple R-squared:  0.08679,   Adjusted R-squared:  0.07461 \nF-statistic: 7.128 on 1 and 75 DF,  p-value: 0.0093\n\n\nThe indirect effect pathway a (X on M) is also significant (p = .009). A change in one unit of supervision is associated with an increase in self-esteem of 0.30.\nSo now we know that X and M share some variance - they are correlated. We have met one of the assumptions that we need to be able to perform a mediation analysis.\n\n\nStep 3 & 4: Test the b pathway of the indirect effect & the direct effect (pathway c’)\n\\[\nY = b_0 + b_1 * X + b_2 * M + e\n\\] A multiple regression model, with Y (dissertation performance) as our outcome variable, and X (supervision) and M (self-esteem) as predictors. Remember that this model is controlling for the effect of X on Y, because interpreting one predictor in a multiple regression model always assumes that the effect of the other predictors are already taken care of, or controlled for.\n\n(fit_indirectb &lt;- summary(lm(Y ~ X + M, d)))\n\n\nCall:\nlm(formula = Y ~ X + M, data = d)\n\nResiduals:\n     Min       1Q   Median       3Q      Max \n-1.39749 -0.48798  0.02245  0.44603  1.29565 \n\nCoefficients:\n            Estimate Std. Error t value Pr(&gt;|t|)    \n(Intercept) -0.46168    0.44402  -1.040   0.3018    \nX            0.23135    0.09794   2.362   0.0208 *  \nM            0.67861    0.09497   7.145 5.26e-10 ***\n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nResidual standard error: 0.6475 on 74 degrees of freedom\nMultiple R-squared:  0.4963,    Adjusted R-squared:  0.4827 \nF-statistic: 36.46 on 2 and 74 DF,  p-value: 9.564e-12\n\n\nThe indirect effect pathway b (M on Y), while controlling for X is also significant (p &lt; .001). A change in one unit of self-esteem is associated with an increase in dissertation-performance of 0.68 of a unit. The predictor (X) relationship with the outcome (Y and pathway c’ - the direct effect) remains significant also (p &lt; .02 , but reduced relative to the step 1 model coefficient (step 1 \\(b\\) = 0.44, step 4 \\(b\\) = 0.23).\nSince the direct pathway c’ is significant, we can say that we have a partially mediated effect of self-esteem on the relationship between supervision and dissertation performance. Supervision predicts self esteem and dissertation performance, while self esteem also predicts dissertation performance.\nIf instead the X coefficient in the model above had been &gt; .05 i.e. no significant, we could have claimed a full or complete mediation of self esteem on the relationship between supervision and dissertation performance\n\n\n\nUsing the power of R and the mediation package\nThe mediation package is called by the library() function loaded at the top of the document.\n\nIt takes the models for pathways a and b (fit_indirecta and fitindirectb here),\nIt needs us to tell it the name of the predictor or treatment variable and the name of the mediator variable as labelled in the models\nand we set the boot argument to T for TRUE, to be able to generate confidence intervals on our co-efficients.\n\n\nresults &lt;- mediate(fit_indirecta, \n                   fit_indirectb, \n                   treat = 'X', \n                   mediator = 'M', \n                   boot = T, \n                   dropobs = TRUE)\n\nRunning nonparametric bootstrap\n\n\n\nsummary(results)\n\n\nCausal Mediation Analysis \n\nNonparametric Bootstrap Confidence Intervals with the Percentile Method\n\n               Estimate 95% CI Lower 95% CI Upper p-value    \nACME             0.2062       0.0508         0.38   0.010 ** \nADE              0.2313       0.0428         0.40   0.012 *  \nTotal Effect     0.4375       0.2062         0.63  &lt;2e-16 ***\nProp. Mediated   0.4712       0.1630         0.87   0.010 ** \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\nSample Size Used: 77 \n\n\nSimulations: 1000 \n\n\n\nACME stands for average causal mediation effects and is the product of pathway a and pathway b from fit indirecta (X = 0.3037992) and fit_indirectb (M = 0.6786069).\nADE stands for average direct effects or pathway c’. This is the X coefficient in our fit_indirectb\nTotal Effect does what it says on the tin. It is the sum of the direct and indirect effect, ACME + ADE, and also calculated as X in model fit_total.\nProp. Mediated is the proportion of the effect of X on Y that goes through M. We divide ACME (or ab) by the total effect (c).\n\n\nplot(results)\n\n\n\n\n\n\nReporting the mediation analysis\nand to use our diagram from the top of the document:\nWe are going to test for the mediator variable of self esteem. (This needs the prime mark added to the direct pathway “c” text!)\n\ndata &lt;- c(0, 0.30, 0,\n          0, 0, 0, \n          0.68, 0.23, 0)\nM&lt;- matrix (nrow=3, ncol=3, byrow = TRUE, data=data)\nplot&lt;- plotmat (M, pos=c(1,2), \n                name= c( \"Self-esteem\",\n                         \"Supervision\", \n                         \"Dissertation \\nPerformance\"), \n                box.type = \"rect\", \n                box.size = 0.12, \n                box.prop=0.5,  \n                curve=0)\n\n\n\n\n(Remember that these data are not standardised so we cannot compare between them for strength of relationships!)\nThe effect of supervision on dissertation performance was partially mediated via self-esteem. The effect of supervision on dissertation performance and the effect of self-esteem on dissertation performance were independently significant predictors. The indirect effect equals (.3)*(0.68) = .0.21. We tested the significance of this indirect effect using bootstrapping procedures. We computed the average indirect effect over 1,000 bootstrapped samples with 95% confidence intervals (bootstrapped indirect effect = 0.21 95% CI [0.06, 0.38]). Since the confidence intervals do not cross zero, we infer statistical significance."
  },
  {
    "objectID": "PSYC234/Week4.html#descriptive-statistics-describe-the-variables",
    "href": "PSYC234/Week4.html#descriptive-statistics-describe-the-variables",
    "title": "4. Mediation",
    "section": "Descriptive Statistics: describe the variables:",
    "text": "Descriptive Statistics: describe the variables:\n\nPrompts: continuous / categorical? Levels? Which one is the reference level if categorical?\nReport the outcome variable (Y)\nReport the predictor variables (Xs)\nSummary stats for continuous variables = calculate and report the mean and sd values for each variable.\n\nMediation analysis – 4 step approach\nStep 1: Total effect: Run the code and write a short paragraph of results\nStep 2: Indirect effect path a: Run the code and write a short paragraph of results.\nStep 3 & 4: Indirect effect path b and direct effect path: Run the code and write a short paragraph of results."
  },
  {
    "objectID": "PSYC234/Week4.html#questions",
    "href": "PSYC234/Week4.html#questions",
    "title": "4. Mediation",
    "section": "Questions",
    "text": "Questions\n\nIs there a total effect?\nIs there an indirect path effect for X predicting the mediator M?\nIs there an effect for indirect path of mediator M on outcome Y?\nIs there an effect of direct path of X on Y?\nIs there a mediated effect of coffee on problem solving?\nIf so, what type of mediation is it? 7.Why have you decided that type of mediation?"
  },
  {
    "objectID": "PSYC234/Week4.html#bootstrap-method",
    "href": "PSYC234/Week4.html#bootstrap-method",
    "title": "4. Mediation",
    "section": "Bootstrap method",
    "text": "Bootstrap method\nRun the bootstrap method Report the following values to 2 dp.\n\nWhat is the average causal mediation effect value?\nWhat is the average direct effect value?\nWhat is the total effect value?\nWhat proportion of variance goes through the indirect pathway?\n\nDraw the mediation analysis diagram with the correct values on the pathways.\nReport the bootstrap analysis and the results."
  },
  {
    "objectID": "PSYC214/Week8.html",
    "href": "PSYC214/Week8.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC214/Week8.html#learning-objectives",
    "href": "PSYC214/Week8.html#learning-objectives",
    "title": "Statistics for Psychologists",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this week’s lecture, we considered the procedures involved in performing a two-factor mixed and fully-within participants ANOVA. Using the hypothetical data on the Stroop effect presented in the lecture, in today’s lab session we will demonstrate how to analyse such ANOVA designs in R, including the calculation of the simple main effects and follow up procedures for breaking down simple main effects of factors with three or more levels. We will also demonstrate how to write up the results of two-factor mixed and fully within-participants designs.\nIf you get stuck at any point, be proactive and ask for help from one of the GTAs."
  },
  {
    "objectID": "PSYC214/Week8.html#getting-started",
    "href": "PSYC214/Week8.html#getting-started",
    "title": "Statistics for Psychologists",
    "section": "Getting Started",
    "text": "Getting Started\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n \nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\nOnce you are logged into the server, create a folder for today’s session. Navigate to the bottom right panel and under the Files option select the New Folder option. Name the new folder psyc214_lab_8. Please ensure that you spell this correctly otherwise when you set the directory using the command given below it will return an error.\nSe we can save this session on the server, click File on the top ribbon and select New project. Next, select existing directory and name the working directory ~/psyc214_lab_8 before selecting create project.\nFinally, open a script for executing today’s coding exercises. Navigate to the top left pane of RStudio, select File -&gt; New File -&gt; R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\n\nLet’s set our working directory:\n\nsetwd(\"~/psyc214_lab_8\")\n\nNow that you have created a folder for today’s session, it’s time to add the Week 8 files. Download the data and files needed for this lab here and save to the newly created folder psyc214_lab_8.\nBefore moving on, let’s load the relevant libraries/functions that we will be using in today’s session.\n\nlibrary(\"tidyverse\")  # For data storage and manipulation\nlibrary(\"tidyr\")      # For tidy data\nlibrary(\"rstatix\")    # For descriptives statistics, outlier detection, running the ANOVAs etc.\nsource(\"simple.R\")    # Custom function for generating the simple main effects\n\nNote that the custom function simple.R that we use to calculate the simple main effects must be present in your directory whenever you seek to use it."
  },
  {
    "objectID": "PSYC214/Week8.html#analysing-hypothetical-data-for-the-mixed-design-stroop-experiment",
    "href": "PSYC214/Week8.html#analysing-hypothetical-data-for-the-mixed-design-stroop-experiment",
    "title": "Statistics for Psychologists",
    "section": "Analysing hypothetical data for the mixed-design Stroop experiment",
    "text": "Analysing hypothetical data for the mixed-design Stroop experiment\nWe begin by analysing the mixed-design Stroop experiment described in the lecture. Recall that in this experiment, a researcher wants to test the hypothesis that response inhibition—the ability to suppress task-irrelevant information—is impaired in patients with schizophrenia. She predicts that if this is true, then a group of patients with schizophrenia will show a larger Stroop effect than a group of healthy controls. Our researcher administers a multi-trial Stroop task to two groups of participants in a 2 (group: healthy vs. schizophrenia) \\(\\times\\) 2 (trial type: congruent vs. incongruent) mixed design, where group is a between-participants factor and trial type is a within-participants factor.\nThe data set contains four columns:\n\nParticipant: represents the participant number, which ranges from 1–80, with \\(N\\) = 40 participants in each of the four conditions resulting from the combination of our two factors.\nGroup: represents whether the participant belongs to the healthy control group (Healthy) or the schizophrenia group (Schizophrenia).\nCongruent: represents the mean response time, averaged across trials, for congruent trials in which the colour word and the ink it is presented in are the same.\nIncongruent: represents the mean response time, averaged across trials, for incongruent trials in which the colour word and the ink it is presented in are different.\n\n\nImport data, set variables as factors, generate descriptive statistics, and perform assumption checks\nThe first thing you need to do is load the data into RStudio. Make sure that you name your data frame as stroopMixed.\n\n# *** ENTER YOUR OWN CODE HERE TO IMPORT THE DATA ***\n\n\n\n# A tibble: 80 × 4\n   Participant Group   Congruent Incongruent\n         &lt;dbl&gt; &lt;chr&gt;       &lt;dbl&gt;       &lt;dbl&gt;\n 1           1 Healthy       586         906\n 2           2 Healthy       608         773\n 3           3 Healthy       609         887\n 4           4 Healthy       621         775\n 5           5 Healthy       642         699\n 6           6 Healthy       593         707\n 7           7 Healthy       616         850\n 8           8 Healthy       721         763\n 9           9 Healthy       631         760\n10          10 Healthy       639         740\n# ℹ 70 more rows\n\n\nThe next thing we need to do is convert the Participant variable into a factor. We can do this with the following code:\n\n# Convert \"Participant into factor\nstroopMixed$Participant = factor(stroopMixed$Participant)\n\nOnce you have done this, you need to convert the Group variable into a factor. We’ll let you do this yourself using your own code:\n\n# *** ENTER YOUR OWN CODE HERE TO CONVERT \"GROUP\" INTO A FACTOR ***\n\nThe next thing we need to do is convert our data from wide format into long format. Sam discussed the distinction between these two formats in his Week 4 lab session and we covered it again in our Week 5 lab session. In brief, we need to create a new variable called TrialType that contains the different levels of this factor, congruent and incongruent, which are currently located in different columns. We can do this using the following piece of code which users the gather() function encountered previously:\n\n# Convert data into long format\nstroopMixedLong = gather(stroopMixed,TrialType,RT,Congruent:Incongruent,factor_key = TRUE)\n\nLet’s unpack what this is doing. The first argument to gather(), stroopMixed, is the name of our current data set in wide format. The second argument tells R we want a new column called TrialType. The third argument, RT, tells R the name of our dependent variable. The fourth argument tells R to take the columns Congruent through to Incongruent and combine them into a column labeled TrialType (our second argument). The final argument, factor_key = TRUE, tells R that we want this new column to be a factor. The results of this transformation are allocated to a new data frame called stroopMixedLong. This contains our data in long format and we will be using this version of the data set from henceforth.\nNext, we will generate some descriptive statistics (mean, standard deviation, and confidence intervals). You did this for a between-participants factorial design in last week’s seminar, so you can once again generate the code for this yourself.\n\n# *** ENTER YOUR OWN CODE HERE TO GENERATE DESCRIPTIVE STATISTICS ***\n\nIf you have executed the code correctly, then you should see the following output:\n\n\n# A tibble: 4 × 7\n  Group         TrialType   variable     n  mean    sd    ci\n  &lt;fct&gt;         &lt;fct&gt;       &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Healthy       Congruent   RT          40  623.  39.9  12.7\n2 Healthy       Incongruent RT          40  770.  51.4  16.4\n3 Schizophrenia Congruent   RT          40  625.  50.8  16.2\n4 Schizophrenia Incongruent RT          40  889.  44.3  14.2\n\n\nAt this stage, we would ordinarily perform various checks including identifying possible outliers and checking that our data satisfy the normality assumption. We won’t perform those checks today, not because they are not important (they most certainly are!), but rather because we don’t have the time to do so. There is one check that we will perform and that is to see if the homogeneity of variance assumption has been violated. In a mixed design like ours, the homogeneity of variance assumption only applies to the between-participants factors, but not the within-participants factors.\nOur between-participants factor is Group (healthy vs. schizophrenia) but remember that each group receives each level of our within-participants factor TrialType (congruent vs. incongruent). So, we need to test whether the variances are equal or not for the healthy and schizophrenia groups for congruent and incongruent trials separately. The following code will generate what we need:\n\nstroopMixedLong %&gt;% \n  # Organise the output by the \"TrialType\" factor\n  group_by(TrialType) %&gt;%\n  # Generate Levene's test on the \"Group\" factor\n  levene_test(RT ~ Group)\n\n# A tibble: 2 × 5\n  TrialType     df1   df2 statistic      p\n  &lt;fct&gt;       &lt;int&gt; &lt;int&gt;     &lt;dbl&gt;  &lt;dbl&gt;\n1 Congruent       1    78     3.95  0.0505\n2 Incongruent     1    78     0.747 0.390 \n\n\nLevene’s test is non-significant for congruent trials, p = .051 (although notice that it is only marginally short of significance), and also for incongruent trials, p = .390. So, we can safely assume that the homogeneity of variance assumption has been satisfied.\n\n\nRunning the mixed ANOVA\nTo run our ANOVA, we are going to use the anova_test function from the rstatix package. This is the function that I recommend you use for mixed or fully-within participants factorial designs. The code required to run the ANOVA is given below:\n\n# Create the mixed design ANOVA model\nstroopMixedModel = anova_test(data = stroopMixedLong, dv = RT, wid = Participant, between = Group, within = TrialType, detailed = TRUE)\n# Print the model summary\n(stroopMixedModel)\n\nANOVA Table (type II tests)\n\n           Effect DFn DFd        SSn      SSd         F         p p&lt;.05   ges\n1     (Intercept)   1  78 84525386.6 136336.6 48358.096 1.06e-110     * 0.996\n2           Group   1  78   147440.3 136336.6    84.353  4.74e-14     * 0.301\n3       TrialType   1  78  1692705.3 205941.9   641.108  2.27e-39     * 0.832\n4 Group:TrialType   1  78   137886.3 205941.9    52.224  2.91e-10     * 0.287\n\n\nTo create the model, the first argument we supplied to anova_test was the name of our data, stroopMixedLong. The second argument we supplied was our dependent variable, RT. The third argument we supplied was Participant, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our between-participants factor, Group. The fourth argument we supplied was our within-participants factor, TrialType.\nNotice that the resulting ANOVA table is different in format to those given in the lecture. The format given in the lecture follows a standard convention, but anova_test frustratingly uses a different format. The main effects of Group, TrialType, and the Group:TrialType interaction are each given on separate rows. The row corresponding to each effect contain the between-group degrees of freedom (DFn), the error degrees of freedom (DFd), the between-group sums of squares (SSn), the error sums of squares (SSd), the \\(F\\) ratio (F), and the p (p) value. On the bright side, this organisation makes it easier to locate the correct degrees of freedom when reporting the different outcomes.\nLooking at our ANOVA table, we can see that there is a significant main effect Group, p \\(&lt;\\) .001. Inspecting our descriptive statistics, we can see that this arose because mean response times are longer for the schizophrenia group than the healthy group. There is also a significant main effect of Trial Type, p \\(&lt;\\) .001, which arose because mean response times are longer for incongruent than congruent trials. Finally, there is a significant interaction between the two factors, p \\(&lt;\\) .001. We will defer interpretation of this interaction until later, once we have calculated the simple main effects and plotted the data. By the way, you can just ignore the first row of the table with the outcome named “intercept”.\nBefore we calculate the simple main effects, notice that the ANOVA table generated by anova_test does not give us the between-group mean squares or the error mean squares that are used to calculate the \\(F\\) ratios. This is a rather silly omission, and I’m not sure why the creators of the rstatix package thought it would be wise to exclude these. Now, you don’t need these values to interpret and report your ANOVA, but I do need to extract these values, so I can show you which error term is being used to test the different ANOVA outcomes and the simple main effects that we will calculate shortly.\nSo, we are going to calculate the mean squares for ourselves. We will first calculate the between-group mean squares. The between-group mean square is simply the between-group sums of squares (SSn) divided by its corresponding degrees of freedom (DFn). We can calculate this as follows:\n\n# Calculate the between-group mean sum of squares\nmixedMeanSquareBetween = stroopMixedModel$SSn/stroopMixedModel$DFn\n# Exclude the intercept (row 1 of 4) from the results\nmixedMeanSquareBetween[2:4]\n\n[1]  147440.3 1692705.3  137886.3\n\n\nThe first value is the between-group mean square for the main effect of Group (147440.3), the second value is the between-group mean square for the main effect of Trial Type (1692705.3), and the third value is the between-group mean square for the interaction (137886.3).\nNext, we will calculate the error mean squares. The error mean square is simply the error sums of squares (SSd) divided by its corresponding degrees of freedom (DFd). We can calculate this as follows:\n\n# Calculate the error mean sum of squares\nmixedMeanSquareError = stroopMixedModel$SSd/stroopMixedModel$DFd\n# Exclude the intercept (row 1 of 4) from the results\nmixedMeanSquareError[2:4]\n\n[1] 1747.906 2640.281 2640.281\n\n\nThe first value is the error mean square for the main effect of Group (1747.906), the second value is the error mean square for the main effect of Trial Type (2640.281), and the third value is the error mean square for the interaction (2640.281). Recall that in the lecture, we saw that in a mixed design, the error term for the within-participants factor is used to test both the main effect of that factor and any interaction involving that factor. Sure enough, we can see that the error term for testing the main effect of our within-participants factor, Trial Type, is the same as the one used to test the interaction (i.e., 2640.281).\nRemember that the \\(F\\) ratio for each outcome is calculated by dividing its between-group mean square by its error mean square. So, let’s calculate the \\(F\\) ratios for ourselves and check they align with what has been given to us in our ANOVA table. We can do that with the following code:\n\n# Calculate F ratios\nmixedFRatios = mixedMeanSquareBetween/mixedMeanSquareError\n# Print the results, excluding the intercept\n(mixedFRatios[2:4])\n\n[1]  84.35256 641.10811  52.22411\n\n\nThe first \\(F\\) ratio is for the main effect of Group (84.353), the second is for the main effect of Trial Type (641.108), and the third is for the interaction (52.224). If you compare these \\(F\\) values with those given in the ANOVA table, you will see that they are identical.\nTo be clear, when conducting a mixed ANOVA in the future, you don’t need to go through the steps of calculating the mean squares. I have gone through these steps with you because I need to make it clear what error terms we are using to test the different ANOVA outcomes and our simple main effects.\n\n\nSimple main effects analysis\nSince the interaction is significant, we need to calculate the simple main effects. Last week, when calculating the simple main effects for a fully-between participants design, we used the testInteractions() function in the phia package. Unfortunately, this package cannot be used with mixed and fully within-participants designs. Accordingly, I have created a custom function called simple() that will compute the simple main effects for you.\nRecall from the lecture that for mixed and within-participants factorial designs, we use an approach to calculating the simple main effects known as the pooled error terms approach. What this means is that the simple main effects of each factor are calculated using the same error term. However, the error term we use will differ depending on the factor for which we are calculating the simple main effects (contrast this approach with that we used for calculating the simple main effects in a fully between-participants design, where we used the same error term to calculate the simple main effects of all factors).\nBefore we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n# Get the mixed ANOVA table\nmixedAnovaTable = get_anova_table(stroopMixedModel)\n\nNext, we need to calculate the cell totals for each of the four conditions and the number of observations (i.e., scores) in each cell:\n\n# Get cell totals and counts\nmixedCellTotals = stroopMixedLong %&gt;%\n  # Organise the output by the\"Group\" and \"TrialType\" factors\n  group_by(TrialType, Group) %&gt;%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(RT),n = n())\n  # Print the results\n  (mixedCellTotals)\n\n# A tibble: 4 × 4\n# Groups:   TrialType [2]\n  TrialType   Group           sum     n\n  &lt;fct&gt;       &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Congruent   Healthy       24919    40\n2 Congruent   Schizophrenia 24999    40\n3 Incongruent Healthy       30799    40\n4 Incongruent Schizophrenia 35576    40\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Group at Trial Type. This means, we are going to:\n\nTest the difference between the healthy and schizophrenia groups for congruent trials only.\nTest the difference between the healthy and schizophrenia groups for incongruent trials only.\n\nTo do this, we need to declare Group as the “fixed” factor (we are always comparing the healthy and schizophrenia groups) and Group as the “across” factor (the comparison between the healthy and schizophrenia groups occurs “across” the congruent and incongruent levels of the Trial Type factor):\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"Group\"\nacross = \"TrialType\"\n\nTo get the simple main effects, we pass the cell totals, ANOVA table, and the fixed and across factors to our function simple():\n\n# Simple main effects of Group at Trial Type\nmixedSmeGroup = simple(mixedCellTotals,mixedAnovaTable,fixed,across)\n(mixedSmeGroup)\n\n       Levels Sum of Squares Degrees of Freedom Mean Square            F\n1   Congruent           80.0                  1      80.000   0.04576906\n2 Incongruent       285246.6                  1  285246.612 163.19337388\n3  Error term       136336.6                 78    1747.906   0.00000000\n             P\n1 8.311544e-01\n2 8.245446e-21\n3 0.000000e+00\n\n\nAs described in the lecture, to calculate the simple main effects of a given factor in a mixed design, we use the mean square error for the main effect of that factor from the original ANOVA table as the error term. The mean square error in the simple main effects table is given in column four (Mean Square) of row three (Error term). You will notice that this is the error mean square for the main effect of Group that we calculated earlier from our initial ANOVA table (i.e., 1747.906). If you divide the between-group mean squares for the effect of Group at congruent and incongruent trials by this value, it will give you the \\(F\\) ratios shown in the simple main effects table.\nLooking at the simple main effects, we can see that the simple main effect of Group at congruent trials is not significant, \\(p\\) = 0.831. This indicates that mean response times for congruent trials did not differ between the healthy and schizophrenia groups. However, the simple main effect of Group at incongruent trials is significant, \\(p\\) &lt; .001. Looking at the descriptive statistics we generated earlier, we can see that this is because mean response times are longer on incongruent trials for the schizophrenia group than the healthy group.\nNext, we are going to calculate the simple main effects of the factor Trial Type at Group. This means, we are going to:\n\nTest the difference between congruent and incongruent trials for the healthy group only.\nTest the difference between congruent and incongruent trials for the schizophrenia group only.\n\nTo do this, we now need to declare Trial Type as the “fixed” factor and Group as the “across” factor:\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"TrialType\"\nacross = \"Group\"\n\nWe then generate the simple main effects of Trial Type with the following:\n\n# Simple main effects of Trial Type at Group\nmixedSmeTrialType = simple(mixedCellTotals,mixedAnovaTable,fixed,across)\n(mixedSmeTrialType)\n\n         Levels Sum of Squares Degrees of Freedom Mean Square        F\n1       Healthy       432180.0                  1  432180.000 163.6871\n2 Schizophrenia      1398411.6                  1 1398411.612 529.6451\n3    Error term       205941.9                 78    2640.281   0.0000\n             P\n1 7.609756e-21\n2 1.632592e-36\n3 0.000000e+00\n\n\nRemember, to calculate the simple main effects of a given factor in a mixed design, we use the mean square error for the main effect of that factor from the original ANOVA table as the error term. The mean square error in the simple main effects table is given in column four (Mean Square) of row three (Error term). You will notice that this is the error mean square for the main effect of Trial Type that we calculated earlier from our initial ANOVA table (i.e., 2640.281). If you divide the between-group mean squares for the effect of Trial Type at healthy and schizophrenia groups by this value, it will give you the \\(F\\) ratios shown in the simple main effects table.\nLooking at the simple main effects, we can see that the simple main effect of Trial Type at healthy is significant, \\(p\\) \\(&lt;\\) .001. Inspecting our descriptive statistics, we can see that this is because for the healthy group, mean response times are longer for incongruent than congruent trials. The simple main effect of Trial Type at schizophrenia is also significant, \\(p\\) \\(&lt;\\) .001. Looking at our descriptive statistics, we can see that this is because for the schizophrenia group, mean response times are also longer for incongruent than congruent trials. Thus, both the healthy and schizophrenia groups show a Stroop effect (longer response times for incongruent than congruent trials), but the effect is larger for the schizophrenia group, which has a much smaller \\(p\\) value (and a correspondingly larger \\(F\\) ratio).\n\n\nWriting up the results\n\n\n\nFigure 1. Mean response times as a function of the group and trial type manipulations. Error bars represent 95% confidence intervals.\n\n\nFigure 1 shows mean response times as a function of the group and trial type manipulations. These data were subjected to a 2 (group: healthy vs. schizophrenia) \\(\\times\\) 2 (trial type: congruent vs. incongruent) mixed Analysis of Variance. There was a significant main effect of group, F(1, 78) = 84.35, p \\(&lt;\\) .001, with longer response times in the schizophrenia group than the healthy group, a significant main effect of trial type, F(1, 78) = 641.11, p &lt; .001, with longer response times for incongruent than congruent trials, and a significant interaction between the two factors, F(1, 78) = 52.22, p &lt; .001.\nTo scrutinise the interaction, a simple main effects analysis was undertaken. For the simple main effects of group at trial type, response times on congruent trials did not differ significantly between the healthy and schizophrenia groups, F(1, 78) = .05, p = .831, whereas response times were significantly longer on incongruent trials for the schizophrenia group compared to the healthy group, F(1, 78) = 163.19, p \\(&lt;\\) .001. For the simple main effects of trial type at group, response times were significantly longer on incongruent trials than congruent trials for the healthy group, F(1, 78) = 163.69, p \\(&lt;\\) .001, and for the schizophrenia group, F(1, 78) = 529.65, p \\(&lt;\\) .001, although the effect was larger for the schizophrenia group.\nHence, the interaction arose because the schizophrenia group demonstrated a larger Stroop effect than the healthy group and this was due to longer response times on incongruent, but not congruent, trials."
  },
  {
    "objectID": "PSYC214/Week8.html#analysing-hypothetical-data-for-the-fully-within-participants-stroop-experiment",
    "href": "PSYC214/Week8.html#analysing-hypothetical-data-for-the-fully-within-participants-stroop-experiment",
    "title": "Statistics for Psychologists",
    "section": "Analysing hypothetical data for the fully within-participants Stroop experiment",
    "text": "Analysing hypothetical data for the fully within-participants Stroop experiment\nWe turn now to an analysis of the fully within-participant Stroop experiment described in the lecture. In this experiment, a researcher wants to examine whether the magnitude of the Stroop effect decreases with practice at the task. The Stroop task is administered over three successive blocks of trials and the expectation is that the magnitude of the Stroop effect will decrease gradually across blocks. She administers a multi-trial Stroop task to a single group of participants in a 2 (trial type: congruent vs. incongruent) \\(\\times\\) 3 (block: block 1 vs. block 2 vs. block 3) fully within-participants design.\nThe data set contains seven columns:\n\nParticipant: represents the participant number, which ranges from 1–40, with \\(N\\) = 40 participants.\nCongruent_Block1: represents the mean response time, averaged across trials, for congruent trials in the first block of trials.\nCongruent_Block2: represents the mean response time, averaged across trials, for congruent trials in the second block of trials.\nCongruent_Block3: represents the mean response time, averaged across trials, for congruent trials in the third block of trials.\nIncongruent_Block1: represents the mean response time, averaged across trials, for incongruent trials in the first block of trials.\nIncongruent_Block2: represents the mean response time, averaged across trials, for incongruent trials in the second block of trials.\nIncongruent_Block3: represents the mean response time, averaged across trials, for incongruent trials in the third block of trials.\n\n\nImport data, set variables as factors, and generate descriptive statistics\nThere’s a bit to get through in this analysis, so I am going to supply the code for everything that follows. We begin, as always, by loading our data set:\n\n# Import data\nstroopWithin = read_csv(\"StroopWithin.csv\")\n(stroopWithin)\n\n\n\n# A tibble: 40 × 7\n   Participant Congruent_Block1 Congruent_Block2 Congruent_Block3\n         &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;            &lt;dbl&gt;\n 1           1              627              687              563\n 2           2              558              681              674\n 3           3              635              709              575\n 4           4              626              673              663\n 5           5              610              610              643\n 6           6              568              655              715\n 7           7              635              641              573\n 8           8              530              677              545\n 9           9              613              621              638\n10          10              542              607              636\n# ℹ 30 more rows\n# ℹ 3 more variables: Incongruent_Block1 &lt;dbl&gt;, Incongruent_Block2 &lt;dbl&gt;,\n#   Incongruent_Block3 &lt;dbl&gt;\n\n\nAs is the case for a fully within-participants design, the data are in entirely wide format and we need to get them into long format for the analysis. We are going to start by grouping the columns Congruent_Block1 through to Incongruent_Block3 into a new variable called Group using the gather function used previously:\n\n# Convert data into long format\nstroopWithinLong = stroopWithin %&gt;%\n  gather(Group,RT,Congruent_Block1:Incongruent_Block3,factor_key = TRUE)\n(stroopWithinLong)\n\n# A tibble: 240 × 3\n   Participant Group               RT\n         &lt;dbl&gt; &lt;fct&gt;            &lt;dbl&gt;\n 1           1 Congruent_Block1   627\n 2           2 Congruent_Block1   558\n 3           3 Congruent_Block1   635\n 4           4 Congruent_Block1   626\n 5           5 Congruent_Block1   610\n 6           6 Congruent_Block1   568\n 7           7 Congruent_Block1   635\n 8           8 Congruent_Block1   530\n 9           9 Congruent_Block1   613\n10          10 Congruent_Block1   542\n# ℹ 230 more rows\n\n\nThe first argument to gather() tells R we want to create a variable called Group. The second argument tells R that RT is the dependent variable. The third argument, Congruent_Block1:Incongruent_Block3, tells R that we want to bundle the columns Congruent_Block1 through to Incongruent_Block3 into the new variable, Group. The fourth argument, factor_key = TRUE, tells R that we want to make this variable a factor. The results of this transformation have been assigned to a new data frame called stroopWithinLong.\nLooking at the new data frame we have created, we can see that it is not exactly what we want. Our new variable Group actually contains both of our independent variables. What we want is to separate these independent variables into two separate columns. We can do that with the separate function:\n\n# Divide Group into seperate columns for Trial Type and Block\nstroopWithinLongSep = stroopWithinLong %&gt;%\n  separate(Group, c(\"TrialType\",\"Block\"))\n(stroopWithinLongSep)\n\n# A tibble: 240 × 4\n   Participant TrialType Block     RT\n         &lt;dbl&gt; &lt;chr&gt;     &lt;chr&gt;  &lt;dbl&gt;\n 1           1 Congruent Block1   627\n 2           2 Congruent Block1   558\n 3           3 Congruent Block1   635\n 4           4 Congruent Block1   626\n 5           5 Congruent Block1   610\n 6           6 Congruent Block1   568\n 7           7 Congruent Block1   635\n 8           8 Congruent Block1   530\n 9           9 Congruent Block1   613\n10          10 Congruent Block1   542\n# ℹ 230 more rows\n\n\nThis bit of code tells R to separate the variable Group into two new variables, one called TrialType and one called Block. The results are stored in a new data frame called stroopWithinLongSep, which is the data frame we will be using henceforth. Looking at this data frame we can see that our transformation has had the desired effect—we now have two new columns corresponding to each of our two independent variables.\nHowever, our two new variables are currently stored as characters (they have the labels &lt;chr&gt; beneath the variable names) and we need to convert them to factors. We also need to convert the Participant variable into a factor. So, let’s do that next:\n\n# Convert variables into factors\nstroopWithinLongSep$Participant = factor(stroopWithinLongSep$Participant)\nstroopWithinLongSep$TrialType   = factor(stroopWithinLongSep$TrialType)\nstroopWithinLongSep$Block       = factor(stroopWithinLongSep$Block)\n\nRight, now is as good a time as any to get some descriptive statistics:\n\n# Get descriptive statistics\nwithinDescriptives = stroopWithinLongSep %&gt;%\n  # Organise the output by the\"TrialType\" and \"Block\" factors\n  group_by(TrialType, Block) %&gt;%\n  # Request means, standard deviations, and confidence intervals\n  get_summary_stats(RT, show = c(\"mean\",\"sd\",\"ci\"))\n  # Print the results\n  (withinDescriptives)\n\n# A tibble: 6 × 7\n  TrialType   Block  variable     n  mean    sd    ci\n  &lt;fct&gt;       &lt;fct&gt;  &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 Congruent   Block1 RT          40  615.  48.9  15.6\n2 Congruent   Block2 RT          40  631.  50.8  16.3\n3 Congruent   Block3 RT          40  632.  53.8  17.2\n4 Incongruent Block1 RT          40  832.  43.7  14.0\n5 Incongruent Block2 RT          40  723.  41.2  13.2\n6 Incongruent Block3 RT          40  650.  49.1  15.7\n\n\nTime is limited, so we are once again going to skip over the usual checks, which you should otherwise always perform.\nHowever, because one of our within-participants factors contains three levels (i.e., Block) there is one check that will be performed when we run the ANOVA. Specifically, this is a check to establish whether the sphericity assumption has been violated. Sam introduced you to this assumption when he discussed single-factor within-participants ANOVA, so I will only describe it briefly. The sphericity assumption states that for a within-participants design with three or more levels, the variance of the difference scores between one pair of levels should not differ significantly from the variances of the difference scores for every other possible pair of levels.\nThe assumption is tested using Mauchly’s test of sphericity, which is applied to any outcome involving a factor with three or more levels. If the test result is significant, then the assumption has been violated. If this occurs, then we must adopt the Greenhouse-Geisser correction for violations of this assumption.\n\n\nRunning the mixed ANOVA\nTo run our ANOVA, we are once again going to use the anova_test function from the rstatix package. The code required to run the ANOVA is given below:\n\n# Create the within-participants design ANOVA model\nstroopWithinModel = anova_test(data = stroopWithinLongSep, dv = RT, wid = Participant, within = c(TrialType, Block), detailed = TRUE)\n(stroopWithinModel)\n\nANOVA Table (type III tests)\n\n$ANOVA\n           Effect DFn DFd         SSn      SSd         F        p p&lt;.05   ges\n1     (Intercept)   1  39 111105237.6  77509.9 55903.885 3.54e-63     * 0.995\n2       TrialType   1  39    713623.2  84476.3   329.457 1.28e-20     * 0.568\n3           Block   2  78    272694.5 221792.5    47.951 2.63e-14     * 0.335\n4 TrialType:Block   2  78    403873.6 158005.4    99.687 3.25e-22     * 0.427\n\n$`Mauchly's Test for Sphericity`\n           Effect     W     p p&lt;.05\n1           Block 0.949 0.368      \n2 TrialType:Block 0.952 0.394      \n\n$`Sphericity Corrections`\n           Effect   GGe      DF[GG]    p[GG] p[GG]&lt;.05   HFe   DF[HF]    p[HF]\n1           Block 0.951   1.9, 74.2 1.01e-13         * 0.999  2, 77.9 2.72e-14\n2 TrialType:Block 0.954 1.91, 74.44 2.61e-21         * 1.002 2, 78.17 3.25e-22\n  p[HF]&lt;.05\n1         *\n2         *\n\n\nTo create the model, the first argument we supplied to anova_test was the name of our data, stroopWithinLongSep. The second argument we supplied was our dependent variable, RT. The third argument we supplied was Participant, which is the column containing the individuals/participants identifier. The fourth argument we supplied was our two within-participants factors, TrialType and Block.\nThe ANOVA output now contains three tables. The first is the main ANOVA table, which is organised in the same format as that produced for our mixed design example from earlier.\nThe second table presents the results of Mauchly’s test of sphericity, which has been applied to the factor Block, which has three levels, and the interaction between Trial Type and Block. In both instances, we can see that the test result is non-significant, which means the sphericity assumption has been met.\nThe third table gives the Greenhouse-Geisser corrected ANOVA table for the main effect of Block and the interaction between Trial Type and Block. Had we violated the sphericity assumption, it is these values that we would report when writing up our ANOVA, instead of the values taken from the original ANOVA table (the main effect of Trial Type would be drawn from the original ANOVA table as it only has two levels, and therefore the sphericity assumption does not apply to this outcome). Because we did not violate the sphericity assumption on this occasion, we can focus solely on the original ANOVA table.\nYou may at this point be wondering why Mauchly’s test of sphericity and the Greenhouse-Geisser correction were not produced when we ran the ANOVA for our mixed design example. The answer is simple; for that design our within-participants factor only contained two levels, so the sphericity assumption did not apply. The anova_test function only generates these tests and corrections when at least one of the within-participants factors has three or more levels.\nLet’s redirect our attention to the original ANOVA table. We can see that there is a significant main effect Trial Type, p \\(&lt;\\) .001. Looking at our descriptive statistics, we can see that this arose because response times are longer for incongruent than congruent trials. There is also a significant main effect of Block, p \\(&lt;\\) .001, which arose because response times get quicker across blocks as participants obtain more experience with the task. There is also a significant interaction between the two factors, p \\(&lt;\\) .001. We will defer interpretation of this interaction until later, once we have calculated the simple main effects and plotted the data.\nAs for our mixed design example, the ANOVA table does not include the mean square values for the outcomes. Previously, we calculated the between-group and error mean squares for ourselves, and verified that these produced the obtained \\(F\\) ratios when we divided the former by the latter. This time, we will just generate the error mean squares. Again, this is so I can show you which error term is being used to test each ANOVA outcome and the simple main effects of each factor.\n\n# Calculate the error mean sum of squares\nwithinMeanSquareError = stroopWithinModel$ANOVA$SSd/stroopWithinModel$ANOVA$DFd\n# Exclude the intercept (row 1 of 4) from the results\nwithinMeanSquareError[2:4]\n\n[1] 2166.059 2843.493 2025.710\n\n\nThe first value is the error mean square for the main effect of Trial Type (2166.059), the second value is the error mean square for the main effect of Block (2843.493), and the third value is the error mean square for the interaction (2025.710). Recall that in the lecture, we saw that in a fully within-participants design the two main effects and the interaction each have their own error terms, and sure enough we can see that the error term is different for each outcome.\n\n\nSimple main effects\nSince our interaction is once again significant, we need to proceed to calculate the simple main effects. Before we can calculate the simple main effects, there are a few things we need to do. First, we need to store our ANOVA table in a dataframe:\n\n# Get ANOVA table\nwithinAnovaTable = get_anova_table(stroopWithinModel)\n\nNext, we need to calculate the cell totals for each of the six conditions and the number of observations (i.e., scores) in each cell:\n\n# Calculate cell totals and counts\nwithinCellTotals = stroopWithinLongSep %&gt;%\n  # Organise the output by the TrialType\" and \"Block\" factors\n  group_by(TrialType, Block) %&gt;%\n  # Request cell totals and number of observations (i.e., scores)\n  summarise(sum = sum(RT),n = n())\n  # Print the results\n  (mixedCellTotals)\n\n# A tibble: 4 × 4\n# Groups:   TrialType [2]\n  TrialType   Group           sum     n\n  &lt;fct&gt;       &lt;fct&gt;         &lt;dbl&gt; &lt;int&gt;\n1 Congruent   Healthy       24919    40\n2 Congruent   Schizophrenia 24999    40\n3 Incongruent Healthy       30799    40\n4 Incongruent Schizophrenia 35576    40\n\n\nThen, we need to specify which simple main effects we want to generate. We are first going to calculate the simple main effects of the factor Trial Type at Block. This means, we are going to:\n\nTest the difference between congruent and incongruent trials for Block 1 only.\nTest the difference between congruent and incongruent trials for Block 2 only.\nTest the difference between congruent and incongruent trials for Block 3 only.\n\nNotice that because Trial Type has only two levels, the simple main effects of this factor involve only pairwise comparisons. To generate these simple main effects, we need to declare Trial Type as the “fixed” factor (we are always comparing the congruent and incongruent trials) and Block as the “across” factor (the comparison between the congruent and incongruent trials occurs “across” the Block 1, Block 2, and Block 3 levels of the Block factor):\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"TrialType\"\nacross = \"Block\"\n\nTo get the simple main effects of Trial Type, we pass the cell totals, ANOVA table, and the fixed and across factors to our function simple():\n\n# Simple main effects of Trial Type at Block\nwithinSmeTrialType = simple(withinCellTotals,withinAnovaTable,fixed,across)\n(withinSmeTrialType)\n\n      Levels Sum of Squares Degrees of Freedom Mean Square          F\n1     Block1     941997.012                  1  941997.012 434.889848\n2     Block2     168820.312                  1  168820.312  77.938931\n3     Block3       6679.512                  1    6679.512   3.083717\n4 Error term      84476.296                 39    2166.059   0.000000\n             P\n1 9.362214e-23\n2 7.711308e-11\n3 8.693084e-02\n4 0.000000e+00\n\n\nAs described in the lecture, to calculate the simple main effects of a given factor in a within-participants design, we use the mean square error for the main effect of that factor from the original ANOVA table as the error term. The mean square error in the simple main effects table is given in column four (Mean Square) of row three (Error term). You will notice that this is the error mean square for the main effect of Trial Type that we calculated earlier from our initial ANOVA table (i.e., 2166.059). If you divide the between-group mean squares for the effect of Trial Type at Block 1, Block 2, and Block 3 by this value, it will give you the \\(F\\) ratios given in the simple main effects table.\nLooking at the simple main effects, we can see that the simple main effect of Trial type at Block 1 is significant, \\(p\\) \\(&lt;\\) .001. Looking at our table of descriptive statistics, we can see that this is because mean response times for incongruent trials were longer than for congruent trials. The simple main effect of Trial Type is also significant at Block 2, \\(p\\) \\(&lt;\\) .001, again reflecting longer mean response times for incongruent than congruent trials. However, the simple main effect of Trial Type at Block 3 is not significant, \\(p\\) = .087. Thus, we obtained a statistically reliable Stroop effect for Blocks 1 and 2, but not for Block 3.\nNext, we are going to calculate the simple main effects of the factor Block at Trial Type. This means, we are going to:\n\nTest the differences between Block 1, Block 2, and Block 3 for congruent trials only.\nTest the differences between Block 1, Block 2, and Block 3 for incongruent trials only.\n\nNotice that because Block has three levels, the simple main effects of this factor involve more than simple pairwise comparisons. To test these simple main effects, we now need to declare Block as the “fixed” factor and Trial Type as the “across” factor:\n\n# Create \"fixed\" and \"across\" factors\nfixed  = \"Block\"\nacross = \"TrialType\"\n\nWe then generate the simple main effects of Block with the following:\n\n# Simple main effects of Block at Trial Type\nwithinSmeBlock = simple(withinCellTotals,withinAnovaTable,fixed,across)\n(withinSmeBlock)\n\n       Levels Sum of Squares Degrees of Freedom Mean Square          F\n1   Congruent       7616.717                  2    3808.358   1.339324\n2 Incongruent     668951.450                  2  334475.725 117.628461\n3  Error term     221792.467                 78    2843.493   0.000000\n             P\n1 2.679808e-01\n2 2.828545e-24\n3 0.000000e+00\n\n\nRemember, to calculate the simple main effects of a given factor in a within-participants design, we use the mean square error for the main effect of that factor from the original ANOVA table as the error term. The mean square error in the simple main effects table is given in column four (Mean Square) of row three (Error term). You will notice that this is the error mean square for the main effect of Block that we calculated earlier from our initial ANOVA table (i.e., 2843.493). If you divide the between-group mean squares for the effect of Block at congruent and incongruent trials by this value, it will give you the \\(F\\) ratios given in the simple main effects table.\nLooking at the simple main effects, we can see that the simple main effect of Block at congruent trials is not significant, \\(p\\) = .268. Hence, mean response times do not differ reliably across blocks for congruent trials. However, the simple main effect of Block at incongruent trials is significant, \\(p\\) \\(&lt;\\) .001. Since there are three levels in the factor Block, this significant simple main effect is like the outcome of a significant single-factor ANOVA with three levels—it tells us that there are differences between the group means, but it does not tell us where they are located. To find out, we need to perform some follow up tests. We have a few options available to us.\nSuppose we hypothesised the interaction from the beginning and that we are only interested in the difference between Block 1 and Block 2, and Block 2 and Block 3 for incongruent trials, but we are not interested in the third comparison, which is between Block 1 and Block 3. In this case, we would have specified planned comparisons, because we do not intend to conduct all possible comparisons, only the ones relevant to testing our specific hypotheses. In this case, we could simply run two repeated measures \\(t\\)-tests, one comparing Block 1 and Block 2 at incongruent trials, and one comparing Block 2 and Block 3 at incongruent trials. Should we apply the Bonferroni correction to these comparisons? One rule of thumb is that if the number of comparisons is one less than the number of levels, i.e., (\\(a\\)-1) comparisons where \\(a\\) is the number of levels in the factor, then we do not need to apply the Bonferroni correction. In our case, our simple main effect has three levels and we are conducting two comparisons, which is one less than the number of levels, so we can proceed without using the Bonferroni correction (for those occasions where there are four or more levels, this rule of thumb does not apply and you should definitely use the Bonferroni correction). Alternatively, suppose we did not specify at the outset which comparisons we would perform if the interaction was significant. In this case, we would perform all three comparisons and we would use a post-hoc test, such as the Tukey test.\nFour our purposes, we are going to perform planned comparisons. To perform these comparisons, we first need to filter our data so that they only contain responses on incongruent trials (we are excluding responses on congruent trials). The following code gives us what we want:\n\n# Filter the data so they only contain responses on \"Incongruent\" trials\nfilter4SME = filter(stroopWithinLongSep, TrialType == \"Incongruent\")\n(filter4SME)\n\n# A tibble: 120 × 4\n   Participant TrialType   Block     RT\n   &lt;fct&gt;       &lt;fct&gt;       &lt;fct&gt;  &lt;dbl&gt;\n 1 1           Incongruent Block1   863\n 2 2           Incongruent Block1   870\n 3 3           Incongruent Block1   794\n 4 4           Incongruent Block1   872\n 5 5           Incongruent Block1   813\n 6 6           Incongruent Block1   856\n 7 7           Incongruent Block1   901\n 8 8           Incongruent Block1   818\n 9 9           Incongruent Block1   799\n10 10          Incongruent Block1   829\n# ℹ 110 more rows\n\n\nThe new data frame filter4SME is a version of our data in which only congruent trials are included. We can then use the pariwise_t_test function that you have used many times to run our \\(t\\)-tests (making sure to specify paired = TRUE as we want repeated measures comparisons!):\n\nfilter4SME %&gt;% \n  # Generate the t-tests for the two comparisons of interest\n  pairwise_t_test(RT ~ Block, paired = TRUE,\n    # Specify the comparisons we want\n    comparisons = list(c(\"Block1\",\"Block2\"),c(\"Block2\",\"Block3\")))\n\n# A tibble: 2 × 10\n  .y.   group1 group2    n1    n2 statistic    df        p    p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt; &lt;chr&gt;       \n1 RT    Block1 Block2    40    40      9.90    39 3.38e-12 6.76e-12 ****        \n2 RT    Block2 Block3    40    40      7.03    39 1.96e- 8 1.96e- 8 ****        \n\n\nThe comparison between Block 1 and Block 2 is significant, p \\(&lt;\\) .001, as is the comparison between Block 2 and Block 3, p \\(&lt;\\) .001. Looking at our descriptive statistics, we can see that this is because mean response times are faster for incongruent trials in Block 2 than in Block 1, and mean response times for incongruent trials are faster in turn in Block 3 than in Block 2. Thus, with practice on the Stroop task, participants responses on incongruent trials get gradually quicker.\n\n\nWriting up the results\n\n\n\nFigure 2. Mean response times as a function of the trial type and block manipulations. Error bars represent 95% confidence intervals.\n\n\nFigure 2 shows mean response times as a function of the trial type and block manipulations. These data were subjected to a 2 (trial type: congruent vs. incongruent) \\(\\times\\) 3 (block: block 1 vs. block 2 vs block 3) fully within-participants Analysis of Variance. There was a significant main effect of trial type, F(1, 39) = 329.46, p &lt; .001, with response times being longer for incongruent than congruent trials, a significant main effect of block, F(2, 78) = 47.95, p &lt; .001, with response times getting quicker across blocks, and a significant interaction between the two factors, F(2, 78) = 99.69, p &lt; .001.\nTo scrutinise the interaction, a simple main effects analysis was undertaken. For the simple main effects of trial type at block, response times were significantly longer on incongruent trials than congruent trials in block 1, F(1, 39) = 434.89, p &lt; .001, and block 2, F(1, 39) = 77.94, p &lt; .001, but not in block 3, F(1, 39) = 3.08, p = .087. Turning to the simple main effects of block at trial type, response times did not differ significantly across blocks for congruent trials, F(2, 78) = 1.34, p = .268, but they did differ significantly across blocks for incongruent trials, F(2, 78) = 117.63, p &lt; .001. Planned comparisons revealed that response times on incongruent trials were faster in block 2 than in block 1, t(39) = 9.90, p &lt; .001, and faster in turn in block 3 than in block 2, t(39) = 7.03, p &lt; .001.\nHence, the interaction arose because the magnitude of the Stroop effect decreased across blocks and this was due to the speeding up of responses on incongruent, but not congruent, trials.\n\n\nAdditional tasks\nPhew!!! We have covered a lot of ground in today’s session – well done for making it through the exercises. If you should happen to want more, then you could recreate the plots in Figures 1 and 2 by co-opting the code we used to generate line plots in lab session 6."
  },
  {
    "objectID": "PSYC214/index.html",
    "href": "PSYC214/index.html",
    "title": "Statistics for Group Comparisons",
    "section": "",
    "text": "Teaching team: Sam Russell and Mark Hurlstone (module coordinator)\nIn this module, you will learn about a statistical technique for performing group comparisons known as Analysis of Variance (ANOVA). It is one of the most common statistical techniques, so it is important to have a thorough understanding of it, which is why we are devoting an entire module to the technique. We will cover a range of statistical tests associated with ANOVA, starting with designs involving a single independent variable (single factor designs), before moving onto designs involving multiple independent variables (factorial designs). We will also cover various follow-up procedures (procedures for identifying which specific conditions differ from one another), including planned comparisons and post-hoc tests, and how to interpret interactions (when the effect of one independent variable depends on the effect of another independent variable, and vice versa). The R statistical package is used throughout and students are expected to develop the skills for conducting appropriate analyses and interpreting the resulting output.\n\n\nWeeks 1-5: Single-Factor Designs - Taught by Dr Sam Russell\n\nWeek 1: Measurement, Variance and Inferential Statistics\nWeek 2: One-Factor Between-Participants ANOVA\nWeek 3: Assumptions of ANOVA and Follow-Up Procedures\nWeek 4: One-Factor Within-Participants ANOVA\nWeek 5: Interim Summary\n\nWeeks 6-10: Factorial Designs - Taught by Dr Mark Hurlstone\n\nWeek 6: Introduction To Factorial Designs\nWeek 7: Two-Factor Between-Participants ANOVA\nWeek 8: Two-Factor Mixed and Within-Participants ANOVA\nWeek 9: Three-Factor ANOVA\nWeek 10: Class Test"
  },
  {
    "objectID": "PSYC214/Week1.html",
    "href": "PSYC214/Week1.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC214/Week1.html#general-introduction",
    "href": "PSYC214/Week1.html#general-introduction",
    "title": "Statistics for Psychologists",
    "section": "1 - General Introduction",
    "text": "1 - General Introduction\n“Once more unto the breach, dear friends, once more” | King Henry V, by William Shakespeare.\n\n\n\nFifty shades of rust\n\n\nThe formation of rust can occur quickly and within minutes or slowly, over many years. It all depends on the material that is rusting, it’s lack of use and the environment!\nIn this analogy, let’s consider our ‘R brains’ as the material vulnerable to rusting. With individual differences, some of us may remember more of last year’s statistic course than others - and that’s completely normal.\nWhen considering repeated use and our environments, it is also unlikely that many of us have interacted with R and statistics over the intervening summer months. Quite sensibly we’ve enjoyed a non-academic environment - holidays, summer jobs, commitments to family and friends, these are all important things that have rightly taken priority!\nIt therefore an entirely natural process for folks to feel rusty and hesitant when re-engaging with R and statistics. As such, in this first session, let’s get re-accustomed with our old friend ‘R’.\n\n\n1.1 Access to R Studio\nYou will recall from last year that R Studio is a free, open-source data science engine that works as a giant calculator on steroids. Beyond number crunching, R studio allows you to organise your data, construct beautiful tables and graphs, as well as document your code and findings for others to inspect.\n\n\n\nVintage computational power and R Studio\n\n\nTo log in to the R server, first make sure that you have the VPN switched on, or you will need to be connected to the university network (Eduroam). To set up the VPN, follow ISS instructions here or connecting to Eduroam here.\nWhen you are connected, navigate to https://psy-rstudio.lancaster.ac.uk, where you will be shown a login screen that looks like the below. Click the option that says “Sign in with SAML”.\n \nThis will take you through to the University login screen, where you should enter your username (e.g. ivorym) and then your university password. This will then redirect you to the R server where you can start using RStudio!\n\n\n\n\n\n\n\n\nNote\n\n\n\nIf you have already logged in through the university login already, perhaps to get to the portal, then you may not see the username/password screen. When you click login, you will be redirected straight to RStudio. This is because the server shares the login information securely across the university.\n\n\n\n\n\n1.2 Creating a folder for today’s session\nOnce you are logged into the server, create a folder for today’s session. This will be where we will house today’s data and script. This will allow you to save you store your work and return at a later date (for example, around exam time - wink wink)\nNavigate to the bottom right panel (see figure below) and click Home. Next, under the Files option select the New Folder option. Name the new folder psyc214_lab_1. Please ensure that you spell this correctly otherwise when you set the directory using the command given below it will return an error.\n\n\n\nCreating a folder on your server\n\n\n\n\n\n1.3 Uploading today’s data file and creating a script\nNow that you have created a folder for today’s session, it’s time to add the Week 1 data file. You can download the file from here.\nNext, in the RStudio Server open your new psyc214_lab_1 folder. When in the new folder, select the Upload tab (see figure below). This will present a box that will ask where the data is that you want to upload. Click on Browse, find the week1_robo_lab.csv file on your desktop and click OK.\n\n\n\nUploading today’s data\n\n\nSo we can save this session on the server, click File on the top ribbon and select New project. Next, select existing directory and name the working directory ~/psyc213_lab_1 before selecting create project.\nFinally, open a script for executing today’s coding exercises. Navigate to the top left pane of RStudio, select File -&gt; New File -&gt; R Script. Working from a script will make it easier to edit your code and you will be able to save your work for a later date.\n\n\n\nCreating a script\n\n\nBe sure to save the script using the File -&gt; Save As….\n\n\n\n1.4 Setting the working directory and opening the data file\nA working directory is the default location or folder on your computer/server by which R will read/save any files. It can be set with the following R code:\n\n  setwd(\"/PSYC214\")\n\n\nsetwd(\"~/psyc214_lab_1\")\n\nNow you’ve downloaded the data and set the working directory it’s time to open the data file in R studio. To do this, please type the following code:\n\nlab1_data &lt;- read.csv(\"week1_robo_lab.csv\")\n\nwhere ‘lab1_data’ is the name we’ve assigned that R will recognise when it calls up our data, ‘read.csv’ is the R command to pull up the data and ‘week1_robo_lab.csv’ is the name of the original data file you downloaded and stored.\nNote. During the rest of this session, you will not need to refer to the original downloaded .csv data file. R has all the information stored under the ‘lab1_data’ variable. Further note, you could have called ‘lab1_data’ by pretty much any name you like… ‘df1’, ‘robo_1’, ‘I.love.stats.com’, etc. - the name is somewhat arbitrary. For the purpose of this lab session and for ease of read, ‘lab1_data’, is perhaps more suitable.\n\n\n\n1.5 loading the relevant library\nFinally, let’s load the relevant libraries that we will be using in today’s session:\n\nlibrary(tidyverse)\nlibrary(rstatix)\nlibrary(ggpubr)"
  },
  {
    "objectID": "PSYC214/Week1.html#todays-lab-activities",
    "href": "PSYC214/Week1.html#todays-lab-activities",
    "title": "Statistics for Psychologists",
    "section": "2 - Today’s lab activities",
    "text": "2 - Today’s lab activities\nNow that you have loaded the dataset, let’s have a play.\n\n2.1 Some background information about the dataset\n\n\n\nIntroducing the robo-lab study\n\n\nIn this lab session, we will examine the fictitious ‘robo_lab’ dataset. Here, a team of researchers wanted to know whether LU statistics students would respond well to a new, ambitious initiative to deploy a team of synthetic robots as lab assistants.\n\n\n\n2.2 Study manipulation\nThe research team developed two robot prototypes and wanted to test which prototype would be optimal for classroom teaching. In a controlled experiment, the researchers randomly assigned groups of students either Robot A(lpha) or Robot B(eta). Note this is one factor design (Robot assignment) made of two levels (Robot A or Robot B).\nThose individuals assigned Robot A(lpha) were denoted as belonging to ‘Group A’.\nThose individuals assigned Robot B(eta) were denoted as belonging to ‘Group B’.\nNote. The groups are mutually exclusively - i.e., a participant was assigned to either Group A/Robot A or Group B/Robot B, not both.\n\n\n\nRobot A(lpha)\n\n\n\n\n\nRobot B(eta)\n\n\n\n\n\n2.3 Dependent (a.k.a. Outcome) variables\nNext, the researchers required some outcome measures to evaluate whether indeed the students had responded more positively toward one robot prototype over another.\nThe research team settled on two dependent variables (or outcome measures). These measured student stats competence (something important!) and attitudes towards teaching support (something also very important!).\nDV1: Student stats competence was measured by a Psyc214 test score; ranging between 40-100.\nDV2: Student attitudes towards the robot lab assistant was assessed with a likert scale response between 1-7; with 1 = ‘strongly dislike’ and 7 = ‘strongly like’.\n\n\n\n2.4 Predictions of the research team\nThe research team had their own expectations for which robot would be best received by students. Specifically, they predicted that:\n\nIndividuals assigned Robot B(eta) would score significantly higher in their Psyc214 tests (Hypothesis 1)\nRobot B(eta) would be significantly more liked than Robot(A)lpha (Hypothesis 2)\n\nThe researchers expected that this difference would be explained by Robot B(eta)’s closer resemblance to other human beings. Indeed, prior to the study, independent raters had reliably agreed that Robot B(eta) resembled a human being significantly more than Robot A(lpha).\n\n\n\n2.5 Refamiliarising with some basic features\nNow that you have opened the dataset and are aware of what the different variables for this study are, it’s time to explore.\nIn this first refresher session, we will be refamiliarising ourselves with some of the key functions and features learned in last year stats classes.\nEarlier, we loaded the package dplyr, which is readily accessible from the tidyverse collection. This package allows us to use some nifty functions such as:\n\npipes %&gt;%\n\narrange()\nsummary()\nsd()\nIQR, etc.\n\n\n\n\n2.6 Examining and ordering data\nNow that we have our pieces in play, let’s take a look at the data.\nWe first want to get a brief overview of what the data set looks like. To do this, we use the use the head() command, which displays the first rows present in the data frame.\nTo view these first rows, please apply the following code:\n\n  head(lab1_data)\n\n  ID Group Likeability Score\n1  1     A           3    55\n2  2     A           2    72\n3  3     A           2    45\n4  4     A           3    50\n5  5     A           2    45\n6  6     A           5    63\n\n\nThis shows us the first 6 rows. Note, while the default is six, you can also specify the number of rows as follows - please try it out.\n\n  head(lab1_data, n=12)\n\nGreat. We can now see the first 12 rows of data. One thing that is noticeable, however, is that the data are ordered by participant ‘ID’. Say we want to get an overview of our data where we can see data ordered by the from lowest Psyc214 test score to highest. We can arrange data this way using the arrange() command, were we include ‘Score’ as the variable of interest. Please try the following:\n\n  lab1_data %&gt;% arrange(Score)\n\n     ID Group Likeability Score\n1    33     A           2    40\n2    60     A           1    40\n3   165     B           5    40\n4    81     A           3    41\n5    46     A           3    42\n6    52     A           2    42\n7    18     A           4    43\n8    44     A           3    43\n9    95     A           3    43\n10   50     A           3    44\n11   53     A           1    44\n12   87     A           3    44\n13    3     A           2    45\n14    5     A           2    45\n15    7     A           4    45\n16   26     A           1    45\n17   28     A           4    46\n18  126     B           5    46\n19   39     A           4    47\n20   61     A           1    47\n21   66     A           4    47\n22   85     A           3    47\n23  133     B           3    47\n24   51     A           3    48\n25   57     A           2    48\n26   86     A           3    48\n27   96     A           4    48\n28   12     A           6    49\n29   40     A           3    49\n30  144     B           4    49\n31  162     B           4    49\n32  167     B           5    49\n33  172     B           4    49\n34  192     B           5    49\n35    4     A           3    50\n36   43     A           2    50\n37   79     A           2    50\n38  173     B           3    50\n39  191     B           5    50\n40   71     A           7    51\n41   91     A           4    51\n42  140     B           5    51\n43  147     B           6    51\n44  166     B           5    51\n45  170     B           6    51\n46  200     B           5    51\n47   24     A           3    52\n48  121     B           6    52\n49  176     B           4    52\n50   17     A           5    53\n51   38     A           4    53\n52   42     A           2    53\n53   62     A           2    53\n54   97     A           4    53\n55  105     B           4    53\n56  110     B           6    53\n57  131     B           7    53\n58   13     A           1    54\n59   64     A           3    54\n60   74     A           2    54\n61  101     B           4    54\n62  194     B           4    54\n63    1     A           3    55\n64   31     A           3    55\n65   37     A           1    55\n66   49     A           2    55\n67   70     A           3    55\n68  111     B           5    55\n69  128     B           6    55\n70  155     B           4    55\n71  163     B           3    55\n72  174     B           4    55\n73  177     B           5    55\n74   72     A           1    56\n75   82     A           3    56\n76  104     B           3    56\n77  136     B           6    56\n78  151     B           3    56\n79  184     B           5    56\n80   14     A           3    57\n81   15     A           2    57\n82   20     A           3    57\n83   45     A           3    57\n84  106     B           5    57\n85  135     B           4    57\n86  139     B           4    57\n87  149     B           4    57\n88  153     B           4    57\n89  168     B           5    57\n90   23     A           3    58\n91   35     A           4    58\n92   36     A           3    58\n93   48     A           2    58\n94   76     A           4    58\n95  109     B           4    58\n96  113     B           7    58\n97  180     B           5    58\n98    9     A           4    59\n99   54     A           2    59\n100  55     A           3    59\n101 102     B           3    59\n102 118     B           3    59\n103 125     B           5    59\n104 150     B           4    59\n105 186     B           5    59\n106 190     B           6    59\n107  16     A           3    60\n108  69     A           2    60\n109  83     A           3    60\n110  90     A           2    60\n111 115     B           5    60\n112 120     B           4    60\n113 143     B           4    60\n114 185     B           4    60\n115 187     B           5    60\n116 193     B           4    60\n117  22     A           2    61\n118  59     A           3    61\n119  92     A           4    61\n120 161     B           5    61\n121  21     A           3    62\n122  27     A           2    62\n123  32     A           2    62\n124  75     A           3    62\n125  77     A           3    62\n126  89     A           4    62\n127 148     B           5    62\n128   6     A           5    63\n129  80     A           4    63\n130  99     A           3    63\n131 100     A           4    63\n132 158     B           7    63\n133  19     A           2    64\n134  25     A           4    64\n135  41     A           3    64\n136  56     A           2    64\n137  68     A           3    64\n138  98     A           4    64\n139 122     B           5    64\n140 152     B           4    64\n141 181     B           4    64\n142 188     B           4    64\n143  58     A           3    65\n144  67     A           0    65\n145  78     A           3    65\n146  84     A           4    65\n147  88     A           4    65\n148  93     A           4    66\n149 119     B           5    66\n150 137     B           5    66\n151 196     B           5    66\n152  29     A           3    67\n153  34     A           4    67\n154  63     A           3    67\n155 130     B           2    67\n156 171     B           3    67\n157 199     B           6    67\n158  73     A           3    68\n159 103     B           3    68\n160 114     B           6    68\n161 123     B           5    68\n162 195     B           5    68\n163   8     A           4    69\n164 116     B           5    69\n165 129     B           3    69\n166 164     B           4    69\n167 198     B           3    69\n168  47     A           3    70\n169  65     A           1    70\n170 112     B           5    70\n171 141     B           5    70\n172 154     B           3    70\n173 157     B           4    70\n174 132     B           6    71\n175 146     B           4    71\n176 179     B           2    71\n177 182     B           5    71\n178   2     A           2    72\n179  11     A           3    72\n180 169     B           4    72\n181 197     B           5    72\n182  94     A           3    73\n183 107     B           3    73\n184 127     B           5    74\n185 159     B           4    75\n186 142     B           5    76\n187  30     A           6    78\n188 160     B           6    78\n189 117     B           5    80\n190 189     B           4    80\n191 108     B           3    81\n192 124     B           4    81\n193 138     B           5    81\n194 145     B           4    82\n195  10     A           1    83\n196 156     B           4    83\n197 183     B           4    85\n198 134     B           5    86\n199 175     B           5    88\n200 178     B           5    88\n\n\nYou will note that the command outputs 100+ data rows showing the values of over 100 of our participants.\nThis can be quite a whooper!\nIf we want to make this output more manageable we could always add an additional &gt;%&gt; pipe command which combines both arrange and head.\nFor example the code below, combining arrange() and head() allows us order the data based on lowest score and then to display only the top 10 results.\n\n  lab1_data %&gt;% arrange(Score) %&gt;% head(n=10)\n\n    ID Group Likeability Score\n1   33     A           2    40\n2   60     A           1    40\n3  165     B           5    40\n4   81     A           3    41\n5   46     A           3    42\n6   52     A           2    42\n7   18     A           4    43\n8   44     A           3    43\n9   95     A           3    43\n10  50     A           3    44\n\n\nPhew. This is more manageable.\nThis table provides some interesting information. We can see that the lowest scores were 40, which was the bottom cutoff point of our measurement scale.\nA quick eyeballing of the data rows suggests the lowest 10 test scores predominately have participants who were assigned to Group A(lpha). In fact, 9 out of 10 of the lowest scores belonged to individuals from Group A.\nIt would also be prudent to check the top 10 highest scores and to get a feel for what the top marks were like. We can again order our data using the arrange() function. This time however, we add the term arrange(desc()) to show that we are asking R for descending ordering.\nPlease try the following command to examine the top scorers\n\n  lab1_data %&gt;% arrange(desc(Score))\n\nYou will note in your own output, that the scores are once again showing over 100 values!\nTo make this more manageable, please try writing your own code combining the arrange(desc()) command, a pipe %&gt;% and the head(n = x) command. If stuck, refer to the command we used higher up.\n\n#ANSWER CODE\nlab1_data %&gt;% arrange(desc(Score)) %&gt;% head(n=10)\n\n    ID Group Likeability Score\n1  175     B           5    88\n2  178     B           5    88\n3  134     B           5    86\n4  183     B           4    85\n5   10     A           1    83\n6  156     B           4    83\n7  145     B           4    82\n8  108     B           3    81\n9  124     B           4    81\n10 138     B           5    81\n\n\nYou now should have an output of descending scores which are more manageable.\n\n\n    ID Group Likeability Score\n1  175     B           5    88\n2  178     B           5    88\n3  134     B           5    86\n4  183     B           4    85\n5   10     A           1    83\n6  156     B           4    83\n7  145     B           4    82\n8  108     B           3    81\n9  124     B           4    81\n10 138     B           5    81\n\n\nTry eyeballing these scores and group IDs to see whether the higher scores tend to be associated with one group over another. Any thoughts?\n\n\n\n2.7 Assessing the descriptives\nAs of yet, we have not examined the basic descriptive statistics of our data, so let’s take a look now.\nWe can compute the minimum and maximum values, 1st and 3rd quartiles, median and mean all at once using the summary() command:\n\nsummary(lab1_data)\n\n       ID            Group            Likeability        Score      \n Min.   :  1.00   Length:200         Min.   :0.000   Min.   :40.00  \n 1st Qu.: 50.75   Class :character   1st Qu.:3.000   1st Qu.:53.00  \n Median :100.50   Mode  :character   Median :4.000   Median :59.00  \n Mean   :100.50                      Mean   :3.725   Mean   :59.69  \n 3rd Qu.:150.25                      3rd Qu.:5.000   3rd Qu.:66.00  \n Max.   :200.00                      Max.   :7.000   Max.   :88.00  \n\n\nThere are still a number of descriptives which may be interesting, but that the summary() function does not provide, however.\nFor example, we may be interested in … (remember from last lecture ;) …:\n\nthe range of our data points (i.e., the difference between the lowest and highest values)\nthe interquartile range of data (i.e., the difference between the first and third quartiles)\nthe standard deviation (i.e., the dispersion of a dataset relative to its mean, that being the square root of the variance)\n\nThese can all be calculated relatively easily with the following commands:\n\nthe range of our data points: range()\nthe interquartile range: IQR()\nthe standard deviation: sd()\n\n\nrange(lab1_data$Score)\n\n[1] 40 88\n\nIQR(lab1_data$Score)\n\n[1] 13\n\nsd(lab1_data$Score)\n\n[1] 10.29689\n\n\nNote that this only provides the information for the ‘Scores’ dependent variable - I.e., DV1.\nWe do not know what the respective values are for the ‘Likeability’ dependent variable (DV2).\nIf we want to examine multiple variables simultaneously, we can use the sapply() function, which can take multiple lists/vector/data frames as inputs within a single command. Nifty.\nHere, we need to specify which columns we would like to assess. In our data the ‘Scores’ dependent variable (DV1) and ‘Likeability’ dependent variable (DV2) are columns 4 and 3 of the datasheet, respectively.\nLet’s try in with the range.\n\nsapply(lab1_data[, 4:3], range)\n\n     Score Likeability\n[1,]    40           0\n[2,]    88           7\n\n\nHere we see that the range in Psyc214 test scores (i.e., the min and max scores in the cohort of all participants) ranged between 40 and 88.\nAlso, the min and max ratings for the likeability scores were between 0 and 7, respectively.\nNow, please try to apply the code above yourself, but change ‘range’ with the standard deviation ‘sd’ function to get the standard deviation for both the score and likeability variables simultaneously.\n\n#ANSWER CODE\nsapply(lab1_data[, 4:3], sd)\n\n      Score Likeability \n  10.296888    1.344774 \n\n\nIf done correctly, you should received the following output:\n\n\n      Score Likeability \n  10.296888    1.344774 \n\n\n\n\n\n2.8 Descriptives by group condition\nNow that we have some information regarding the overall data, we can rejoice.\nWell only briefly…\nAlthough this information is useful, it gives information regarding all data points and does not generate these values based on the two assigned independent groups - i.e., the two levels (Group Robot A, Group Robot B) of our single factor (Robot Assignment).\nAlthough there are work arounds for this, simpler functions are available in the rstatix() package (previously loaded).\nUsing the package’s group_by() command and pipes %&gt;% we can ask R to separate our data based on the group assigned. Note, here the name of the grouping variable for our dataset is ‘Group’.\nLet’s ask for the mean for our two groups with the following command. First we tell R we are using the lab1_data. Then we say within this data set we want to separate the data by the ‘Group’ variable. Finally we ask for summary stats for the DV1 the Score. We will ask only for the mean.\n\n  lab1_data %&gt;%\n  group_by(Group) %&gt;%\n  get_summary_stats(Score, type = \"mean\")\n\n# A tibble: 2 × 4\n  Group variable     n  mean\n  &lt;chr&gt; &lt;fct&gt;    &lt;dbl&gt; &lt;dbl&gt;\n1 A     Score      100  56.6\n2 B     Score      100  62.8\n\n\nGreat. Now let’s do the same again, but this time we will ask for summary statistics for both the Score (DV1) and Likeability (DV2) variables. Furthermore, let’s also ask for the sd, min, max and iqr.\n\n  lab1_data %&gt;%\n  group_by(Group) %&gt;%\n  get_summary_stats(Score, Likeability, show = c(\"mean\", \"sd\", \"min\", \"max\", \"iqr\"))\n\n# A tibble: 4 × 8\n  Group variable        n  mean    sd   min   max   iqr\n  &lt;chr&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;\n1 A     Score         100 56.6   9.05    40    83    14\n2 A     Likeability   100  2.94  1.15     0     7     2\n3 B     Score         100 62.8  10.6     40    88    15\n4 B     Likeability   100  4.51  1.03     2     7     1\n\n\nOk, perfect. This now tells us a lot of information. For example, we can see that the mean test scores for Group A and B are 56.6 and 62.8 respectively. On reflection, and considering the importance of getting a nice score, these means do seem quite different from one another."
  },
  {
    "objectID": "PSYC214/Week1.html#and-more",
    "href": "PSYC214/Week1.html#and-more",
    "title": "Statistics for Psychologists",
    "section": "3. And more",
    "text": "3. And more\n\n3.1 Visualising data\nAs a next step, let’s plot the data and see the distribution of data points in visual form. To do this, we’ve already loaded the excellent data visualization package ggpubr.\nTo plot the data we use the ggboxplot() command. First we are required to say what dataset we’ll be using, which is again ‘lab1_data’. Next we type ‘main=’ to specify the title for the plot. Let’s call the plot “Box plot of test scores by robot condition”. We then need to specify what data we would like to use for each axis. For the x-axis we would like to plot the different groups, so let’s add x = “Group”. Our y-axis will be the “Score”. We can colour the groups also and add shapes to data points. Finally, let’s give the x-axis and y-axis each a descriptive label. “Robot group condition” and “Psyc214 test score” seem appropriate here.\n\n  # Box plot of test scores by Robot Condition\nggboxplot(lab1_data,main=\"Box plot of test scores by robot condition\", \n               x = \"Group\", y = \"Score\", #variables for axes\n                color = \"Group\", palette =c(\"#999999\", \"#333333\"), #colour of data\n                add = \"jitter\", shape = \"Group\", # shape of data\n               xlab=\"Robot group condition\", ylab=\"Psyc214 test score\") #labels for axes\n\n\n\n\nObserving the spread of data points for Group A and B, respectively, we can see trends in which Group B tends to have more data points clustered higher on the test score axis than Group A. Group A has more data points clustered lower on the test score axis than Group B and these distributions are reflected by the boxplots. Again it seems we have a signficant difference, but we will need a statistical test to be certain.\n\n\n\n3.2 Independent 2-group T-test\nTo get to the bottom of this we need to perform an independent 2-group t-test. This inferential statistical test is appropriate when you want to examine whether there are any statistically significant differences between the means of two unrelated groups. This can be run using the t_test() function, where ‘Score’ is our y (dependent variable) and ‘Group’ is our predictive factor x (independent variable).\n\n# independent 2-group t-test\n  #t_test(Score ~ Group) # where y is numeric Score and x is a binary factor \n# Compute t-test\n t.test(formula = Score ~ Group, pool.sd = FALSE, var.equal = TRUE, data = lab1_data)\n\n\n    Two Sample t-test\n\ndata:  Score by Group\nt = -4.4152, df = 198, p-value = 1.658e-05\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -8.896848 -3.403152\nsample estimates:\nmean in group A mean in group B \n          56.61           62.76 \n\n\nThe output provides the t-value and it’s 95% confidence interval - i.e., we can be 95% certain that the true value of t falls within the the lower and upper confidence intervals. We also get the means of our groups, the degrees of freedom for our t-test and the p-value. This p-value is significantly lower than 0.05 meaning that we reject the null hypothesis - i.e., we accept that the true difference is not roughly equal to zero. As such, there is a statistical difference in Psyc214 assessment scores between the two groups. What we are yet to discover is whether this represents a small, moderate or large effect.\nTo calculate the effect size, where 0.2 = small effect, 0.5 = moderate effect, 0.8 = large effect (Cohen, 1988), we use the ‘cohens_d()’ function.\n\n  # Cohen's d effect size calculation\ncohens_d(Score ~ Group,  # Formula\n       data = lab1_data) # Dataframe containing the variables\n\n# A tibble: 1 × 7\n  .y.   group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 Score A      B       -0.624   100   100 moderate \n\n\nThis output shows us that we have a moderate negative effect of -0.62 ; note. it is negative because of the order of our factors - i.e., Group A (which is presented first in our equation) has a lower set of scores than Group B. As you typically report the effect size as a value between 0-1 we need to treat the result as an absolute value, meaning we ignore the minus symbol when reporting our results.\n\n\n\n3.3 Reporting the results of the independent 2-group T-test in APA format\nAll results should be written up in accordance with the American Psychological Association’s (APA) guidance. This ensures that your results are easy to interpret and that all the relevant information is present for those wishing to review/replicate your work.\nThe current results can be reported as following: “An independent 2-group T-test was carried out to examine whether there were statistical differences in the Psyc214 assessment scores between those students assigned Robot A(lpha) and those assigned Robot B(eta). This test was found to be statistically significant, t(198)= -4.42, 95% CI [-8.90, -3.40], p &lt; .001, d = 0.62. This effect size represents a moderate effect (Cohen, 1988). These results indicate that individuals assigned Robot A(lpha) (M = 56.61, SD = 9.05) typically received lower Psyc214 assessment scores than those individuals assigned Robot B(eta) (M = 62.76, SD = 10.60)”"
  },
  {
    "objectID": "PSYC214/Week1.html#further-tasks",
    "href": "PSYC214/Week1.html#further-tasks",
    "title": "Statistics for Psychologists",
    "section": "4. Further tasks",
    "text": "4. Further tasks\n\nrepeat the steps in ‘2.8 Descriptives by group condition’, this time examining the summary statistics for our second dependent variable - ‘Likeability’.\n\n\n  #ANSWER CODE\n  lab1_data %&gt;%\n  group_by(Group) %&gt;%\n  get_summary_stats(Likeability, type = \"mean\")\n\n# A tibble: 2 × 4\n  Group variable        n  mean\n  &lt;chr&gt; &lt;fct&gt;       &lt;dbl&gt; &lt;dbl&gt;\n1 A     Likeability   100  2.94\n2 B     Likeability   100  4.51\n\n\n\nrepeat the steps in ‘3.1 Visualising data’, this time creating box plots for our second dependent variable - ‘Likeability’. Rename the title and axes to something more suitable and perhaps even change the colour of data points. Describe the trends in data to a classmate or your inner self.\n\n\n#ANSWER CODE\n  # Box plot of likeability scores by Robot Condition\nggboxplot(lab1_data,main=\"Box plot of test scores by robot condition\", \n               x = \"Group\", y = \"Likeability\", #variables for axes\n                color = \"Group\", palette =c(\"#999999\", \"#333333\"), #colour of data\n                add = \"jitter\", shape = \"Group\", # shape of data\n               xlab=\"Robot group condition\", ylab=\"Likeability scores\") #labels for axes\n\n\n\n\n\ncompute an independent t-test (steps 3.2) to examine whether there are any statistical differences between the means of our two groups in their Likeability ratings. Write up the results in APA style.\n\n\n#ANSWER CODE\n# independent 2-group t-test\n  #t_test(Score ~ Group) # where y is numeric Score and x is a binary factor \n# Compute t-test\n t.test(formula = Likeability ~ Group, pool.sd = FALSE, var.equal = TRUE, data = lab1_data)\n\n\n    Two Sample t-test\n\ndata:  Likeability by Group\nt = -10.155, df = 198, p-value &lt; 2.2e-16\nalternative hypothesis: true difference in means between group A and group B is not equal to 0\n95 percent confidence interval:\n -1.874879 -1.265121\nsample estimates:\nmean in group A mean in group B \n           2.94            4.51 \n\n  # Cohen's d effect size calculation\ncohens_d(Likeability ~ Group,  # Formula\n       data = lab1_data) # Dataframe containing the variables\n\n# A tibble: 1 × 7\n  .y.         group1 group2 effsize    n1    n2 magnitude\n* &lt;chr&gt;       &lt;chr&gt;  &lt;chr&gt;    &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;ord&gt;    \n1 Likeability A      B        -1.44   100   100 large    \n\n\n\nbefore you finish, make sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\n\nPlease end your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\n\n\nEnding session on R Studio\n\n\n\n…\n\nNow breathe! You’ve rocked it!!!\n\n\n\nTop work!"
  },
  {
    "objectID": "PSYC214/Week3.html",
    "href": "PSYC214/Week3.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC214/Week3.html#parameter-eta2-95-ci",
    "href": "PSYC214/Week3.html#parameter-eta2-95-ci",
    "title": "Statistics for Psychologists",
    "section": "Parameter | Eta2 | 95% CI",
    "text": "Parameter | Eta2 | 95% CI\nGroup | 0.10 | [0.04, 1.00]\n\nOne-sided CIs: upper bound fixed at [1.00].\n\n:::\n:::\n\n:::\n\nThis has provided us with a whole bunch of useful information! \nIt shows us the between group variability value (do you remember, the numerator value in our F ratio equation from lecture 2?), which R has named the Group Mean Square (a value here of 611.30). \n\nIt also provides the within group variability value - aka the error term (do you remember, the denominator in our F ratio equation from lecture 2), which R has named the Residuals mean square (a value here of 48.80)\n\nIt provides the crucial F-ratio statistic, here a value of 12.52. \n\nThe output also shows an Eta-squared (η2) effect size of 0.10. As a rule of thumb, η2 = 0.01 indicates a small effect; η2 = 0.06 indicates a medium effect; η2 = 0.14 indicates a large effect. Our eta-squared (η2) represents a medium effect - i.e., our experimental manipulation had a moderate effect on student Psyc214 test scores. \n\n******\n#### 3.2 Reporting the results of the one-factor between-participants ANOVA in APA format\n\nAll results should be written up in accordance with the American Psychological Association's (APA) guidance. This ensures that your results are easy to interpret and that all the relevant information is present for those wishing to review/replicate your work.\n\nThe current results can be reported as following:\n\"A one-factor between-participants ANOVA revealed that Psyc214 scores were significantly different between our robot groups (Robot A *M* = 58.10, *SD* = 6.45; Robot B *M* = 60.40, *SD* = 7.27; Robot O *M* = 63.60, *SD* = 7.22), *F*(2,237) = 12.52, *p* &lt; .001, η2 = 0.10, 95% CI[0.04, 1.00].\n\n  ![Hold up!](https://images.unsplash.com/photo-1542707309-4f9de5fd1d9c?ixid=MnwxMjA3fDB8MHxwaG90by1wYWdlfHx8fGVufDB8fHx8&ixlib=rb-1.2.1&auto=format&fit=crop&w=987&q=80)\n\n*Hold up!!! While the ANOVA tells us that there are differences between groups, it doesn't tell us specifically which groups differ from one another. For example do Group A and B statistically differ? A and O? B and O? The only way to know this definitively is to carry out posthoc/planned contrast tests**\n\n******\n#### 3.3 Decisions, decisions, decisions...\n\nJust to recall, from lecture 3, following an ANOVA, we need to further scrutinize the differences between our separate groups.\n\nThis can be done in one of three ways. \n1) T-tests with no adjustments (let's ignore this one, because it is just bonkers - please refer to lecture 3 if unsure why)\n2) Planned comparisons of specific relationships with adjusted p-values (e.g., Bonferroni corrections)\n3) Post-hoc tests (e.g., Tukey HSD) which are interested in differences between *all* possible combinations of groups and also have conservative *p* values.\n\nTo recap, as a rule of thumb, planned comparisons are always preferable. But these entail that you have a pre-specified idea of which groups should differ.\n\n**We're not going to make this decision for you. Please choose whether you would like to run a:**\\\ni) planned comparison test in which we compare one or two pairs of specific groups of interest (number of levels minus 1 - i.e., we can run up to two comparison tests).\\\nii) post hoc tests where we are interested in statistical differences between all possible combinations of groups.\\\n\n**Take your pick now. If you're an advocate for the 'Planned Comparisons' please skip to section 3.4 and ignore 3.5 for now. If you're an advocate of 'Post hoc tests' please skip to section 3.5 and ignore 3.4 for now**\n\n******\n#### 3.4 Planned comparisons\n\nOh hello, and welcome. I can see you've gone for the planned comparisons  - i.e., you have an idea which specific groups should differ and want to see that through.\n\nLet's run a between-groups pairwise t-test with Bonferroni correction. Note, to get an adjusted *p* value with Bonferroni corrections for multiple comparisons, we need to be sure that we include *p.adjust.method = \"bonferroni*. If we do not add this line of code then we will just get regular p-values without the conservative adjustment.\n\nLet's only compare the scores for Omega versus Alpha and Omega versus Beta:\n\n\n::: {.cell}\n\n```{.r .cell-code}\nlab3_data %&gt;% \n  # Execute independent-samples t-tests - remember to set pool.sd = FALSE, and include Bonferroni corrections for multiple comparions\n  pairwise_t_test(Score ~ Group, pool.sd = FALSE, var.equal = TRUE, p.adjust.method = \"bonferroni\",\n    # Just generate the two comparisons of interest: e.g., Omega vs. Alpha    # and Omega vs. Beta    \n  comparisons = list(c(\"O\",\"A\"), c(\"O\",\"A\")))\n\n# A tibble: 2 × 10\n  .y.   group1 group2    n1    n2 statistic    df         p   p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 Score A      O         80    80     -5.08   158   1.04e-6 2.08e-6 ****        \n2 Score A      O         80    80     -5.08   158   1.04e-6 2.08e-6 ****        \n\n:::\nOh blast, there’s an error in our code as we seem to have compared Omega versus Alpha twice! Try to adjust your code to ensure you also compare Omega versus Alpha.\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\nlab3_data %&gt;% \n  # Execute independent-samples t-tests - remember to set pool.sd = FALSE, and include Bonferroni corrections for multiple comparions\n  pairwise_t_test(Score ~ Group, pool.sd = FALSE, var.equal = TRUE, p.adjust.method = \"bonferroni\",\n    # Just generate the two comparisons of interest: e.g., Omega vs. Alpha    # and Omega vs. Beta    \n  comparisons = list(c(\"O\",\"A\"), c(\"O\",\"B\")))\n\n# A tibble: 2 × 10\n  .y.   group1 group2    n1    n2 statistic    df         p   p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 Score A      O         80    80     -5.08   158   1.04e-6 2.08e-6 ****        \n2 Score B      O         80    80     -2.83   158   5   e-3 1.1 e-2 *           \n\n\nThat’s better. If we now scrutinize the output we can see that the Omega group does indeed statistically differ from the other two groups.\n\n\n# A tibble: 2 × 10\n  .y.   group1 group2    n1    n2 statistic    df         p   p.adj p.adj.signif\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;     &lt;dbl&gt;   &lt;dbl&gt; &lt;chr&gt;       \n1 Score A      O         80    80     -5.08   158   1.04e-6 2.08e-6 ****        \n2 Score B      O         80    80     -2.83   158   5   e-3 1.1 e-2 *           \n\n\n\n\n\nWe would now add this information in our APA write up. Make sure when you interpret and write up the output that you report the adjusted p-value (i.e., ‘p.adj’ in the output), as opposed to the non-adjusted and dicey p value (i.e., ‘p’ in the output).\nWe also want to generate some effect sizes for these pairwise comparisons. This can be done using the rstatix’s cohens_d() function.\n\n# Cohen's d effect size for O vs. A\ncohens_d(lab3_data$Score[lab3_data$Group == \"O\"],lab3_data$Score[lab3_data$Group == \"A\"])\n\nCohen's d |       95% CI\n------------------------\n0.80      | [0.48, 1.12]\n\n- Estimated using pooled SD.\n\n\nFantastic. This tells us that our pairwise comparison for the O and A groups has a Cohen’s D effect size of 0.80.\nNow try your own code out to generate an effect size for groups O and B.\n\n#ANSWER CODE\n# Cohen's d effect size for O vs. B\ncohens_d(lab3_data$Score[lab3_data$Group == \"O\"],lab3_data$Score[lab3_data$Group == \"B\"])\n\nCohen's d |       95% CI\n------------------------\n0.45      | [0.13, 0.76]\n\n- Estimated using pooled SD.\n\n\n\n\nCohen's d |       95% CI\n------------------------\n0.45      | [0.13, 0.76]\n\n- Estimated using pooled SD.\n\n\nThe current results can be reported as following: “A one-factor between-participants ANOVA revealed that Psyc214 scores were significantly different between our robot groups (Robot A, M = 58.10, SD = 6.45; Robot B, M = 60.40, SD = 7.27; Robot O, M = 63.60, SD = 7.22), F(2,237) = 12.52, p &lt; .001, η2 = 0.10, 95% CI[0.04, 1.00]. Planned comparisons with Bonferroni adjustments, found that students who were assigned Robot A and B had significantly lower Psyc214 scores than those assigned Robot O, t(156) = -5.08, p &lt; .001, d = .80 and t(158) = -2.83, p = 0.011, d = .45, respectively.”\nNote, we didn’t report all combinations. Given our hypotheses, we were interested in Omega. Therefore, we did not need to look at the combination of Alpha and Beta.\n\n\n3.5 Post hoc tests\nOh hello, and welcome. I can see that you’re more of a post hoc kind of individual - i.e., you haven’t got any specific idea of which groups should differ or are just plain nosy and would like to examine all possible combinations of group differences.\nWe will run a Tukey-Kramer HSD test. This was covered in Lecture 3 - but fortunately, we will not need to get out our Tukey Studentized Range Statistic table and line up the values by hand - R will do this all for us (lovely, generous R)\nOk, so let’s enter the posthoc command:\n\nlab3_data %&gt;% tukey_hsd(Score ~ Group) # where tukey_hsd is our post-hoc test of choice\n\n# A tibble: 3 × 9\n  term  group1 group2 null.value estimate conf.low conf.high      p.adj\n* &lt;chr&gt; &lt;chr&gt;  &lt;chr&gt;       &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;     &lt;dbl&gt;      &lt;dbl&gt;\n1 Group A      B               0     2.26   -0.343      4.87 0.103     \n2 Group A      O               0     5.50    2.89       8.11 0.00000369\n3 Group B      O               0     3.24    0.632      5.84 0.0103    \n# ℹ 1 more variable: p.adj.signif &lt;chr&gt;\n\n\nPlease look at the output. It compares groups A and B, A and O, B and O. The p-values are adjusted to be conservative, to try and combat the vulnerability of familywise type I error:\n\nA and B, p = 0.103\nA and O, p = 0.00000369\nB and O, p = 0.0103\n\nThe posthoc results show that there is no statistical difference in scores between groups A and B. A and O are highly significantly different from one another, while B and O are statistically significantly different at a of p = 0.01.\nWe can generate effect sizes for each combination using the cohens_d() function from rstatix.\nThe following code produces the first Cohen’s d effect size for the comparison of group A and B:\n\n# Cohen's d effect size for A vs. B\ncohens_d(lab3_data$Score[lab3_data$Group == \"A\"],lab3_data$Score[lab3_data$Group == \"B\"])\n\nCohen's d |         95% CI\n--------------------------\n-0.33     | [-0.64, -0.02]\n\n- Estimated using pooled SD.\n\n\nNow try your own code to generate a Cohen’s d statistic for A and O; B and O:\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\n# Cohen's d effect size for A vs. O\ncohens_d(lab3_data$Score[lab3_data$Group == \"A\"],lab3_data$Score[lab3_data$Group == \"O\"])\n\nCohen's d |         95% CI\n--------------------------\n-0.80     | [-1.12, -0.48]\n\n- Estimated using pooled SD.\n\n\n\n#ANSWER CODE\n# Cohen's d effect size for B vs. O\ncohens_d(lab3_data$Score[lab3_data$Group == \"B\"],lab3_data$Score[lab3_data$Group == \"O\"])\n\nCohen's d |         95% CI\n--------------------------\n-0.45     | [-0.76, -0.13]\n\n- Estimated using pooled SD.\n\n\nIf done correctly, you should have the following:\n\n\nCohen's d |         95% CI\n--------------------------\n-0.80     | [-1.12, -0.48]\n\n- Estimated using pooled SD.\n\n\n\n\nCohen's d |         95% CI\n--------------------------\n-0.45     | [-0.76, -0.13]\n\n- Estimated using pooled SD.\n\n\n\n\n\nThe current results can be reported as following: “A one-factor between-participants ANOVA revealed that Psyc214 scores were significantly different between our robot groups (Robot A, M = 58.10, SD = 6.45; Robot B, M = 60.40, SD = 7.27; Robot O, M = 63.60, SD = 7.22), F(2,237) = 12.52, p &lt; .001, η2 = 0.10, 95% CI[0.04, 1.00]. Tukey-Kramer HSD posthoc tests showed that the difference in students’ test scores (5.50) between participants assigned Robot A and Robot O was statistically significant (adjusted p &lt; 0.001, d = -0.80). The difference in students’ test scores (3.24) between participants assigned Robot B and Robot O was statistically significant (adjusted p = .01, d = -0.45), but there was no statistical group difference between Robot A and B (2.26, adjusted p = 0.10, d = -0.33).”\nNow that you’ve finished this. Please go on to ‘4 Further tasks’.\n\n\n\n4 Further tasks\n\nWell done for completing what has been the most difficult lab week thus far. Regardless of your allegiance - be it planned comparisons or post hoc tests - it is important you are fluent with both. Therefore, please navigate up and run the alternative set of pairwise comparisons to the one you originally chose - in short, if you haven’t had the chance to run planned comparisons, please do it now. If you haven’t yet run post-hoc tests, give them a whirl.\nWe didn’t end up running tests of homogeneity of variance for the ‘Likeability’ variable - remember, we decided not to analyze this data without either a transformation or a change to a non-parametric alternative. Regardless, just for practice, why not run a Levene’s Test - (see 3.1.4 Assumption of Homogeniety of Variance). How is it looking? Talk to your self or a study buddy.\n\n\n\n\n\n\n\nAnswer\n\n\n\n\n\n\n#ANSWER CODE\nlab3_data %&gt;% levene_test(Likeability ~ Group)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2   236      1.75 0.177\n\n\n\n\n\n\nBefore you finish, make sure you save a copy of the script that you have been working on by the end of the session. This provides you with the record - the digital trace - on what you have done. And it means you can come back and repeat any of the work you have performed.\n\nPlease end your session on the RStudio server, this logs you out of the server and stops any ongoing activities and tasks you have set up, maybe in the background.\n\nPlease chill, you must be fatigued. See you next week and top work.\n\n\n\n\nTop work!"
  },
  {
    "objectID": "PSYC214/Week6.html",
    "href": "PSYC214/Week6.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC214/Week6.html#learning-objectives",
    "href": "PSYC214/Week6.html#learning-objectives",
    "title": "Statistics for Psychologists",
    "section": "Learning Objectives",
    "text": "Learning Objectives\nIn this Week’s lecture, we provided a conceptual introduction to factorial designs and interactions (deferring a discussion of the statistical mechanics of factorial ANOVA for future weeks). You learned that the outcomes of a factorial ANOVA include:\n\nMain effects: the mean differences among the levels of one factor are called the main effect of that factor. Main effects provide information about the independent effects of each factor. A two-factor study has two main effects, one for each of the two factors.\nInteraction: sometimes, one factor has a direct influence on the effect of a second factor, producing an interaction between factors. An interaction occurs whenever two factors, acting together, produce mean differences not explained by the main effects of the two factors.\nSimple main effects: the simple main effects break down the main effects into their component parts. They reveal the effect of one factor at each level of the second factor. A simple main effects analysis allows us to determine how two factors are combining to influence the dependent variable.\n\nIn factorial designs, data visualization becomes even more important than in single-factor designs. If a factorial ANOVA produces a significant interaction, then the easiest way to interpret it is by graphing the data in terms of an interaction plot and studying the simple main effects. Interaction plots are either graphed as line plots, like the ones presented in the lecture, or as bar plots. In this lab class, we will show you how to generate both kinds of plots in an appropriate format for presenting in your reports. You will also obtain experience of detecting/interpreting interactions by looking at each of the simple main effects in graphed data."
  },
  {
    "objectID": "PSYC214/Week6.html#todays-lab-activities",
    "href": "PSYC214/Week6.html#todays-lab-activities",
    "title": "Statistics for Psychologists",
    "section": "Today’s Lab Activities",
    "text": "Today’s Lab Activities\n\nA 2 \\(\\times\\) 2 factorial study\nLet’s start by revisiting the COVID-19 vaccination example presented in the lecture. In that example, a researcher is interested in identifying effective strategies for encouraging members of the general public to get vaccinated against COVID-19. The researcher decides to test an often employed intervention to reduce risky intentions or behaviours known as a fear appeal. A fear appeal is a persuasive message that attempts to induce fear in message recipients by emphasizing the potential danger and harms that will befall them if they do not adopt the messages’ recommendations. Graphic images on cigarette packages (e.g., a diseased eye, cancerous lungs, or a damaged heart) encouraging people to quit smoking are a classic example of a fear appeal. The researcher crafts a fear appeal—a short verbal message—drawing people’s attention to the various risks associated with contracting COVID-19, including long-term debilitating symptoms (so-called “long COVID”) such as lack of attention, concentration, and lethargy, as well as potential death. The researcher includes in the message a graphic image of a patient in a distressed state on a ventilator in a hospital bed. The researcher wants to know if a group of participants that receive the fear appeal express a greater intention to get vaccinated against COVID-19 than a group of participants that do not receive the fear appeal. The researcher plans to measure such intentions by asking people how likely it is that they would get vaccinated against COVID-19 on a scale ranging from 0 (Very Unlikely) to 10 (Very Likely).\nHowever, the researcher knows that for fear appeals to be effective, they typically must be accompanied by a self-efficacy message—a statement that assures message recipients that they are capable of performing the fear appeal’s recommended actions and/or that performing the recommended actions will result in desirable consequences. For example, cigarette packets as well as arousing fear using graphic images also direct smokers to resources to help them to quit smoking and highlight the benefits of doing so (the self-efficacy message component). Accordingly, the researcher creates a self-efficacy message that emphasizes to message recipients how easy it is for them to get vaccinated, how to do so, and the benefits that will be obtained once they have been immunized.\nThe researcher wants to know what independent or interactive effects the fear appeal and self-efficacy message may have on COVID-19 vaccination intentions, so she decides to run a 2 \\(\\times\\) 2 fully between-participants factorial study involving a total of N = 200 participants, with the following factors:\n\nFear: no fear appeal vs. fear appeal\nEfficacy: no efficacy message vs. efficacy message\n\nThis results in four different groups of participants. One group (N = 50) receives no fear appeal and no efficacy message. A second group (N = 50) receives the fear appeal but no efficacy message. A third group (N = 50) receives no fear appeal but does receive an efficacy message. A fourth group (N = 50) receives both the fear appeal and the efficacy message. All groups of participants then indicate their intention to vaccinate against COVID-19 using the vaccination intention measure described above. The four conditions are summarized in the table below.\n![}(images/DataMatrix.png){width=75%}\nFirst things first, let’s load the data set:\n\n# Import the data\ndata = read_csv(\"fearVac.csv\")\n# View the data\n(data)\n\n\n\n# A tibble: 200 × 4\n     Par Fear           Efficacy            Intention\n   &lt;dbl&gt; &lt;chr&gt;          &lt;chr&gt;                   &lt;dbl&gt;\n 1     1 No Fear Appeal No Efficacy Message         6\n 2     2 No Fear Appeal No Efficacy Message         4\n 3     3 No Fear Appeal No Efficacy Message         7\n 4     4 No Fear Appeal No Efficacy Message         8\n 5     5 No Fear Appeal No Efficacy Message         5\n 6     6 No Fear Appeal No Efficacy Message         6\n 7     7 No Fear Appeal No Efficacy Message         2\n 8     8 No Fear Appeal No Efficacy Message         6\n 9     9 No Fear Appeal No Efficacy Message         5\n10    10 No Fear Appeal No Efficacy Message         3\n# ℹ 190 more rows\n\n\nTake a look at the table summarising the data. Beneath each variable name, R tells us the variable type. There is an issue here in that the variables Fear and Efficacy are designated as character variables &lt;chr&gt;. This simply means that they are represented as strings of text. However, we want these variables to be represented as factors &lt;fct&gt;. Furthermore, we want to re-order the levels of each factor, as R will otherwise represent these in alphabetical order, which is not what we want. For Fear, we want the levels to-be-ordered “no fear appeal” –&gt; “fear appeal” and for Efficacy we want the levels to-be-ordered “no efficacy message” –&gt; “efficacy message”. The next code will convert the variables into factors and re-order the levels:\n\n# Convert Fear and Efficacy into factors and ensure \"No Fear Appeal\" is Level 1 of the Fear factor\n# and \"No Efficacy Message\" is Level 1 of the Efficacy factor\ndata$Fear     = factor(data$Fear, levels = c(\"No Fear Appeal\",\"Fear Appeal\"))\ndata$Efficacy = factor(data$Efficacy, levels = c(\"No Efficacy Message\",\"Efficacy Message\"))\n\nRight, lets take a look at our data set again:\n\n(data)\n\n# A tibble: 200 × 4\n     Par Fear           Efficacy            Intention\n   &lt;dbl&gt; &lt;fct&gt;          &lt;fct&gt;                   &lt;dbl&gt;\n 1     1 No Fear Appeal No Efficacy Message         6\n 2     2 No Fear Appeal No Efficacy Message         4\n 3     3 No Fear Appeal No Efficacy Message         7\n 4     4 No Fear Appeal No Efficacy Message         8\n 5     5 No Fear Appeal No Efficacy Message         5\n 6     6 No Fear Appeal No Efficacy Message         6\n 7     7 No Fear Appeal No Efficacy Message         2\n 8     8 No Fear Appeal No Efficacy Message         6\n 9     9 No Fear Appeal No Efficacy Message         5\n10    10 No Fear Appeal No Efficacy Message         3\n# ℹ 190 more rows\n\n\nLook at the variable type beneath the variable name. Notice that the Fear and Efficacy variables are now represented as factors  as we requested.\nYou can inspect the full data set by typing view(data) in the console. I suggest you do this now, so that you can see how the data is organized.\nNow we have inspected the raw data, let’s generate some descriptive statistics which we will store in a dataframe called descriptives. Specifically, we want the number of cases n, the mean, the standard deviation sd, the standard error se, and the confidence intervals ci.\n\ndescriptives = data %&gt;%\n  # Organise output by Fear and Efficacy\n  group_by(Fear,Efficacy) %&gt;%\n  # Get the mean, sd, se, and ci\n  get_summary_stats(Intention, show = c(\"mean\", \"sd\", \"se\", \"ci\"))\n  # Round the results to *at least* two-decimal places\n  options(digits = 4) \n  # Print the results\n  print.data.frame(descriptives)\n\n            Fear            Efficacy  variable  n mean    sd    se    ci\n1 No Fear Appeal No Efficacy Message Intention 50 5.16 2.024 0.286 0.575\n2 No Fear Appeal    Efficacy Message Intention 50 4.66 1.869 0.264 0.531\n3    Fear Appeal No Efficacy Message Intention 50 4.80 1.829 0.259 0.520\n4    Fear Appeal    Efficacy Message Intention 50 7.12 1.769 0.250 0.503"
  },
  {
    "objectID": "PSYC214/Week6.html#plotting-line-graphs",
    "href": "PSYC214/Week6.html#plotting-line-graphs",
    "title": "Statistics for Psychologists",
    "section": "Plotting line graphs",
    "text": "Plotting line graphs\nOkay, let’s get plotting the data!\nIn most statistics textbooks, the standard advice when it comes to plotting the results of a factorial experiment is that you should use a line plot, like the ones in the lecture, as this makes it easier to spot an interaction (or lack thereof) between factors. As well as plotting the means, we also want to include error bars illustrating the variability in the data. In the example next, we are using standard errors. Run the following code and it should produce the graph below. Pay attention to the comments in the code, so it is clear what each element is doing.\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n\n# jitt is used later to add a small amount of spatial jitter to our lines, data points, and error bars to prevent them from overlapping\njitt = position_dodge(0.1) \n# Using the dataframe called descriptives, place \"Fear\" on the x-axis, \"mean\" on the y-axis, and group the data \n# according to the two levels of the \"Efficacy\" factor \nggplot(data = descriptives, mapping = aes(x = Fear, y = mean, group = Efficacy)) +\n    # geom_line produces two lines: one for \"no efficacy message\" and one for \"efficacy message\"\n    geom_line(size = 1, position = jitt, aes(color = Efficacy)) + \n    # geom_point adds mean data points to our lines\n    geom_point(size = 3, position = jitt, aes(color = Efficacy)) +\n    # geom_errorbar adds error bars to our geom_points - here we are using the standard error\n    geom_errorbar(position = jitt, mapping = aes(ymin = mean - se, ymax = mean + se, color = Efficacy), size = 1, width = .05) +\n    # Here we are manually setting the colour of the lines, data points, and error bars\n    scale_color_manual(values=c(\"#355C7D\", \"#F67280\")) +\n    # Change y-axis lower and upper limits \n    ylim(0,10) +\n    # Manually set the x- and y-axis titles\n    labs(x = \"Fear\", y = \"Vaccination Intention\") +\n    # Use black and white theme for background\n    theme_bw() +\n    # The theme function allows us to set various properties of our figure manually \n    theme(panel.grid.major = element_blank(), # Removes the major gridlines\n          panel.grid.minor = element_blank(), # Removes the minor gridlines\n          legend.box.background = element_rect(colour = \"black\"), # Adds a black border around the legend\n          legend.position = \"bottom\", # Positions the legend at the bottom of the graph\n          legend.direction = \"vertical\") # Orients the legend in the vertical direction\n\nWarning: Using `size` aesthetic for lines was deprecated in ggplot2 3.4.0.\nℹ Please use `linewidth` instead.\n\n\n\n\n\nThis looks good. At this point, we might wish to save our figure so that we can export it elsewhere (e.g., a Word document or a PowerPoint presentation). Arguably, the most useful file format for this purpose is the Portable Network Graphic, or PNG for short. You can save the graph as a PNG file as follows:\n\nggsave(\"FearAppealLinePlot.png\", height = 6, width = 5, dpi = 300)\n\nAs you can see, we need to specify a name for the file, “FearAppealLinePlot.png”, that includes the .png extension at the end. We also have the option of specifying the desired height and width of the figure in inches, and the dpi, which stands for “Dots-Per-Inch” and corresponds to the resolution of the image. I recommend always setting the dpi to 300.\nLet’s take a closer look at the line graph we have produced. Remember from the lecture that a useful way to spot an interaction in a line graph is to determine whether the lines are parallel or not. If the lines are non-parallel, then this is an indication of the presence of an interaction. In our case, the lines are indeed non-parallel, suggesting that the fear and efficacy factors are combining with one another to influence the dependent variable. To break down the interaction, we can look at the simple main effects (if you have forgotten what these are, you may want to consult the lecture slides).\nFor these data, there are four simple main effects. For simplicity, we will characterise these effects using the operators =, &lt;, and &gt;. For example, condition A = condition B implies that the means for the two conditions are (roughly) the same; condition A &lt; condition B implies that the mean for condition A is less than the mean for condition B; whereas condition A &gt; condition B implies that the mean for condition A is greater than the mean for condition B. We will treat relatively small differences between means as indicating that they do not differ, with relatively large differences between means indicating that they do differ.\nFor our data, there are four simple main effects (two for the fear factor and two for the efficacy factor):\n\nThe simple main effect of fear (no fear appeal vs. fear appeal), at the no efficacy message level of the efficacy factor: no fear appeal = fear appeal\nThe simple main effect of fear (no fear appeal vs. fear appeal), at the efficacy message level of the efficacy factor: no fear appeal &lt; fear appeal\nThe simple main effect of efficacy (no efficacy message vs. efficacy message), at the no fear appeal level of the fear factor: no efficacy message = efficacy message\nThe simple main effect of efficacy (no efficacy message vs. efficacy message), at the fear appeal level of the fear factor: no efficacy message &lt; efficacy message\n\nThe simple main effects indicate that the effect of the fear factor varies according to the different levels of the efficacy factor (and vice versa), which is indicative of an interaction between the two factors.\nMore specifically, what the data show is that in the absence of a self-efficacy message, there is no effect of the fear appeal on vaccination intentions; that is, vaccination intentions do not differ between the no fear appeal and fear appeal conditions. However, in the presence of a self-efficacy message there is an effect of the fear appeal on vaccination intentions; that is, vaccination intentions are higher in the fear appeal condition than in the no fear appeal condition.\nThus, our data suggest that fear appeals are an effective strategy for increasing COVID-19 vaccination intentions, provided that they are accompanied by a self-efficacy message.\nBefore moving on, I will emphasise, as I did in the lecture, that we would not know for certain whether there “really” is an interaction until we subjected our data to a factorial ANOVA and looked at the significance value of the interaction effect.\nBack to plotting!\nI’m a big fan of using colour in graphs, but unfortunately the APA doesn’t share this sentiment. The APA guidlines state that you should not use colour in your figures, so I have been a very naughty boy by adding a touch of colour to my figure. We can easily resolve this by changing the hexademical codes for our colours to black and grey (#000000, #CCCCCC). The problem is that if we have more than two factors, and hence more than two lines in our plot, we have to use different shades of grey for at least two of the lines, and it can sometimes become difficult to visually distinguish them.\nAnother approach is to keep the colour of the lines identical (e.g., black) but vary the shape of the mean data points. In our current line graph, we used circles (or dots) as the markers for our mean data points. However, as can be seen from the figure below, there are various different shapes we can use in our plots.\n![}(images/BuiltInShapes.png){width=100%}\nNow, we will generate our line graph again but this time using different shapes (circles vs. squares) rather than different colours to distinguish the different lines for the no efficacy message and efficacy message levels of the Efficacy factor. The code below is very similar to that which we used earlier, so for clarity I have only commented on those elements that have changed.\n\njitt = position_dodge(0.1)\nggplot(data = descriptives, mapping = aes(x = Fear, y = mean, group = Efficacy)) +\n  # Here is the main bit of code that has changed from the last figure.\n  # Previously we used aes(color = Efficacy) in geom_line and geom_point to\n  # tell R to produce different colors for each level of the efficacy factor.\n  # Now we are using aes(shape = Efficacy), which tells R to use different shapes instead.\n  # Note also the custom specification of shapes below.\n  ########################################################################\n  geom_line(size = 1, position = jitt, aes(shape = Efficacy)) +\n  geom_point(size = 3, position = jitt, aes(shape = Efficacy)) +\n  # Custom values for the shapes: 15 = a black square, 16 = a black circle\n  scale_shape_manual(values = c(15, 16)) +\n  ########################################################################\n  geom_errorbar(position = jitt, mapping = aes(ymin = mean - se, ymax = mean + se, color = Efficacy), size = 1, width = .05) +\n  # We are also manually setting the color of both lines, data points, and error bars to black\n  scale_color_manual(values=c(\"#000000\", \"#000000\")) +\n  ylim(0,10) +\n  labs(x = \"Fear\", y = \"Vaccination Intention\") +\n  theme_bw() +\n  theme(panel.grid.major = element_blank(), \n        panel.grid.minor = element_blank(), \n        legend.box.background = element_rect(colour = \"black\"), \n        legend.position = \"bottom\", \n        legend.direction = \"vertical\") \n\n\n\n\nIdeally, I would like the data points to be larger in this figure. You can do this by adjusting the size argument in geom_point above—it’s currently set to a value of 3, which is quite small. However, if we increase it further it will obscure our standard error error bars. The solution is that we could plot the standard deviation in the error bars instead (but they take up a lot of visualization space) or the confidence intervals (which will be a bit bigger than the standard errors). I recommend the latter. We requested both of these descriptive statistics earlier, so if you wanted to replot the data with confidence intervals, then you would change se to ci (there are two of these values that must be changed) in the call to geom_errorbar and increase the size value in geom_point to, say, 5 or 6.\nOkay, that’s enough of line plots for now—let’s look at how to plot the same data as a bar graph instead."
  },
  {
    "objectID": "PSYC214/Week6.html#plotting-bar-graphs",
    "href": "PSYC214/Week6.html#plotting-bar-graphs",
    "title": "Statistics for Psychologists",
    "section": "Plotting bar graphs",
    "text": "Plotting bar graphs\nAlthough most textbooks will recommend that you plot the results of a factorial experiment as a line graph, in practice bar graphs are a popular option too. There are no rules regarding which type of graph you should use, so I recommend plotting the type that you find easiest to read. Personally, I find bar graphs easier to read than line graphs. Bar graphs tend to work well when you have factors with a small number of levels, but when you have one or more factors with many levels, a line graph will typically be a better option.\nLet’s plot the results of our factorial study as a bar graph. You can do so using the following code:\n\n# Using the dataframe called descriptives, place \"Fear\" on the x-axis, \"mean\" on the y-axis, and group the data \n# according to the two levels of the \"Efficacy\" factor \nggplot(data = descriptives, mapping = aes(x = Fear, y = mean, fill = Efficacy)) +\n  # geom_col is used to create our bar plots. Width and position_dodge control the degree of spatial \n  # separation of the columns\n  geom_col(width = 0.45, position = position_dodge(0.55)) +\n  # geom_errorbar adds error bars to our geom_cols - here we are using the standard error\n  geom_errorbar(mapping = aes(ymin = mean - se, ymax = mean + se),\n      size = .5, width = .1, position = position_dodge(.55)) +\n  # Here we are manually setting the colour of the columns and error bars\n  scale_fill_manual(values = c(\"#355C7D\", \"#F67280\")) +\n  # We are using scale_y_continuous this time to set the y-axis limits because we want to remove the\n  # white space that ggplot leaves between 0 and the x-axis line using expand = c(0,0). \n  scale_y_continuous(limits = c(0,10.5), expand = c(0,0)) +\n  # Manually set the x- and y-axis titles\n  labs(x = \"Fear\", y = \"Vaccination Intention\") +\n  # Use black and white theme for background\n  theme_bw() +\n  # The theme function allows us to set various properties of our figure manually \n  theme(panel.grid.major = element_blank(), # Removes the major gridlines\n        panel.grid.minor = element_blank(), # Removes the minor gridlines\n        legend.box.background = element_rect(colour = \"black\"), # Adds a black border around the legend\n        legend.position = \"bottom\", # Positions the legend at the bottom of the graph\n        legend.direction = \"vertical\") # Orients the legend in the vertical direction\n\n\n\n\nHow would you spot an interaction in a bar graph? Imagine that there are two lines: one resting on top of the blue bars and one resting on top of the pink bars. If those lines are non-parallel, then you most probably have an interaction.\nI couldn’t resist using some colour once again in our bar graph, but just a reminder that for plotting graphs for your lab reports or third year dissertation you should use APA compatible colours, such as black and grey (#000000, #CCCCCC). Colour is set in ggplot using either named colours or more commonly hexadecimal colour codes (as we have been using here). If you search online for “hexadecimal colour palettes” it will return images containing a selection of colours and the hexadecimal codes for using them in ggplot. Try the following resource: http://derekogle.com/NCGraphing/resources/colors.\nThe colours I have used here were taken from the palette below, which I accessed online (there are many other palettes to choose from if you are fussy about colours, which I tend to be).\n![}(images/ColourPalette.png){width=100%}\nBefore moving on, let’s save our most recent figure.\n\nggsave(\"FearAppealBarPlot.png\", height = 6, width = 5, dpi = 300)"
  },
  {
    "objectID": "PSYC214/Week6.html#plotting-multiple-graphs-and-interpreting-simple-main-effects",
    "href": "PSYC214/Week6.html#plotting-multiple-graphs-and-interpreting-simple-main-effects",
    "title": "Statistics for Psychologists",
    "section": "Plotting multiple graphs and interpreting simple main effects",
    "text": "Plotting multiple graphs and interpreting simple main effects\nSometimes, we want to plot multiple graphs within the same figure. In ggplot, we can plot multiple graphs using the facet_wrap command, which you may have encountered in Sam’s lab classes. For the first example, I am going to generate three different line plots in the same figure, each illustrating an interaction between factors. I am then going to get you to identify the simple main effects in each graph, as we did earlier.\nIn this example, we have three examples of alternative hypothetical interactions between the Fear and Efficacy manipulations for our COVID-19 experiment. The data are contained in the following tibble (note, we already have the means, so we don’t need to generate any descriptive statistics):\n\nthreeInteractions = tibble(\n  Example = c(\"A\",\"A\",\"A\",\"A\",\"B\",\"B\",\"B\",\"B\",\"C\",\"C\",\"C\",\"C\"),\n  Fear = c(\"No Fear Appeal\",\"No Fear Appeal\",\"Fear Appeal\",\"Fear Appeal\",\n        \"No Fear Appeal\",\"No Fear Appeal\",\"Fear Appeal\",\"Fear Appeal\",\n        \"No Fear Appeal\",\"No Fear Appeal\",\"Fear Appeal\",\"Fear Appeal\"),\n  Efficacy = c(\"No Efficacy Message\",\"Efficacy Message\",\"No Efficacy Message\",\"Efficacy Message\",\n        \"No Efficacy Message\",\"Efficacy Message\",\"No Efficacy Message\",\"Efficacy Message\",\n        \"No Efficacy Message\",\"Efficacy Message\",\"No Efficacy Message\",\"Efficacy Message\"),\n  Mean  = c(4.2,9,4,4.5,4.75,5.25,1,9,2,8,8,2)\n)\n# Make sure \"Example\", \"Fear\", and \"Efficacy\" are coded as factors\nthreeInteractions$Example  = factor(threeInteractions$Example) # \"Example\" is already organised alphabetically, \n# which is what we want\nthreeInteractions$Fear     = factor(threeInteractions$Fear, levels = c(\"No Fear Appeal\",\"Fear Appeal\"))\nthreeInteractions$Efficacy = factor(threeInteractions$Efficacy, levels = c(\"No Efficacy Message\",\"Efficacy Message\"))\n\nLet’s take a look at the data:\n\n(threeInteractions)\n\n# A tibble: 12 × 4\n   Example Fear           Efficacy             Mean\n   &lt;fct&gt;   &lt;fct&gt;          &lt;fct&gt;               &lt;dbl&gt;\n 1 A       No Fear Appeal No Efficacy Message  4.2 \n 2 A       No Fear Appeal Efficacy Message     9   \n 3 A       Fear Appeal    No Efficacy Message  4   \n 4 A       Fear Appeal    Efficacy Message     4.5 \n 5 B       No Fear Appeal No Efficacy Message  4.75\n 6 B       No Fear Appeal Efficacy Message     5.25\n 7 B       Fear Appeal    No Efficacy Message  1   \n 8 B       Fear Appeal    Efficacy Message     9   \n 9 C       No Fear Appeal No Efficacy Message  2   \n10 C       No Fear Appeal Efficacy Message     8   \n11 C       Fear Appeal    No Efficacy Message  8   \n12 C       Fear Appeal    Efficacy Message     2   \n\n\nWe want to generate a single figure that contains three graphs in different panels, one panel for example A, one panel for example B, and one panel for example C. The code required to generate the figure is much the same as for our initial line graph example, but we are now going to use the command facet_wrap(Example) to instruct ggplot to generate separate graphs for each of our three examples of an interaction. The code for generating the figure is below:\n\n# Using the dataframe called threeInteractions, place \"Fear\" on the x-axis, \"Mean\" on the y-axis, \n# and group the data according to the two levels of \"Efficacy\" \nggplot(data = threeInteractions, mapping = aes(x = Fear, y = Mean, group = Efficacy)) +\n  # geom_line produces two lines: one for \"No Efficacy Message\" and one for \"Efficacy Message\"\n  geom_line(size = 1, aes(color = Efficacy)) +\n  # geom_point adds mean data points to our lines\n  geom_point(size = 3, aes(color = Efficacy)) +\n  # Here we are manually setting the colour of the lines and data points\n  scale_color_manual(values=c(\"#355C7D\", \"#F67280\")) +\n  # This tells ggplot to plot the figure with different panels for Examples A, B, and C on a single row.\n  # You can also specify the number of columns as an option.\n  ###################################\n  facet_wrap(~Example, nrow = 1) +\n  ###################################\n  # Change y-axis lower and upper limits\n  ylim(0,10) +\n  # Manually set the x-axis, y-axis, and legend titles\n  labs(x = \"Fear\", y = \"Vaccination Intention\", color = \"Efficacy\") +\n  # Use black and white theme for background\n  theme_bw() +\n  # The theme function allows us to set various properties of our figure manually\n  theme(panel.grid.major = element_blank(), # Removes the major gridlines\n        panel.grid.minor = element_blank(), # Removes the minor gridlines\n        legend.box.background = element_rect(colour = \"black\"), # Adds a black border around the legend\n        legend.position = \"bottom\", # Positions the legend at the bottom of the graph\n        legend.direction = \"vertical\") # Orients the legend in the vertical direction\n\n\n\n\nRight, this is a good point to give you some practice extracting simple main effects. So, for each of the examples I want you to identify the pattern of the four simple main effects as we did earlier using the =, &lt;, and &gt; operators. Don’t worry if you find this a bit tricky, it takes some time to get your head around simple main effects. If you get stuck, give us a shout!\n\nExample A\n\nSimple main effect of Fear at the No Efficacy Message level of Efficacy: No Fear Appeal = Fear Appeal\nSimple main effect of Fear at the Efficacy Message level of Efficacy: _______\nSimple main effect of Efficacy at the No Fear Appeal level of Fear: _______\nSimple main effect of Efficacy at the Fear Appeal level of Fear _______\n\n\n\nExample B\n\nSimple main effect of Fear at the No Efficacy Message level of Efficacy: _______\nSimple main effect of Fear at the Efficacy Message level of Efficacy: _______\nSimple main effect of Efficacy at the No Fear Appeal level of Fear: _______\nSimple main effect of Efficacy at the Fear Appeal level of Fear _______\n\n\n\nExample C\n\nSimple main effect of Fear at the No Efficacy Message level of Efficacy: _______\nSimple main effect of Fear at the Efficacy Message level of Efficacy: _______\nSimple main effect of Efficacy at the No Fear Appeal level of Fear: _______\nSimple main effect of Efficacy at the Fear Appeal level of Fear _______\n\nBefore moving on, let’s save our most recent figure.\n\nggsave(\"ThreeInteractionsLinePlot.png\", height = 6, width = 8, dpi = 300)\n\nOkay, we are almost finished but I want to conclude the activities by showing you how to create multiple bar graphs within the same figure. We will once again use the data illustrating the three different types of interactions. The code required to generate the figure is much the same as for our initial bar graph example, but we are now going to use the command facet_wrap(Example) to instruct ggplot to generate separate graphs for each of our three examples of an interaction. The code for generating the figure is below:\n\n# Using the dataframe called threeInteractions, place \"Fear\" on the x-axis, \"Mean\" on the y-axis, \n# and group the data according to the two levels of \"Efficacy\" \nggplot(data = threeInteractions, mapping = aes(x = Fear, y = Mean, fill = Efficacy)) +\n  # geom_col is used to create our bar plots. Width and position_dodge control the degree of spatial\n  # separation of the columns\n  geom_col(width = 0.45, position = position_dodge(0.55)) +\n  # Here we are manually setting the colour of the columns \n  scale_fill_manual(values = c(\"#355C7D\", \"#F67280\")) +\n  # This tells ggplot to plot the figure with different panels for Examples A, B, and C on a single row.\n  # You can also specify the number of columns as an option.\n  ###################################\n  facet_wrap(~Example, nrow = 1) +\n  ###################################\n  # We are using scale_y_continuous this time to set the y-axis limits because we want to remove the\n  # white space that ggplot leaves between 0 and the x-axis line using expand = c(0,0).\n  scale_y_continuous(limits = c(0,10.5), expand = c(0,0)) +\n  # Manually set the x-axis, y-axis, and legend titles\n  labs(x = \"Fear\", y = \"Vaccination Intention\", fill = \"Efficacy\") +\n  # Use black and white theme for background\n  theme_bw() +\n  # The theme function allows us to set various properties of our figure manually\n  theme(panel.grid.major = element_blank(), # Removes the major gridlines\n        panel.grid.minor = element_blank(), # Removes the minor gridlines\n        legend.box.background = element_rect(colour = \"black\"), # Adds a black border around the legend\n        legend.position = \"bottom\", # Positions the legend at the bottom of the graph\n        legend.direction = \"vertical\") # Orients the legend in the vertical direction\n\n\n\n\nBefore closing, let’s save our most recent figure to the directory.\n\nggsave(\"ThreeInteractionsBarPlot.png\", height = 6, width = 8, dpi = 300)\n\nThere are a couple of additional things I should mention regarding graphs. Statistics textbooks generally recommend that you should only plot all of the data, which is necessary to discern an interaction, if you actually have a significant interaction, otherwise you should plot whatever main effects are significant. I strongly disagree with this recommendation and it seems that most other researchers do as well because I rarely if ever see this advice being followed in published research. Whether your interaction is significant or not, I recommend graphing all of the data, simply because that gives your audience a complete picture of the results. Thus, even if only the main effects are significant you should plot all the results, so both the significant main effects and the absence of a significant interaction are visible to your audience. The other thing to mention is that a graph is generally always preferred to presenting your results in a table for the simple reason that “a picture speaks a thousand words”—graphs are easier to read than tables. And remember, APA style dictates that we can’t present the same data in different formats—that is, you should not present the same data as both a table and a graph. When forced to choose, graphs win hands down. Just make sure you include both the means and a measure of variability (e.g., standard deviation, standard error, or confidence interval) in the form of error bars.\nOkay, that’s the end of the exercises for today’s session. Well done! Please bear in mind, we will be using the code employed today in future sessions to generate the plots for our data, but I won’t give you the code—you will have to adpat the code I have given you here."
  },
  {
    "objectID": "PSYC214/Week6.html#further-tasks",
    "href": "PSYC214/Week6.html#further-tasks",
    "title": "Statistics for Psychologists",
    "section": "Further tasks",
    "text": "Further tasks\nIf you still have time remaining, consider completing the following activity. In all of the plotting exercises above we used the theme function in ggplot to set various properties of our plots (see the bottom of each code segment). This function is incredibly useful and can be used to change all the non-data components of a plot (titles, labels, fonts, background, gridlines, and legends). To learn more about the arguments that theme accepts, type help(theme) into the console. Your additional tasks, should you choose to accept them, are as follows. Modify the code we used to generate the first line plot so that:\n\nThe x-axis title (i.e., Fear) is presented in size 14 bold font.\nThe x-axis text (i.e., No Fear Appeal, Fear Appeal) is presented in size 12 black font.\n\nThe y-axis title (i.e., Vaccination Intention) is presented in size 14 bold font.\nThe y-axis text (i.e., the numbers on the y-axis) is presented in size 12 black font.\nThe legend title (i.e., Efficacy) is presented in size 14 bold font.\n\nWell done folks! We’ve covered a lot of ground today, that’s all until our next session. Enjoy the rest of the week!"
  },
  {
    "objectID": "PSYC214/Week5.html",
    "href": "PSYC214/Week5.html",
    "title": "Statistics for Psychologists",
    "section": "",
    "text": "Caution\n\n\n\nThis page is under construction for 24/25 and may be subject to change before the teaching week."
  },
  {
    "objectID": "PSYC214/Week5.html#a-between-participants-single-factor-study",
    "href": "PSYC214/Week5.html#a-between-participants-single-factor-study",
    "title": "Statistics for Psychologists",
    "section": "A Between-Participants Single-Factor Study",
    "text": "A Between-Participants Single-Factor Study\nA short-term memory researcher wants to know whether objectively grouping lists of verbal items improves verbal short-term memory performance. She administers a serial recall task to three different groups of participants (described below). In the study phase of this task, participants are given a list of six words to study (e.g., gorilla, leprosy, nursery, radio, botany, calcium) spoken at a rate of one word per second, with a brief pause between each word. In the test phase, immediately after the final item has been spoken, participants must recall the list of words (by speaking them aloud) in their original presentation order. The dependent measure of interest is verbal recall accuracy: the percentage of words recalled in their correct position in the study list. For example, suppose the participant recalled gorilla, leprosy, radio, nursery, botany, calcium in response to the above study list. The first two items (gorilla, leprosy) and last two items (botany, calcium) have been recalled in their correct positions, but the middle two items (nursery, radio) have exchanged positions with one another, and are therefore incorrect. Thus, in this example, the participant recalled 4/6 = 67% of items accurately. The researcher administers to each participant a total of 20 serial recall study-test trials, with each study list comprising the same set of words, but presented in a different random order. The final score for each participant is the percentage of words recalled accurately, averaged across all 20 study-test trials.\nThere are 90 participants in total, allocated randomly and evenly to one of the following three conditions:\n\nUngrouped: in this condition, the study lists are presented in an ungrouped fashion. That is, the temporal pause separating each spoken word in study lists is always 0.5 seconds.\n2-2-2: in this condition, the study lists are organised into three groups, each containing two items. This is achieved by extending the temporal pauses after the second and fourth items in study lists from 0.5 seconds to 1.5 seconds.\n3-3: in this condition, the study lists are organised into two groups, each containing three items. This is achieved by extending the temporal pause after the third item in study lists from 0.5 seconds to 1.5 seconds.\n\nThe researcher wants to know whether (a) grouping study lists improves verbal recall accuracy compared to an ungrouped scenario, and (b) whether the degree of improvement depends on the type of grouping used (grouping in twos \\(vs.\\) grouping in threes). The researcher has chosen to run the study as a between-participants design rather than a within-participants design because she is worried about the possibility of carryover effects. Specifically, if the study was run as a within-participants design there is a risk that participants may carry over a grouping strategy from one condtion into another condition. For example, having completed the 3-3 condition, participants may then spontaneously group the ungrouped lists in the ungrouped condition into threes. This would make interpretation of the results of the experiment difficult, hence the decision to use a between-participants design.\nThe researcher has two hypotheses she wants to test. Specifically, she predicts that (1) verbal recall accuracy will be higher in the 2-2-2 condition than in the ungrouped condition, and (2) that verbal recall accuracy will be higher, in turn, in the 3-3 condition than in the 2-2-2 condition. She has thus decided a priori to conduct planned comparisons comparing accuracy for the ungrouped vs. 2-2-2 conditions, and the 2-2-2 vs. 3-3 conditions.\nFirst things first, let’s load the data set:\n\n# Import the data\ngroupingData = read_csv(\"Grouping.csv\")\n# Make sure Condition is a factor with levels ordered: ungrouped, 2-2-2, 3-3\ngroupingData$Condition = factor(groupingData$Condition, levels = c(\"Ungrouped\",\"2-2-2\",\"3-3\"))\n# Print the data\n(groupingData)\n\n\n\n# A tibble: 90 × 3\n   Participant Condition Accuracy\n         &lt;dbl&gt; &lt;fct&gt;        &lt;dbl&gt;\n 1           1 Ungrouped     62.7\n 2           2 Ungrouped     53.7\n 3           3 Ungrouped     68.7\n 4           4 Ungrouped     77.3\n 5           5 Ungrouped     60.2\n 6           6 Ungrouped     63.7\n 7           7 Ungrouped     46.9\n 8           8 Ungrouped     67.4\n 9           9 Ungrouped     60.4\n10          10 Ungrouped     49.5\n# ℹ 80 more rows\n\n\nWe have three columns: Participant (the participant number ranging from 1 to 90), Condition (the condition to which each participant was allocated: ungrouped vs. 2-2-2 vs. 3-3), and Accuracy (the percentage of list items recalled in the correct order, averaged across all study-test trials). You can take a closer look at the data using view(groupingData) which will open up the full data set in a new window.\n\nDescriptives statistics and assumption checks\nLet’s begin by calling some descriptive statistics. Specifically, we want the mean and sd of the Accuracy dependent variable, as a function of our Condition factor. We can obtain these quantities using the get_summary_stats() function in the rstatix package:\n\n# Get descriptive statistics\ndescriptives = groupingData %&gt;%\n  # Organise the output by Condition\n  group_by(Condition) %&gt;%\n  # Request mean and standard deviation\n  get_summary_stats(Accuracy, show = c(\"mean\", \"sd\"))\n  # Round the results to *at least* two-decimal places\n  options(digits = 4) \n  # Print the results\n  print.data.frame(descriptives)\n\n  Condition variable  n  mean     sd\n1 Ungrouped Accuracy 30 61.99 11.416\n2     2-2-2 Accuracy 30 63.75  9.177\n3       3-3 Accuracy 30 72.19  9.441\n\n\nOkay, let’s take a look at these descriptive statistics. Starting with the means, we can see that the means for the ungrouped and 2-2-2 conditions are very similar—it does not look like organizing study lists into three groups of two items improved performance. However, the mean for the 3-3 condition is much larger than the means for the ungrouped and 2-2-2 conditions, suggesting that organising the study list into two groups of three items did improve performance. Turning to the standard deviations, these are very similar for each of the three conditions, so it does not look like the data violate the homogeneity of variance assumption. However, let’s run Levene’s test for equality of variances to verify if that is indeed the case.\nWe can execute Levene’s test on our data using the levene_test() function in the rstatix package:\n\n# Levene's test for homogeneity of variance\ngroupingData %&gt;% \n  levene_test(Accuracy ~ Condition)\n\n# A tibble: 1 × 4\n    df1   df2 statistic     p\n  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt;\n1     2    87      1.24 0.295\n\n\nThe test result is nonsignificant (p = .295), suggesting that the variances do not differ systematically. In short, using Levene’s test criterion, we can be satisfied that the homogeneity of variance assumption has been met.\nSo far so good, let’s now check to see if there are any outliers or extreme values in the data for each of our three conditions. We can do this using the identify_outliers function contained in the rstatix package:\n\ngroupingData %&gt;% \n  # Organise the output by Condition\n  group_by(Condition) %&gt;%\n  # Identify outliers and extreme values of our dependent measure\n  identify_outliers(Accuracy)\n\n# A tibble: 1 × 5\n  Condition Participant Accuracy is.outlier is.extreme\n  &lt;fct&gt;           &lt;dbl&gt;    &lt;dbl&gt; &lt;lgl&gt;      &lt;lgl&gt;     \n1 3-3                82     96.3 TRUE       FALSE     \n\n\nWe have a single outlier in the 3-3 condition—one participant’s score is quite a bit larger than the other scores in the distribution. However, as it is only a single non-extreme value it will not exert undue influence on the data and we can safely proceeed without having to remove, replace, or transform it.\nLet us now establish whether our data satisfy the assumption of normality. To do this, we can run the Shapiro-Wilk test, which you will recall tests whether data are normally distributed. We can do this using the shapiro_test() function in the rstatix package:\n\n# Shapiro Wilk test for normality\ngroupingData %&gt;%\n  # Organise output by Condition\n  group_by(Condition) %&gt;%\n  # Run Shapiro Wilk tests\n  shapiro_test(Accuracy)\n\n# A tibble: 3 × 4\n  Condition variable statistic      p\n  &lt;fct&gt;     &lt;chr&gt;        &lt;dbl&gt;  &lt;dbl&gt;\n1 Ungrouped Accuracy     0.938 0.0788\n2 2-2-2     Accuracy     0.971 0.563 \n3 3-3       Accuracy     0.974 0.648 \n\n\nThe test statistics are nonsignificant for all three conditions (all p &gt; .05), although only marginally so for the ungrouped condition. This indicates that the distributions of scores do not differ significantly from a normal distribution. We can therefore be satisfied that our data meet the assumption of normality. Remember that the Shapiro-Wilk test is sensitive to sample size—as sample size increases, the test becomes less accurate. The accepted rule of thumb is that if the number of observations (i.e., participants) in each condition is less than 50, then the Shapiro-Wilk test should provide an accurate test result. However, if the number of observations is above this value, then the test result may be inaccurate, so you should instead inspect the QQ plots of the data to determine if the normality assumption has been met. We have 30 observations per condition, so we can safely rely on the results of the Shapiro-Wilk test in this instance. Accordingly, we won’t bother looking at the QQ plots on this occasion.\n\n\nANOVA and planned comparisons\nOkay, we have performed our checks and we are now ready to run our ANOVA. We can do this using the aov function:\n\n# Our model with Accuracy as the DV and Condition as the factor\ngroupingModel = aov(data = groupingData, Accuracy ~ Condition) \n# Ensure we get exact p values rather than scientific notation\noptions(scipen = 999)\n# Create model summary\nsummary(groupingModel)\n\n            Df Sum Sq Mean Sq F value  Pr(&gt;F)    \nCondition    2   1783     891     8.8 0.00033 ***\nResiduals   87   8807     101                    \n---\nSignif. codes:  0 '***' 0.001 '**' 0.01 '*' 0.05 '.' 0.1 ' ' 1\n\n\nLooking at the table, the F value is 8.805, which is rather large as \\(F\\) values go, so it comes as no surprise to find when we look at the p value that the effect of condition is significant (p &lt; .001). The large F value and small p value imply that the effect of condition is large, but let’s verify this by looking at the effect size. We can do this using the effectsize() function in the effectsize package:\n\n# *** ENTER YOUR OWN CODE HERE FOR CALCULATIING THE ETA-SQUARED EFFECT SIZE FOR THE EFFECT OF CONDITION ***\n\n\n\n# Effect Size for ANOVA\n\nParameter | Eta2 |       95% CI\n-------------------------------\nCondition | 0.17 | [0.06, 1.00]\n\n- One-sided CIs: upper bound fixed at [1.00].\n\n\nThe \\(\\eta^2\\) effect size is 0.17, which represents a large effect consistent with our expectations based on the results of our ANOVA (reminder: 0.01 = small effect; 0.06 = medium effect; 0.14 = large effect).\nRemember that because there are three levels in our condition factor, the ANOVA only tells us that there are differences between the means of our groups, but not where those differences are located. So, we need to conduct some follow-up tests. In this case, the researcher decided to conduct planned comparisons comparing the ungrouped and 2-2-2 conditions, and the 2-2-2 and 3-3 conditions. We can perform these comparisons using the pairwise_t_test() function in the rstatix package:\n\ngroupingData %&gt;% \n  # Execute independent-samples t-tests - remember to set pool.sd = FALSE \n  # and var.equal = TRUE\n  pairwise_t_test(Accuracy ~ Condition, pool.sd = FALSE, var.equal = TRUE, \n    # Just generate the two comparisons of interest: ungrouped vs. 2-2-2\n    # and 2-2-2 vs. 3-3\n    comparisons = list(c(\"Ungrouped\",\"2-2-2\"), c(\"2-2-2\",\"3-3\"))) %&gt;%\n  # Ensure we get exact p values rather than scientific notation\n  p_format(digits = 4, leading.zero = FALSE) \n\n# A tibble: 2 × 10\n  .y.      group1    group2    n1    n2 statistic    df p     p.adj p.adj.signif\n* &lt;chr&gt;    &lt;chr&gt;     &lt;chr&gt;  &lt;int&gt; &lt;int&gt;     &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt;       \n1 Accuracy Ungrouped 2-2-2     30    30    -0.657    58 .514  .514  ns          \n2 Accuracy 2-2-2     3-3       30    30    -3.51     58 .000… .002  **          \n\n\nSince we are only performing two planned comparisons, I haven’t bothered to invoke the Bonferroni correction (annoyingly, the table includes the Bonferroni adjusted p value in the penultimate column, p.adj, despite the fact that we haven’t request it explicitly! Just make sure if you are not using the correction that you read the values from the column labelled p). Consistent with our intuitions when inspecting the descriptive statistics, we can see from inspection of the table that the comparison between the ungrouped and 2-2-2 conditions is nonsignificant (p = .514), whereas the comparison between the 2-2-2 and 3-3 conditions is significant (p &lt; .001).\nThe next step is to get the Cohen’s d effect size for each of our planned comparisons. We can get the Cohen’s d effect size for the ungrouped vs. 2-2-2 comparison using the following code which makes use of the cohens_d() function from the rstatix package:\n\n# Let's break down what the piece of code below is doing:\n# The function cohens_d() requires that we specify the two groups we wish to produce our d value for along with \n# the dependent variable.\n# The first argument, groupingData$Accuracy[groupingData$Condition == \"Ungrouped\"], tells R that for the first\n# group we want the variable Accuracy in the data set groupingData to be the dependent variable (groupingData$Accuracy) and we want the first group to be the ungrouped condition ([groupingData$Condition == \"Ungrouped\"])\n# The second argument, groupingData$Accuracy[groupingData$Condition == \"2-2-2\"], tells R that for the second \n# group we also want the variable Accuracy in the data set groupingData to be the dependent variable (groupingData$Accuracy) and we want the second group to be the 2-2-2 condition ([groupingData$Condition == \"2-2-2\"])\n\n# Cohen's d for ungrouped vs. 2-2-2\ncohens_d(groupingData$Accuracy[groupingData$Condition == \"Ungrouped\"],groupingData$Accuracy[groupingData$Condition == \"2-2-2\"])\n\nCohen's d |        95% CI\n-------------------------\n-0.17     | [-0.68, 0.34]\n\n- Estimated using pooled SD.\n\n\nFor the ungrouped vs. 2-2-2 comparison, the Cohen’s d effect size is equal to –0.17, which represents a small effect (reminder: 0.2 = small effect, 0.5 = moderate effect, 0.8 = large effect; Cohen, 1988).\nNow, let’s get the Cohen’s d effect size for the 2-2-2 vs. 3-3 comparison:\n\n# *** ENTER YOUR OWN CODE HERE FOR CALCULATING THE COHEN'S D EFFECT SIZE FOR THE 2-2-2 VS. 3-3 COMPARISON ***\n\n\n\nCohen's d |         95% CI\n--------------------------\n-0.91     | [-1.43, -0.37]\n\n- Estimated using pooled SD.\n\n\nFor the 2-2-2 vs. 3-3 comparison, the Cohen’s d effect size is equal to –0.91, which represents a large effect.\n\n\nWriting up the results\nA single-factor between-participants Analysis of Variance (ANOVA) was performed, with condition as the independent variable (control vs. 2-2-2 vs. 3-3) and verbal recall accuracy as the dependent variable. There was a significant effect of condition, F(2, 87) = 8.81, p &lt; .001, \\(\\eta^2\\) = .17. Planned comparisons revealed that verbal recall accuracy did not differ significantly between the ungrouped condition (M = 62.00, SD = 11.40) and the 2-2-2 condition (M = 63.80, SD = 9.18), t(58) = –0.657, \\(p\\) = .514, d = –.17, whereas verbal recall accuracy was higher in the 3-3 condition (M = 72.20, SD = 9.44) than in the 2-2-2 condition, t(58) = –3.51, \\(p\\) &lt; .001, d = –.91.\nNote: you can also include the 95% CIs for the \\(\\eta^2\\) and Cohen’s d effect sizes (as Sam has showed you in previous labs) but I haven’t included them above. Their inclusion is optional and since I have not requested you include them in your PSYC204 Short Report CWA, I thought you might think me hypocritical if I included them here (I may be many things, but I’m not a hypocrite).\nOkay, that’s all the exercises for the first data set complete, well done, but there’s no time for a break because we have another data set to contend with."
  },
  {
    "objectID": "PSYC214/Week5.html#further-tasks",
    "href": "PSYC214/Week5.html#further-tasks",
    "title": "Statistics for Psychologists",
    "section": "Further tasks",
    "text": "Further tasks\nWe have covered a lot of material in today’s session and I don’t expect you to have time to complete any further tasks. However, if you have managed to reach this point and still have time remaining, here are two things you can do (you should borrow and adapt the code from previous weeks for these two tasks):\n\nGenerate a bar figure plotting the results for the grouping study. Plot the condition means and standard error error bars.\nGenerate a line figure plotting the results for the word length study. Plot the condition means and standard error error bars.\n\nTime to relax now and pat yourself on the back for a job well done! See you in next week’s lab session."
  },
  {
    "objectID": "PSYC214/index.html#structure-and-content",
    "href": "PSYC214/index.html#structure-and-content",
    "title": "Statistics for Group Comparisons",
    "section": "",
    "text": "Weeks 1-5: Single-Factor Designs - Taught by Dr Sam Russell\n\nWeek 1: Measurement, Variance and Inferential Statistics\nWeek 2: One-Factor Between-Participants ANOVA\nWeek 3: Assumptions of ANOVA and Follow-Up Procedures\nWeek 4: One-Factor Within-Participants ANOVA\nWeek 5: Interim Summary\n\nWeeks 6-10: Factorial Designs - Taught by Dr Mark Hurlstone\n\nWeek 6: Introduction To Factorial Designs\nWeek 7: Two-Factor Between-Participants ANOVA\nWeek 8: Two-Factor Mixed and Within-Participants ANOVA\nWeek 9: Three-Factor ANOVA\nWeek 10: Class Test"
  },
  {
    "objectID": "PSYC234/index.html#structure-and-content",
    "href": "PSYC234/index.html#structure-and-content",
    "title": "From association to modelling causality",
    "section": "",
    "text": "Weeks 11-14: Multiple Linear Regression (all in the class test in week 20)\nLecturer: Emma Mills\nLecture 1: Review of correlation, simple regression and demonstration of multiple regression\nLecture 2: Multiple regression including categorical predictors and planned contrasts\nLecture 3: Multiple regression models that include interactions (moderated variables)\nLecture 4: Multiple regression and mediation\nWeek 15: Factor Analysis and Binomial Tests\nWe have split lecture 5 into two parts.\nLecture 5 Part 1: Factor analysis (Emma Mills) - not in the class test in week 20\nLecture 5 Part 2: Binomial test (Amy Atkinson) - in the class test in week 20\nWeeks 16-17: Non Parametric Tests - in the class test in week 20\nLecturer: Dr Amy Atkinson\nLecture 6: Wilcoxon rank-sum test and Wilcoxon signed-rank test\nLecture 7: Kruskal-Wallis test and Friedman’s ANOVA\nWeeks 18- 19: Non-linear regression models - in the class test in week 20.\nLecturer: Dr Amy Atkinson\nLecture 8: Binary logistic regression models.\nLecture 9: Multiple binary logistic regression & ordinal logistic regression models."
  }
]