---
title: "Factor Analysis"
author: "Emma Mills"
date: "01/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(corrplot)
library(ggplot2)
library(car)
```

## Factor Analysis

Factors, with respect to factor analysis methods, are different from the factors that we talk about with the ANOVA method.

Also, sometimes, the method of factor analysis is used interchangeably with principal components analysis. Both factor analysis and principal components result in a reduced set of variables but they are slightly different. Principal components are built from the information from multiple variables that influence that component.  Think of them like a reduction of variables - from many we make few.  But they are still measures.

Factor analysis has a different perspective: From several variables (three or more), factor analysis attempts to separate the common parts of variance (the communality) from the uniqueness of the variance.  The common parts are theorised to correlate with each other because they are *being* influenced by the same latent (invisible?) construct.  The communality of a variable with other variables defines a factor loading; uniqueness + error is left over.  So, for good factors, we would like to see high communality values.

In contrast, if we have single, independent variables, high correlation values between a pair of variables can be problematic when running our statistical models (this is the multicollinearity aspect from the lecture).

It follows, that a condition of multicollinearity amongst pairs of variables may suggest that a factor analysis is plausible for a given dataset.

The theory that lies behind a dataset can help suggest how many latent variables may be suggested in the variables, but the actual process is very data driven.  There is a succession of steps where you make decisions based upon output from a series of tests.

### Helpful tips for transforming variables before you begin

  - reversed scored variables - transform them so they work in the same direction as the rest of the variables
  - remove outcome, categorical and ordinal variables from the dataset
  - relies heavily on Pearson's r correlations so all the assumptions of linearity persist
    - especially sensitive to the presence of ordinal and binary variables

Interpreting Factors:

 - a factor should have at least three variables loaded to a sufficient level (overdetermination)
 - a variable should have most of its loading on one factor
 - a factor should show a moderate level of internal consistency (=> .70)
 - factors should make theoretical sense
 
 
Steps:

 - parallel analysis identifies how many factors
 - conduct a factor analysis
   - evaluate against four interpretation criteria above
 - repeat with one less factor
 - repeat until can reduce no longer.
 - Name them
 - Report them
 
 It is better to have too many factors than too few in your solution.  Too few factors means that information may be lumped in with the error term (Uniqueness value).

```{r cars}

d1 <- read_csv("RL_numeric_tidy.csv")


Y <- d1[ , 2] # Also store logrt as outcome variable in Y - needed later for internal consistency measures

describe(d1) # be careful here because the print out makes everything seem like a numerical variable - asterisks on the variable labels (eg, itemname) denote a character / factor class of variable
```

```{r}
m_nfa_full <- lm(Y ~ Age + 
                   Q9 + Q10 + Q11 +
                   Q12 + Q13 + Q14 +
                   Q15 + Q16 + Q17 +
                   Q18 + Q19 + Q20, d1)

summary(m_nfa_full)
```

Let's do some stargazing because understanding this model in full is not what is required. Very few of the predictor variables here are having a significant impact upon the outcome variable. The model explains 7% of the variance in Y. Seeing the full model, hopefully will help you remember to put the dataset back together again to re-estimate the model.

The model for this set of predictors on Y is not significant (F (13, 169) = 1.039, *p* < .42).  It explains 7% of the variance on `logrt`.

As factor analysis can also make larger datasets more manageable and more tractable for estimation of relationships, it helps to have a large dataset to begin with.  You can check how large your dataset is by either looking on your environment row or calling it:

```{r}
dim(d1)
```

183 observations across 15 variables.  The variables we are interested in are columns 4 onward...I will remove these and make a copy at the same time, so that the original dataframe remains intact.


### Removing columns that won't be needed in the factor analysis

Below is some code that splits the dataset.  This is an example of the circle in the workflow diagram.  Because of decisions I make later on in the script, I return to the code on line 119 and add more variables, to ensure that when I call on the d2 dataframe at the end of the script, I have all the original, discarded variables in one place.

```{r}
d <- d1[ , -c(1:2)] # a super quick snippet of code to remove the first 3 columns

d2 <- d1[ , c(1, 3)] # I am saving the unwanted variable set to a dataframe of its own for modelling process once we have a factor structure - see near the end of the document.

head(d) # all numerical variables.

summary(d)
```

### Visualise all variables; test for normality

```{r}
# par(mfrow = c(4, 3))
```

```{r}
apply(d,2,hist)
```

### Visualise all variables through a correlation matrix

Variable sets can get big pretty quickly, and because of measurement error and interactions, many a time one variable will overlap in what it measures with another variable - or to describe it another way, they will be correlated.  When lots of variables do this with one another, we call this multicollinearity.

We want out predictor to be correlated with our outcome variable - that is how we find effects, but having too much correlation between our predictors can be problematic.

When predictors overlap too much, they introduce redundancy into the model, and space where useful information could be is taken up with a kind of repeated information that the model has already seen.  Regression modelling thrives on having lots of sources of different information to find structure and patterns in the data.  When you have overlapping information, you have too much of one kind of information, and the model can get stuck, or give you poor estimations of coefficients.

Some correlation is okay and a model can cope, but strong correlation between predictors could cause problems.

We can visualise the correlations between variables using a correlation matrix.  We have used a kind of correlation matrix in weeks 2 - 4. Using a different one here that leaves out the histrograms of the variables on the diagonal, and concentrates on just the relationships between the variables:

```{r}
# send correlations to a new dataframe first
datamatrix <- cor(d)
corrplot(datamatrix, method = "number")

```

If the labels are too big and I can't see numbers,  turn off numbers and use just colour instead.

```{r}
corrplot(datamatrix, method = "shade", tl.pos = "n")
```

What does the graph mean? It is a coloured correlation matrix.  We can use this because we are try to get an overview of the level of correlation values across the entire dataset; we are not interested in the granular detail of the size of an association between two variables here. 

Ignore the diagonal line of dark blues squares.  This is where a variable is being paired with itself so = a perfect correlation of 1. The triangle below the diagonal line and the triangle above the diagonal line are symmetrical reflections of each other, so you only need to look at either the upper or lower triangle for the overview.

Each square represents a pair of variables in the dataframe and their correlation value - or how much of the same information they seem to share.  As the correlation gets stronger (moves toward from -1 or moves towards 1 from 0), the colour of the square moves from a paler shade to a deeper shade of either red or blue.  

If we were checking for multi-collinearity amongst our variables, we would like to see squares with no very deep red or blue colours.

If we are checking whether a factor analysis is feasible, we *would* like to see squares with very deep red or blue colours.  We also would like all squares to have the colour of around -.3 or .3 and above.  What do you think of the state of this dataset for a factor analysis?

Look at Age and its correlation with all the Q variables - no correlation whatsover with any of them.  I have left this variable in this dataset at this point to show how in the factor analysis, Age should load onto a factor of its own - because it does not share any of the same information with any of the other predictors here.

So:  for suitability for factor analysis, the greater proportion of your variables should be correlated at +/- .3.

There are a couple of tests you can run to see if a factor analysis is feasible with the data you have:


### Test 1: The Kaiser-Meyer-Olkin Index (KMO (1974))

A cutoff / threshold for this test is an `Overall MSA` score of equal or above .60 and some others write equal or above .70.

```{r}
KMO(r = cor(d))
```


The KMO test gives us a *Measure of Sampling Adequacy* (MSA) for each item and the set of variables overall (Overall MSA). MSA levels of  > .80 for each item is desirable; variables showing MSA levels <.50 are not recommended for factor analysis.  Using a method called "principal axis" can help, however,  when MSA values are lower than desired.

How do these variables look to you?


### Test 2: Bartlett's test of sphericity (1950)

```{r}
cortest.bartlett(d)
```

Bartlett's test of sphericity tests the null hypothesis that the correlation matrix of the dataset has values of 1 on the diagonals and values of 0 elsewhere.  A correlation matrix that shows this pattern of values shows a perfect correlation when a variable is paired with itself and an absence of correlation between pairs of  variables.  Consequently, because we are testing a null hypothesis, we are looking for a chi-square *p*-value <.05 to indicate that factor analysis is feasible. We have that here (p < .001), so we can reject the null hypothesis.

So far so good!

### Test 3: Parallel Analysis

```{r}
parallel_test <- fa.parallel(d)
```

This method send a warning message.  We are not using any methods to estimate factors here, as we are trying to detect a number of factors to estimate before we begin.

To read this plot, you need to look at how each of the blue lines dip beneath the red dashed lines. Choosing the triangle blue line (for factor analysis), count the number of symbols from the top left hand corner until the line dips beneath the bottom most red dashed lines. This is the number of factors recommended by the parallel analysis to put into your factor analysis.

Here the component number is 1 and the factor number is 2.

### Test 4: A scree test:

Very similar to the Parallel Analysis function above with slightly less information.

```{r}
scree(d)
```

### Conducting the factor analysis:

The parallel analysis suggests 2 factors so we can have a try for that number of factors now (nfactors argument below).  In the code below, the "fm" argument is asking the function to use principle axis ("pa") estimation and the "rotate" argument is asking the function to use an oblique rotation.  You don't need to know much about these at this point. 

```{r}
mod <- fa(r = d, 
          nfactors = 2,
          fm = "pa",
          rotate = "oblimin")

print(mod)
```

Lots of information here.  Look at the Age loadings.  LOW!  On both factors!  Because we want to see the structure of the factors for loadings above 0.3 we can ask the print function to show only those loadings that are above 0.3.


```{r}
print(mod, cut = 0.3, digits = 3) # just loadings that are above 0.3 to check for quality
```
There are two boxes in the output:  

Second box:  Columns PA1 - PA2 are the latent variables or factors estimated by the `fa` function. They list how each of the variables load onto 1 of 2 factors:  The loading columns show how each of the columns in the datasets (on the rows) load onto each of the hypothesised factors.  

First box: - (Use the slider to see all of the output) Beginning with row `SS loadings`: this gives you an idea of robustness in the factors, `Proportion Var` gives the variance each factor explains in the data and `Cumulative Var`, how much total variance is explained as we add each factor variance onto the previous factor variance.  So by the time we get to PA2 about 55% of the variance in the data has been explained by the factor structure. `Proportion Explained` gives the weighting of the factor in the amount of variance explained (here factor 1 gets the majority of the proportion) and `Cumulative Proportion` is the consecutive summing of proportion explained.  By the time all the factors are summed the cumulative proportion should equal 1.

#### Looking at loadings within factors

The following guidelines exist for gauging how valid each variable loading is:

 - 0.3 - 0.4 = ok
 - > 0.5 - better
 - > 0.7 - best
 
Against these guidelines, PA1 looks strong - all > 0.6; PA2 looks ok for loadings but only two at this level - all the others (that we have hidden) are < .03.
 
#### Looking for cross-loadings of variables across factors

Here we are looking across the variables (across the rows). 

A significant loading is a loading greater than > .03.  And cross loadings is official when a variable shows up on more than one factor at that level.

```{r}
print(mod, digits = 3) # getting all loadings rather than just > .03 now; Two outputs are returned.
```

  - all variables load on PA1 & PA2 and only two variables are above 0.3.  This suggests that the factor structure of the data is better expressed by one latent variable.
  
#### Looking for Communalities

This is a value that will be familiar to you.  Each factor will explain an amount of variance.  It's a little bit like the R^2 in linear regression.  Column `h2` is where you look for this.  A rule of thumb for a valid factor is `h2` > 0.25.

  - all except Age are above 0.25

#### Looking for factor correlations

This is at the heart of multicollinearity.  Where factors have correlations lower than .85 multicollinearity may not be problematic.  Where factors are correlated above .85, there is a massive overlap between factors, which means they could be measuring the same thing, and thus can be combined.

Factor correlations are stored in the first output box: 

 - PA1 >.85 between our factors.

Interpreting Factors:

 - a factor should have at least three variables loaded to a sufficient level (overdetermination) - yes
 - a variable should have most of its loading on one factor - not here but mostly PA1 > 0.3
 - a factor should show a moderate level of internal consistency (=> .70) - see below
 - factors should make theoretical sense


#### Checking for internal consistency 

Put each of the items into a latent variable that is indicated by the model.

```{r}
names(d)
print(mod, cut = 0.3, digits = 3)

PA1 = c("Q10", "Q11", "Q12", "Q13", "Q14", "Q16", "Q17", "Q18", "Q19", "Q20")
PA2 = c("Q9", "Q15")

```

Desiderata is an internal consistency score of >.70

Look in each of the second boxes of the output for the raw alpha value.

```{r}
alpha_PA1 <- psych::alpha(d[PA1], check.keys = TRUE)
print(alpha_PA1, digits = 3) # alpha  = 0.92
```

```{r}
alpha_PA2 <- psych::alpha(d[PA2], check.keys = TRUE)
print(alpha_PA2, digits = 3) # alpha = 0.84
```

Our factors have great internal consistency.

What is interesting here is that each of the items Q9 - Q20 are supposed to represent 4 subscales of a questionnaire, with three questions pertaining to each subscale. Clearly, in this sample, the questionnaire does not perform to its original design or purpose.

#### Draw your factor analysis structure

```{r}
fa.diagram(mod)
```

The package draws all of the variables as belonging to PA1 - even though we checked PA2 above - the strongest structure is just one latent variable.

### Below is commented out because the GPArotation package is not loaded on the server - I have performed this at home.

```{r}
# install.packages("GPArotation")
library(GPArotation)

mod_r <- fa(r = d, 
            nfactors = 4,
            fm = "pa",
            max.iter = 100,
            rotate = "promax")

print(mod_r, cut = 0.3, digits = 3) # just loadings that are above 0.3 to check for quality
```
Look at factor loadings:  within factors

 - 0.3 - 0.4 = ok
 - > 0.5 - better
 - > 0.7 - best
 
PA1 looks ok; PA2 has only one loading; PA3 looks ok; PA4 looks ok.
 
#### Looking for cross-loadings of variables across factors

Here we are looking across the variables (across the rows). 

A significant loading is a loading greater than > .03.  And cross loadings is official when a variable shows up on more than one factor at that level.

```{r}
print(mod_r, digits = 3)
```
IGNORE AGE

  - Q9  cross loadings on 1 and 4; strongest 1
  - Q10 no cross loadings: 4
  - Q11 no cross loadings: 4
  - Q12 cross loadings on 3 and 4; strongest 4
  - Q13 no cross loadings: 4
  - Q14 no cross loadings: 3
  - Q15 no cross loadings: 1
  - Q16 no cross loadings: 1
  - Q17 no cross loadings: 1
  - Q18 no cross loadings: 3
  - Q19 no cross loadings: 3
  - Q20 cross loadings on 2 and 4; strongest 2
  
#### Looking for Communalities

This is a value that will be familiar to you.  Each factor will explain variance.  It's a little bit like the R^2 in linear regression.  Column `h2` is where you look for this.  A rule of thumb is `h2` > 0.25.

  - all are above 0.25

#### Looking for factor correlations

This is at the heart of multicollinearity.  Where factors have correlations lower than .85 multicollinearity may not be problematic.  Where factors are correlated above .85, there is a massive overlap between factors, which means they could be measuring the same thing, and thus can be combined.

Factor correlations are stored above in the first output box:

 - No correlations >.85 between our factors.

Interpreting Factors:

 - a factor should have at least three variables loaded to a sufficient level (overdetermination) - no - PA2 has only one
 - a variable should have most of its loading on one factor - no
 - a factor should show a moderate level of internal consistency (=> .70)
 - factors should make theoretical sense
 
```{r}

PA1 = c("Q9", "Q15", "Q16", "Q17")
PA2 = c("Q20")
PA3 = c("Q14", "Q18", "Q19")
PA4 = c("Q10","Q11", "Q12", "Q13")
```

Desiderata is an internal consistency score of >.70

```{r}
alpha_PA1 <- psych::alpha(d[PA1], check.keys = TRUE)
print(alpha_PA1, digits = 3) # alpha = .83
```

```{r}
# alpha_PA2 <- psych::alpha(d[PA2], check.keys = TRUE)
# print(alpha_PA2, digits = 3) # only one variable so alpha cannot be calculated
```

```{r}
alpha_PA3 <- psych::alpha(d[PA3], check.keys = TRUE)
print(alpha_PA3, digits = 3) # alpha = 0.81
```

```{r}
alpha_PA4 <- psych::alpha(d[PA4], check.keys = TRUE)
print(alpha_PA4, digits = 3) # alpha = 0.89
```


```{r}
fa.diagram(mod_r)
```

Okay. So, for a different method, we can recover 4 latent factors; we have a proposed factor structure of 4 factors to reduce down from 12 variables.  What we have to do next is validate those factors by testing them on our outcome variable.  We do this in a number of steps called train and test.

Back to our two factor structure.  We will continue to use the `mod` results.  Look for this in the Environment pane.  Click on it to view the list.  This is a list of 50 items, one of which is `$scores`.  This contains a score for each dataset observation for each of the proposed factors.  It's like a new dataset.  Let take a look:

```{r}
head(mod$scores)
```

Call `mod$scores` and save them into a new dataset with the `Y` data that we earlier stored .

```{r}
d_fa <- cbind(Y, mod$scores) # new dataset of our factor scores for each original observation, paired with the outcome variable of the original dataset of logrt

head(d_fa)

```

To train and test the data, we need to split our observations into two:  70% will go into a train dataset and 30% will go into a test dataset.  This is normal practice and stops a terrible questionable research practice called "double dipping".

## Splitting 183 observations into a split dataset with 70% in one set and 30% in another:

```{r}
set.seed(100) # so we can reproduce this process
indices = sample(1:nrow(d_fa), 0.7*nrow(d_fa)) # sample 183 numbers randomly and select 70% of them
train = d_fa[indices, ] # select out the rows that are the same numbers as indices for train dataset
test = d_fa[-indices, ] # select out the rows that are NOT the same numbers as indices for 30% test dataset.
```

### Build a regression model on the train dataset

```{r}
summary(mod_train <- lm(Y ~ PA1 + PA2, train))
```

Next, we predict scores and match them to the test data to see how closely our model predicts a new set of data.  We withheld 30% of our scores that we know we have to match the observed values to the predicted scores from our factor model. The closer the match between the test observe scores and the predictions from the train model, the more accurate our factor structure is.

```{r}
mod_test <- predict(mod_train, 
                    newdata = test, 
                    type = "response")

test$Y_pred <- mod_test # save the model output as a new column in our test dataset

head(test[c("Y", "Y_pred")], 10) # lets compare the first 10 rows of observed rt and predicted rt values
```

The predictions are not great.  We could plot them to show the general difference:


```{r}

ggscatter(test, x = "Y", y = "Y_pred",
          alpha = 0.2,
          add = "reg.line",
          conf.int = TRUE,
          add.params = list(color = "blue",
                            fill = "lightgray"),) +
  stat_cor(method = "pearson", label.x = 20, label.y = 2.7) +
  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = "red")

```

The red line here shows the perfect line of correlation between our observed values (test - Y) and the predicted values from the train dataset. Perfect agreement between observed values of Y and predicted values of Y would line up on the red line. The blue line shows the regression line.  How much of our predicted dataset actually correlates with the observed data in the test dataset. We can see here that 2% is our correlation value - not great.  

What would be the outcome of using this factor structure in models, rather than the original independent variables?  Well, the low correlation means that large amounts of error or uniqueness are in your structures which will dampen down your effects in the end.  

```{r}
fa_lm <- lm(Y ~ . , d_fa) # the full stop here is a shorthand way of calling all the variables in the dataset into the linear regression model (saves you some typing!)

summary(fa_lm)
```

This model is significant but, compared to the model with the independent predictors and no factor structure, explains even less variance - which is good,  because we know the data is not good.

Put the categorical variables back into the dataset so that all the original variables are  now present.  Remember we also removed `brookesIMG` so we need to restore that, also.

```{r}
d_fa <- cbind(d_fa, d2) # 

head(d_fa)

```

```{r}

mod_fa_full <- lm(Y ~ PA1 + PA2 + Age, d_fa)

summary(mod_fa_full)
```

This model, with a two factor structure predicts approximately 1% of the variance in the `Y` outcome variable. You would have named the latent variables yourself and report the model in the usual way.  Remember the indicators of the reliability in the factor structure were less than satisfactory.  I would be consulting with my colleagues over whether this model is actually better, and be open to running a model with the independent predictors.

This has been a whirlwind whistle-stop tour of a process for factor analysis.  Other methods exist.  But if you think you have a dataset (a large survey?) that merits thinking about it in terms of underlying constructs or latent variables, this may be a template script with which you can start to explore.
