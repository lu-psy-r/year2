---
title: "Factor Analysis_Birthweight"
author: "Emma Mills"
date: "01/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(corrplot)
library(ggplot2)
library(car)
```

## Factor Analysis

Factors, with respect to factor analysis methods, are different from the factors that we talk about with the ANOVA method.

Also, sometimes, the method of factor analysis is used interchangeably with principal components analysis. Both factor analysis and principal components result in a reduced set of variables but they are slightly different. Principal components are built from the information from multiple variables that influence that component.  Think of them like a reduction of variables - from many we make few.  But they are still measures.

Factor analysis has a different perspective: From several variables (three or more), factor analysis attempts to separate the common parts of variance (the communality) from the uniqueness of the variance.  The common parts are theorised to correlate with each other because they are *being* influenced by the same latent (invisible?) construct.  The communality of a variable with other variables defines a factor loading; uniqueness + error is left over.  So, for good factors, we would like to see high communality values.

In contrast, if we have single, independent variables, high correlation values between a pair of variables can be problematic when running our statistical models (this is the multicollinearity aspect from the lecture).

It follows, that a condition of multicollinearity amongst pairs of variables may suggest that a factor analysis is plausible for a given dataset.

The theory that lies behind a dataset can help suggest how many latent variables may be suggested in the variables, but the actual process is very data driven.  There is a succession of steps where you make decisions based upon output from a series of tests.

### Helpful tips for transforming variables before you begin

  - reversed scored variables - transform them so they work in the same direction as the rest of the variables
  - remove outcome, categorical and ordinal variables from the dataset
  - relies heavily on Pearson's r correlations so all the assumptions of linearity persist
    - especially sensitive to the presence of ordinal and binary variables

Interpreting Factors:

 - a factor should have at least three variables loaded to a sufficient level (overdetermination)
 - a variable should have most of its loading on one factor
 - a factor should show a moderate level of internal consistency (=> .70)
 - factors should make theoretical sense
 
 
Steps:

 - parallel analysis identifies how many factors
 - conduct a factor analysis
   - evaluate against four interpretation criteria above
 - repeat with one less factor
 - repeat until can reduce no longer.
 - Name them
 - Report them
 
 It is better to have too many factors than too few in your solution.  Too few factors means that information may be lumped in with the error term (Uniqueness value).

Birthweight data taken from 
https://miro.medium.com/max/235/1*gFApNX9eWM-RU_ZhbHY_2A.png


```{r cars}

d1 <- read_csv("Birthweight_Sheffield.csv")

Y <- d1[ , c(1,2)] # Also store ID & Birthweight as outcome variable in Y - needed later for internal consistency measures

d <- d1[ , -c(1,2)] # Also store ID & Birthweight as outcome variable in Y - needed later for internal consistency measures

describe(d1) # be careful here because the print out makes everything seem like a numerical variable - asterisks on the output denote a character / factor class of variable
```

### Estimate the model on observed variables
```{r}
m_nfa_full <- lm(Birthweight ~ Length + Headcirc +
                   Gestation + smoker + mage +
                   mnocig + mheight + mppwt +
                   fage + fedyrs + fnocig + fheight, 
                 d1)

summary(m_nfa_full)
```

Let's do some stargazing because understanding this model in full is not what is required.  Most of the predictor variables here are not showing a significant impact upon the outcome variable of `Birthweight`. However, the set of predictors is significant (F (12, 29) = 8.742, *p* < .001). The model explains 78% of the variance in `Birthweight`. Predictor `smoker` would not be included in a factor analysis, however, because it is a categorical variable.  Seeing the full model, hopefully will help you remember to put the dataset back together again to re-estimate the model.

### Estimate the model with no categorical predictors - remove smoker from the model

```{r}
m_nfa <- lm(Birthweight ~ Length + Headcirc +
                   Gestation + mage +
                   mnocig + mheight + mppwt +
                   fage + fedyrs + fnocig + fheight, 
                 d1)

summary(m_nfa)
```

The model for this set of predictors on birthweight is also significant (F (11, 30) = 9.02, *p* < .001).  It explains 77% of the variance on `Birthweight`.

As factor analysis can also make larger datasets more manageable and more tractable for estimation of relationships, it helps to have a large dataset to begin with.  You can check how large your dataset is by either looking on your environment row or calling it:

```{r}
dim(d1)
```

Forty-two observations across 14 variables


### Removing columns that won't be needed in the factor analysis

Below is some code that splits the dataset.  This is an example of the circle in the workflow diagram.  Because of decisions I make later on in the script, I return to the code on line 119 and add more variables, to ensure that when I call on the d2 dataframe at the end of the script, I have all the original, discarded variables in one place.

```{r}
colnames(d1)
d <- d1[ , -c(1, 2, 6)] # a super quick snippet of code to remove the smoker variable & the outcome variable
head(d) # all numerical variables.

summary(d)
```

### Visualise all variables; test for normality

```{r}
par(mfrow = c(6, 3))
```

```{r}
apply(d,2,hist)
```



### Visualise all variables through a correlation matrix

Variable sets can get big pretty quickly, and because of measurement error and interactions, many a time one variable will overlap in what it measures with another variable - or to describe it another way, they will be correlated.  When lots of variables do this with one another, we call this multicollinearity.

We want out predictor to be correlated with our outcome variable - that is how we find effects, but having too much correlation between our predictors can be problematic.

When predictors overlap too much, they introduce redundancy into the model, and space where useful information could be is taken up with a kind of repeated information that the model has already seen.  Regression modelling thrives on having lots of sources of different information to find structure and patterns in the data.  When you have overlapping information, you have too much of one kind of information, and the model can get stuck, or give you poor estimations of coefficients.

Some correlation is okay and a model can cope, but strong correlation between predictors could cause problems.

We can visualise the correlations between variables using a correlation matrix.  We have used a kind of correlation matrix in weeks 2 - 4. Using a different one here that leaves out the histrograms of the variables on the diagonal, and concentrates on just the relationships between the variables:

```{r}
# send correlations to a new dataframe first
datamatrix <- cor(d) 
corrplot(datamatrix, method = "number")

```

If the labels are too big and I can't see numbers,  turn off numbers and use just colour instead.

```{r}
corrplot(datamatrix, method = "shade", tl.pos = "n")
```

What does the graph mean? It is a coloured correlation matrix.  We can use this because we are trying to get an overview of the level of correlation values across the entire dataset; we are not interested in the granular detail of the size of an association between two variables here. 

Ignore the diagonal line of dark blues squares.  This is where a variable is being paired with itself so = a perfect correlation of 1. The triangle below the diagonal line and the triangle above the diagonal line are symmetrical reflections of each other, so you only need to look at either the upper or lower triangle for the overview.

Each square represents a pair of variables in the dataframe and their correlation value - or how much of the same information they seem to share.  As the correlation gets stronger (moves toward from -1 or moves towards 1 from 0), the colour of the square moves from a paler shade to a deeper shade of either red or blue.  

If we were checking for multi-collinearity amongst our variables, we would like to see squares with no very deep red or blue colours.

If we are checking whether a factor analysis is feasible, we *would* like to see squares with very deep red or blue colours.  We also would like all squares to have the colour of around -.3 or .3 and above.  What do you think of the state of this dataset for a factor analysis?

So:  for suitability for factor analysis, the greater proportion of your variables should be correlated at +/- .3.

This is maybe not too bad.  Some low correlations between some pairs of variables.  For the purposes of demonstration,  it will suffice.

There are a couple of tests you can run to see if a factor analysis is feasible with the data you have:


### Test 1: The Kaiser-Meyer-Olkin Index (KMO (1974))

A cutoff / threshold for this test is an `Overall MSA` score of equal or above .60 and some others write equal or above .70.

```{r}
KMO(r = cor(d))
```
The KMO test gives us a *Measure of Sampling Adequacy* (MSA) for each item and the set of variables overall (Overall MSA). MSA levels of  > .80 for each item is desirable; variables showing MSA levels <.50 are not recommended for factor analysis.  Using a method called "principal axis" can help, however,  when MSA values are lower than desired.

`fnocig` shows a very low MSA score so, for the purposes of our demonstration today, we'll remove it and repeat the KMO calculation.  Also, ID is in here, which is not a predictor as such but a label - so we will return to the top and remove that with the outcome variable:

```{r}
d <- d[ , -10] # remove column 10  = fnocig
```

None of the variables are now below 50 and the MSA value has increased.


### Test Two: Bartlett's test of sphericity (1950)

```{r}
cortest.bartlett(d)
```

Bartlett's test of sphericity tests the null hypothesis that the correlation matrix of the dataset has values of one on the diagonals and values of zero else.  A correlation matrix that shows this pattern of values shows a perfect correlation when a variable is paired with itself and an absence of correlation between pairs of variables.  Consequently, because we are testing a null hypothesis, we are looking for a chi-square *p*-value <.05 to indicate that factor analysis is feasible.  

**Edited to reflect a correct conclusion given the p-value**
We do have that here, (*p* < .001) so we reject the null hypothesis of no relationship between the variables.

### Test Three: Parallel Analysis

```{r}
parallel_test <- fa.parallel(d)
```

We are not using any methods to estimate factors here, as we are trying to detect a number of factors to estimate before we begin.

To read this plot, you need to look at how the blue lines dip beneath the red dashed lines. Read off the factor / component number before it dips beneath the dashed line.  The component number is from the line of Xs; the factor number is from the line of triangles.

**Edited:  component and factor numbers are now the right way round for a correct interpretation**
Here the component number (Xs) is 2 and the factor number (triangles) is 2.

### Test Four: A scree test:

```{r}
scree(d)
```


### Conducting the factor analysis:

The parallel analysis suggests two factors so we can have a try for that number of factors now.  In the code below, the "fm" argument is asking the function to use principle axis ("pa") estimation and the "rotate" argument is asking the function to use an oblique rotation.  You don't need to know much about these at this point. 

```{r}
mod <- fa(r = d, 
          nfactors = 2,
          fm = "pa",
          rotate = "oblimin")

print(mod, cut = 0.3, digits = 3) # just loadings that are above 0.3 to check for quality
```

There are two boxes in the ouput:  

Second box:  Columns PA1 - PA2 are the latent variables or factors estimated by the `fa` function. They list how each of the variables load onto one of two factors:  The loading columns (PA1 and PA2) show how each of the columns in the datasets (on the rows) load onto each of the hypothesised factors.  

First box: Beginning with row `SS loadings`: this gives you an idea of robustness in the factors, `Proportion Var` gives the variance each factor explains in the data and `Cumulative Var`, how much total variance is explained as we add each factor variance onto the previous factor variance.  So by the time we get to PA2 about 44% of the variance in the data has been explained by the factor structure.

### Looking at loadings within factors

Look down each column (PA1 & PA2).  Here are guidelines for judging whether the loadings of each variable for that column are satsifactory:

 - 0.3 - 0.4 = ok
 - > 0.5 - better
 - > 0.7 - best
 
Against these guidelines, PA1 looks ok; PA2 looks ok.
 
#### Looking for cross-loadings of variables across factors

Here we are looking across the variables (across the rows of PA1 and PA2). 

A significant loading is a loading greater than > .03.  And cross loadings is official when a variable shows up on more than one factor at that level.

```{r}
print(mod, digits = 3) # getting all loadings rather than just > .03 now; Two outputs are returned.
```

`fheight`: has cross loading of a similar size across PA1 & PA2, which may be problematic
  
#### Looking for Communalities

This is a value that will be familiar to you.  Each factor will explain variance.  It's a little bit like the R^2 in linear regression.  Column `h2` is where you look for this.  A rule of thumb is `h2` > 0.25.

  - `mnocig`, `fedyrs` and `fheight` have h2 values < 0.25

#### Looking for factor correlations

This is at the heart of multicollinearity.  Where factors have correlations lower than .85, multicollinearity may not be problematic.  Where factors are correlated above .85, there is a massive overlap between factors, which means they could be measuring the same thing, and thus can be combined.

Factor correlations are stored above in the first output box:  The four by four list of values with the 1.000 on the diagonals

 - PA1 and PA2 are correlated at .144 (small correlation)

Interpreting Factors:

 - a factor should have at least three variables loaded to a sufficient level (overdetermination) - yes
 - a variable should have most of its loading on one factor - no - fheight
 - a factor should show a moderate level of internal consistency (=> .70)
 - factors should make theoretical sense
 
#### Checking for internal consistency 

Put the variables into the factor structure:
 
```{r}
names(d)
print(mod, cut = 0.3, digits = 3)

PA1 = c("Length", "Headcirc", "Gestation", "mheight", "mppwt", "fheight")
PA2 = c("mage", "mnocig" ,"fage", "fedyrs")

```

Desiderata is an internal consistency score of >.70

Look in each of the first boxes of the output for the raw alpha value.

```{r}
alpha_PA1 <- psych::alpha(d[PA1], check.keys = TRUE)
print(alpha_PA1, digits = 3) # alpha = 0.68
```

```{r}
alpha_PA2 <- psych::alpha(d[PA2], check.keys = TRUE)
print(alpha_PA2, digits = 3) # alpha = 0.58
```

PA1 has internal consistency below 70% (alpha = 0.68), PA2 has internal consistency below 70% (alpha = 0.58) do not have great internal consistency.  This may be the outcome of having a less than perfect, weak dataset for factor analysis to begin with. For PA1 this could be because of `fheight` having almost equal loadings on PA1 and PA2.  If this was a real analysis, I would remove `fheight` and re-estimate the factor structure without it,  letting it be an independent predictor alongside the factor structure.

Also, look at the variables that are in PA2 - age of mother and father, smoking behaviour of mother and fathers years of education.  I am not sure that this factor makes theoretical sense.  Ways to further explore this could be to expand the factor structure to three.

#### Draw your factor analysis structure

```{r}
fa.diagram(mod)
```

Okay. So, we have a proposed factor structure of 2 factors to reduce down from 10 variables.  What we have to do next is validate those factors by testing them on our outcome variable.  We do this in a number of steps called train and test.

We have low reliability on some of the factors, so this isn't a great empirical example.  But we will continue because it is a demonstration

We will continue to use the `mod` results.  Look for `mod` in the Data section of your Environment panel.  `mod` is a list of 52 items, one of which is `$scores`.  This contains a score for each dataset observation for each of the proposed factors.  It's like a new dataset.


```{r}
head(mod$scores)
```

Call `mod$scores` and save them into a new dataset with the `Birthweight` and `ID` data that we earlier stored in `Y`.

```{r}
d_fa <- cbind(Y, mod$scores) # new dataset of our factor scores for each original observation, paired with the outcome variable of the original dataset of logrt

```

To train and test the data, we need to split our observations into two:  70% will go into a train dataset and 30% will go into a test dataset.  This is normal practice and stops a terrible questionable research practice called "double dipping".

## Splitting 42 observations into a split dataset with 70% in one set and 30% in another:

```{r}
set.seed(100) # so we can reproduce this process
indices = sample(1:nrow(d_fa), 0.7*nrow(d_fa)) # sample 42 numbers randomly and select 70% of them
train = d_fa[indices, ] # select out the rows that are the same numbers as indices
test = d_fa[-indices, ] # select out the rows that are NOT the same numbers as indices for 30% test dataset.
```

### Build a regression model on the train dataset

```{r}
summary(mod_train <- lm(Birthweight ~ PA1 + PA2, train))

```

Next, we predict scores and match them to the test data to see how closely our model predicts a new set of data.  We withheld 30% of our scores that we know we have to match the observed values to the predicted scores from our factor model. The closer the match between the test observe scores and the predictions from the train model, the more accurate our factor structure is.

```{r}
mod_test <- predict(mod_train, 
                    newdata = test, 
                    type = "response")

test$BW_pred <- mod_test # BW = short for birthweight; save the model output as a new column in our test dataset

head(test[c("Birthweight", "BW_pred")], 10) # lets compare the first 10 rows of observed BW and predicted BW values

```

The predictions are not great.  We could plot them to show the general difference:

Here X axis = observed values of Birthweight and y axis = predictions from our factor model for birthweight.  The red line is the line of perfect correlation, where our observed birthweight and predicted birthweight would match exactly.

```{r}
library(ggpubr)
ggscatter(test, x = "Birthweight", y = "BW_pred",
          add = "reg.line") +
  geom_abline(intercept = 0, slope = 1, size = 0.5, color = "red")

```
alternative code for the plot:

```{r}

ggscatter(test, x = "Birthweight", y = "BW_pred",
          alpha = 0.5,
          add = "reg.line",
          conf.int = TRUE,
          add.params = list(color = "blue",
                            fill = "lightgray"),) +
  stat_cor(method = "pearson", label.x = 3, label.y = 2.7) +
  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = "red")

```

The red line here shows the perfect line of correlation between our observed values (test) and the predicted values from the train dataset.  The blue line shows the regression line.  How much of our predicted dataset actually correlates with the observed data in the test dataset. If our factor structure gave us accurate predictions, the points would fall along the red line.  The difference in slopes between the blue and red line give you an idea of the distance between our observed values and predicted values, given this factor structure.

What would be the outcome of using this factor structure in models, rather than the original independent variables?  Well, the low correlation means that large amounts of error or uniqueness are in your structures which will dampen down your effects in the end.

Lets put all the variables back into the model and see how the dataset with the factors.  Remember that we took out `fnocig` and `smoker` so we need to put those back too.

```{r}

d_fa$fnocig <- d1$fnocig
d_fa$smoker <- d1$smoker

fa_lm <- lm(Birthweight ~ PA1 + PA2 + smoker + fnocig, d_fa)

summary(fa_lm)
```
This model is significant but, compared to the model with the independent predictors and no factor structure, explains less variance (60% compared to 78%).  I am not sure that PA2 makes theoretical sense here.  What do you think?

This has been a whirlwind whistle-stop tour of a process for factor analysis.  Other methods exist.  But if you think you have a dataset (a large survey?) that merits thinking about it in terms of underlying constructs or latent variables, this may be a template script with which you can start to explore.