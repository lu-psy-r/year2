---
title: "Factor Analysis"
author: "Emma Mills"
date: "01/02/2022"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(tidyverse)
library(psych)
library(corrplot)
library(ggplot2)
library(car)
```

## Factor Analysis

Factors, with respect to factor analysis methods, are different from the factors that we talk about with the ANOVA method.

Also, sometimes, the method of factor analysis is used interchangeably with principal components analysis. Both factor analysis and principal components result in a reduced set of variables but they are slightly different. Principal components are built from the information from multiple variables that influence that component.  Think of them like a reduction of variables - from many we make few.  But they are still measures.

Factor analysis has a different perspective: From several variables (three or more), factor analysis attempts to separate the common parts of variance (the communality) from the uniqueness of the variance.  The common parts are theorised to correlate with each other because they are *being* influenced by the same latent (invisible?) construct.  The communality of a variable with other variables defines a factor loading; uniqueness + error is left over.  So, for good factors, we would like to see high communality values.

In contrast, if we have single, independent variables, high correlation values between a pair of variables can be problematic when running our statistical models (this is the multicollinearity aspect from the lecture).

It follows, that a condition of multicollinearity amongst pairs of variables may suggest that a factor analysis is plausible for a given dataset.

The theory that lies behind a dataset can help suggest how many latent variables may be suggested in the variables, but the actual process is very data driven.  There is a succession of steps where you make decisions based upon output from a series of tests.

### Helpful tips for transforming variables before you begin

  - reversed scored variables - transform them so they work in the same direction as the rest of the variables
  - remove outcome, categorical and ordinal variables from the dataset
  - relies heavily on Pearson's r correlations so all the assumptions of linearity persist
    - especially sensitive to the presence of ordinal and binary variables

Interpreting Factors:

 - a factor should have at least three variables loaded to a sufficient level (overdetermination)
 - a variable should have most of its loading on one factor
 - a factor should show a moderate level of internal consistency (=> .70)
 - factors should make theoretical sense
 
 
Steps:

 - parallel analysis identifies how many factors
 - conduct a factor analysis
   - evaluate against four interpretation criteria above
 - repeat with one less factor
 - repeat until can reduce no longer.
 - Name them
 - Report them
 
 It is better to have too many factors than too few in your solution.  Too few factors means that information may be lumped in with the error term (Uniqueness value).

```{r cars}
d1 <- read_csv("Data_SingleWords.csv")

Y <- d1[ , 20] # Also store logrt as outcome variable in Y - needed later for internal consistency measures

describe(d1) # be careful here because the print out makes everything seem like a numerical variable - asterisks on the variable labels (eg, itemname) denote a character / factor class of variable
```

```{r}
m_nfa_full <- lm(logrt ~ group + age.months + 
                   Voice + Nasal + Fricative + Liquid_SV +
                   Bilabials + Labiodentals + Alveolars + 
                   Palatals + Velars + Glottals +
                   
                   Length + Ortho_N + BG_Sum + BG_Mean +
                   BG_Freq_By_Pos + TOWREW_skill + TOWRENW_skill +
                   spoonerisms + OLD + PLD + Lg.UK.CDcount +
                   brookesIMG + AoA_Kup_lem + regularity, 
                 
                 d1)

summary(m_nfa_full)
```

Let's do some stargazing because understanding this model in full is not what is required.  Most of the predictor variables here are having a significant impact upon the outcome variable of `logrt`. The model explains 21% of the variance in `logrt`. Almost half of those predictor variables will not be included in the factor analysis, however, because they are categorical variables.  Seeing the full model, hopefully will help you remember to put the dataset back together again to re-estmate the model.

Have a look at a model with just the continuous variables:

```{r}
m_nfa <- lm(logrt ~ Length + Ortho_N + BG_Sum + BG_Mean +
              BG_Freq_By_Pos + TOWREW_skill + TOWRENW_skill +
              spoonerisms + OLD + PLD + Lg.UK.CDcount +
              brookesIMG + AoA_Kup_lem + regularity, d1)

summary(m_nfa)
```

The model for this set of predictors on response times of single word reading is significant (F (14, 5044) = 40.23, *p* < .001).  It explains 10% of the variance on `logrt`.

As factor analysis can also make larger datasets more manageable and more tractable for estimation of relationships, it helps to have a large dataset to begin with.  You can check how large your dataset is by either looking on your environment row or calling it:

```{r}
dim(d1)
```

5059 observations across 34 variables.  The variables we are interested in are columns 20 onward...I will remove these and make a copy at the same time, so that the original dataframe remains intact.


### Removing columns that won't be needed in the factor analysis

Below is some code that splits the dataset.  This is an example of the circle in the workflow diagram.  Because of decisions I make later on in the script, I return to the code on line 119 and add more variables, to ensure that when I call on the d2 dataframe at the end of the script, I have all the original, discarded variables in one place.

```{r}
d <- d1[ , -c(1:20)] # a super quick snippet of code to remove the first 20 columns

d2 <- d1[ , c(1:21, 32, 34)] # I am saving the unwanted variable set to a dataframe of its own for modelling process once we have a factor structure - see near the end of the document.

head(d) # all numerical variables.

summary(d)
```

### Visualise all variables; test for normality

```{r}
par(mfrow = c(6, 3))
```

```{r}
apply(d,2,hist)
```

```{r}
# regularity is on a binary scale - needs to be removed
d <- d[, -14]
```



### Visualise all variables through a correlation matrix

Variable sets can get big pretty quickly, and because of measurement error and interactions, many a time one variable will overlap in what it measures with another variable - or to describe it another way, they will be correlated.  When lots of variables do this with one another, we call this multicollinearity.

We want out predictor to be correlated with our outcome variable - that is how we find effects, but having too much correlation between our predictors can be problematic.

When predictors overlap too much, they introduce redundancy into the model, and space where useful information could be is taken up with a kind of repeated information that the model has already seen.  Regression modelling thrives on having lots of sources of different information to find structure and patterns in the data.  When you have overlapping information, you have too much of one kind of information, and the model can get stuck, or give you poor estimations of coefficients.

Some correlation is okay and a model can cope, but strong correlation between predictors could cause problems.

We can visualise the correlations between variables using a correlation matrix.  We have used a kind of correlation matrix in weeks 2 - 4. Using a different one here that leaves out the histrograms of the variables on the diagonal, and concentrates on just the relationships between the variables:

```{r}
# send correlations to a new dataframe first
datamatrix <- cor(d)
corrplot(datamatrix, method = "number")

```

If the labels are too big and I can't see numbers,  turn off numbers and use just colour instead.

```{r}
corrplot(datamatrix, method = "shade", tl.pos = "n")
```

What does the graph mean? It is a coloured correlation matrix.  We can use this because we are try to get an overview of the level of correlation values across the entire dataset; we are not interested in the granular detail of the size of an association between two variables here. 

Ignore the diagonal line of dark blues squares.  This is where a variable is being paired with itself so = a perfect correlation of 1. The triangle below the diagonal line and the triangle above the diagonal line are symmetrical reflections of each other, so you only need to look at either the upper or lower triangle for the overview.

Each square represents a pair of variables in the dataframe and their correlation value - or how much of the same information they seem to share.  As the correlation gets stronger (moves toward from -1 or moves towards 1 from 0), the colour of the square moves from a paler shade to a deeper shade of either red or blue.  

If we were checking for multi-collinearity amongst our variables, we would like to see squares with no very deep red or blue colours.

If we are checking whether a factor analysis is feasible, we *would* like to see squares with very deep red or blue colours.  We also would like all squares to have the colour of around -.3 or .3 and above.  What do you think of the state of this dataset for a factor analysis?

So:  for suitability for factor analysis, the greater proportion of your variables should be correlated at +/- .3.

This is maybe not too bad.  Some low correlations between some pairs of variables.  For the purposes of demonstration,  it will suffice.

There are a couple of tests you can run to see if a factor analysis is feasible with the data you have:


### Test 1: The Kaiser-Meyer-Olkin Index (KMO (1974))

A cutoff / threshold for this test is an `Overall MSA` score of equal or above .60 and some others write equal or above .70.

```{r}
KMO(r = cor(d))
```


The KMO test gives us a *Measure of Sampling Adequacy* (MSA) for each item and the set of variables overall (Overall MSA). MSA levels of  > .80 for each item is desirable; variables showing MSA levels <.50 are not recommended for factor analysis.  Using a method called "principal axis" can help, however,  when MSA values are lower than desired.

`brookesIMG` shows a very low MSA score so, for the purposes of our demonstration today, we'll remove it

```{r}
d <- d[ , -12] # remove column 12  = brookesIMG
```


### Test 2: Bartlett's test of sphericity (1950)

```{r}
cortest.bartlett(d)
```

Bartlett's test of sphericity tests the null hypothesis that the correlation matrix of the dataset has values of one on the diagonals and values of zero elsewhere.  A correlation matrix that shows this pattern of values shows a perfect correlation when a variable is paired with itself and an absence of correlation between pairs of  variables.  Consequently, because we are testing a null hypothesis, we are looking for a chi-square *p*-value <.05 to indicate that factor analysis is feasible. We don't have that here, so we fail to reject the null hypothesis.

We'll carry on anyway for the purposes of demonstration.

### Test 3: Parallel Analysis

```{r}
parallel_test <- fa.parallel(d)
```

This method send a warning message.  We are not using any methods to estimate factors here, as we are trying to detect a number of factors to estimate before we begin.

To read this plot, you need to look at how each of the blue lines dip beneath the red dashed lines. Choosing the triangle blue line (for factor analysis), count the number of symbols from the top left hand corner until the line dips beneath the bottom most red dashed lines. This is the number of factors recommended by the parallel analysis to put into your factor analysis.

Here the component number is 4 and the factor number is 4.

### Test 4: A scree test:

Very similar to the Parallel Analysis function above with slightly less information.

```{r}
scree(d)
```

### Conducting the factor analysis:

The parallel analysis suggests 4 factors so we can have a try for that number of factors now.  In the code below, the "fm" argument is asking the function to use principle axis ("pa") estimation and the "rotate" argument is asking the function to use an oblique rotation.  You don't need to know much about these at this point. 

```{r}
mod <- fa(r = d, 
          nfactors = 4,
          fm = "pa",
          rotate = "oblimin")

print(mod, cut = 0.3, digits = 3) # just loadings that are above 0.3 to check for quality
```
There are two boxes in the ouput:  

Second box:  Columns PA1 - PA4 are the latent variables or factors estimated by the `fa` function. They list how each of the variables load onto one of four factors:  The loading columns show how each of the columns in the datasets (on the rows) load onto each of the hypothesised factors.  

First box: Beginning with row `SS loadings`: this gives you an idea of robustness in the factors, `Proportion Var` gives the variance each factor explains in the data and `Cumulative Var`, how much total variance is explained as we add each factor variance onto the previous factor variance.  So by the time we get to PA4 about 69% of the variance in the data has been explained by the factor structure.

#### Looking at loadings within factors

The following guidelines exist for gauging how valid each variable loading is:

 - 0.3 - 0.4 = ok
 - > 0.5 - better
 - > 0.7 - best
 
Against these guidelines, PA1 looks ok; PA2 looks ok; PA3 looks ok; PA4 looks ok.
 
#### Looking for cross-loadings of variables across factors

Here we are looking across the variables (across the rows). 

A significant loading is a loading greater than > .03.  And cross loadings is official when a variable shows up on more than one factor at that level.

```{r}
print(mod, digits = 3) # getting all loadings rather than just > .03 now; Two outputs are returned.
```

  - Only Length: PA1 & PA2
  
#### Looking for Communalities

This is a value that will be familiar to you.  Each factor will explain an amount of variance.  It's a little bit like the R^2 in linear regression.  Column `h2` is where you look for this.  A rule of thumb for a valid factor is `h2` > 0.25.

  - all are above 0.25

#### Looking for factor correlations

This is at the heart of multicollinearity.  Where factors have correlations lower than .85 multicollinearity may not be problematic.  Where factors are correlated above .85, there is a massive overlap between factors, which means they could be measuring the same thing, and thus can be combined.

Factor correlations are stored above in the first output box:  The four by four list of values with the 1.000 on the diagonals

 - There are no correlations >.85 between our factors.

Interpreting Factors:

 - a factor should have at least three variables loaded to a sufficient level (overdetermination) - yes
 - a variable should have most of its loading on one factor - no - Length
 - a factor should show a moderate level of internal consistency (=> .70)
 - factors should make theoretical sense


#### Checking for internal consistency 

```{r}
names(d)
print(mod, cut = 0.3, digits = 3)

PA1 = c("Length", "Ortho_N", "OLD", "PLD")
PA2 = c("BG_Sum", "BG_Mean" ,"BG_Freq_By_Pos")
PA3 = c("TOWREW_skill", "TOWRENW_skill", "spoonerisms")
PA4 = c("Lg.UK.CDcount","AoA_Kup_lem")
```

Desiderata is an internal consistency score of >.70

Look in each of the first boxes of the output for the raw alpha value.

```{r}
alpha_PA1 <- psych::alpha(d[PA1], check.keys = TRUE)
print(alpha_PA1, digits = 3) # 0.36 not great
```

```{r}
alpha_PA2 <- psych::alpha(d[PA2], check.keys = TRUE)
print(alpha_PA2, digits = 3) # alpha = 0.64
```

```{r}
alpha_PA3 <- psych::alpha(d[PA3], check.keys = TRUE)
print(alpha_PA3, digits = 3) # alpha = 0.19
```

```{r}
alpha_PA4 <- psych::alpha(d[PA4], check.keys = TRUE)
print(alpha_PA4, digits = 3) # alpha = 0.46
```


Our factors do not have great internal consistency.  This may be the outcome of having a less than perfect, weak dataset for factor analysis to begin with.  Ways to further explore this could be to expand the factor structure to 5 or reduce to three.

#### Draw your factor analysis structure

```{r}
fa.diagram(mod)
```

Okay. So, we have a proposed factor structure of 4 factors to reduce down from 14 variables.  What we have to do next is validate those factors by testing them on our outcome variable.  We do this in a number of steps called train and test.

We have low reliability on some of the factors, so this isn't a great empirical example.  But we will continue because it is a demonstration

We will continue to use the `mod` results.  This is a list of 53 items, one of which is `$scores`.  This contains a score for each dataset observation for each of the proposed factors.  It's like a new dataset.

```{r}
head(mod$scores)
```

Call `mod$scores` and save them into a new dataset with the `logrt` data that we earlier stored in `Y`.

```{r}
d_fa <- cbind(Y, mod$scores) # new dataset of our factor scores for each original observation, paired with the outcome variable of the original dataset of logrt

head(d_fa)

```

To train and test the data, we need to split our observations into two:  70% will go into a train dataset and 30% will go into a test dataset.  This is normal practice and stops a terrible questionable research practice called "double dipping".

## Splitting 28041 observations into a split dataset with 70% in one set and 30% in another:

```{r}
set.seed(100) # so we can reproduce this process
indices = sample(1:nrow(d_fa), 0.7*nrow(d_fa)) # sample 5059 numbers randomly and select 70% of them
train = d_fa[indices, ] # select out the rows that are the same numbers as indices for train dataset
test = d_fa[-indices, ] # select out the rows that are NOT the same numbers as indices for 30% test dataset.
```

### Build a regression model on the train dataset

```{r}
summary(mod_train <- lm(logrt ~ PA1 + PA2 + PA3 + PA4, train))
```

Next, we predict scores and match them to the test data to see how closely our model predicts a new set of data.  We withheld 30% of our scores that we know we have to match the observed values to the predicted scores from our factor model. The closer the match between the test observe scores and the predictions from the train model, the more accurate our factor structure is.

```{r}
mod_test <- predict(mod_train, 
                    newdata = test, 
                    type = "response")

test$logrt_pred <- mod_test # save the model output as a new column in our test dataset

head(test[c("logrt", "logrt_pred")], 10) # lets compare the first 10 rows of observed rt and predicted rt values
```

The predictions are not great.  We could plot them to show the general difference:

```{r}
library(ggpubr)
ggscatter(test, x = "logrt", y = "logrt_pred",
          add = "reg.line") +
  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = "red")

```

This is a terrible plot.  There are so many observations, we cannot see what we are trying to see.  Select out a random sample of say 20% to visualise and communicate how your observed values and predicted values match (or don't - in this case).

We can re-use the code from above to gather together 20% of the dataset. Play with the percentages if you like. As you increase the percentage, lower the `alpha` argument in the ggscatter() function for the plot so that you can still see separate data points. Have a play.


```{r}
set.seed(101) # so we can reproduce this process
indices = sample(1:nrow(test), 0.20*nrow(test)) # sample 8413 numbers randomly and select 5% of them
# select out the rows that are the same numbers as indices = 420 observations
comms = test[indices, ] # select out the rows that are NOT the same numbers as indices for 30% test dataset.
```

```{r}

ggscatter(comms, x = "logrt", y = "logrt_pred",
          alpha = 0.2,
          add = "reg.line",
          conf.int = TRUE,
          add.params = list(color = "blue",
                            fill = "lightgray"),) +
  stat_cor(method = "pearson", label.x = 3, label.y = 2.7) +
  geom_abline(intercept = 0, slope = 1, size = 0.5, colour = "red")

```

The red line here shows the perfect line of correlation between our observed values (test - logrt) and the predicted values from the train dataset.  The blue line shows the regression line.  How much of our predicted dataset actually correlates with the observed data in the test dataset. We can see here that 24% is our correlation value - not great.  This is probably down to low reliability in the factor structure.  

What would be the outcome of using this factor structure in models, rather than the original independent variables?  Well, the low correlation means that large amounts of error or uniqueness are in your structures which will dampen down your effects in the end.  

```{r}
fa_lm <- lm(logrt ~ . , d_fa) # the full stop here is a shorthand way of calling all the variables in the dataset into the linear regression model (saves you some typing!)

summary(fa_lm)
```

This model is significant but, compared to the model with the independent predictors and no factor structure, explains less variance (5% compared to 10%).

Put the categorical variables back into the dataset so that all the original variables are  now present.  Remember we also removed `brookesIMG` so we need to restore that, also.

```{r}
d_fa <- cbind(d_fa, d2) # 

head(d_fa)

```

```{r}
mod_fa_full <- lm(logrt ~ group + age.months + 
                    Voice + Nasal + Fricative + Liquid_SV +
                    Bilabials + Labiodentals + Alveolars + 
                    Palatals + Velars + Glottals +
                    
                    PA1 + PA2 + PA3 + PA4 +
                    regularity + brookesIMG, 
                  
                  d_fa)

summary(mod_fa_full)
```

This model, with a four factor structure on the continuous, linguistic variables,  significantly predicts approximately 20% of the variance in the `logrt` outcome variable. You would have named the latent variables yourself and report the model in the usual way.  Remember the indicators of the reliability in the factor structure were less than satisfactory.  I would be consulting with my colleagues over whether this model is actually better, and be open to running a model with the independent predictors.

This has been a whirlwind whistle-stop tour of a process for factor analysis.  Other methods exist.  But if you think you have a dataset (a large survey?) that merits thinking about it in terms of underlying constructs or latent variables, this may be a template script with which you can start to explore.
